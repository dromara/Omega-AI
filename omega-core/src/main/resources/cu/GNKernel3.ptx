//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31442593
// Cuda compilation tools, release 11.7, V11.7.99
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_52
.address_size 64

	// .globl	GroupNormKernel
.extern .shared .align 16 .b8 share_mem[];

.visible .entry GroupNormKernel(
	.param .u32 GroupNormKernel_param_0,
	.param .u32 GroupNormKernel_param_1,
	.param .u32 GroupNormKernel_param_2,
	.param .u32 GroupNormKernel_param_3,
	.param .f32 GroupNormKernel_param_4,
	.param .u64 GroupNormKernel_param_5,
	.param .u64 GroupNormKernel_param_6,
	.param .u64 GroupNormKernel_param_7,
	.param .u64 GroupNormKernel_param_8,
	.param .u64 GroupNormKernel_param_9,
	.param .u64 GroupNormKernel_param_10
)
{
	.reg .pred 	%p<37>;
	.reg .f32 	%f<186>;
	.reg .b32 	%r<122>;
	.reg .b64 	%rd<27>;


	ld.param.u32 	%r57, [GroupNormKernel_param_0];
	ld.param.u32 	%r58, [GroupNormKernel_param_1];
	ld.param.u64 	%rd14, [GroupNormKernel_param_5];
	ld.param.u64 	%rd9, [GroupNormKernel_param_6];
	ld.param.u64 	%rd10, [GroupNormKernel_param_7];
	ld.param.u64 	%rd11, [GroupNormKernel_param_8];
	ld.param.u64 	%rd12, [GroupNormKernel_param_9];
	ld.param.u64 	%rd13, [GroupNormKernel_param_10];
	cvta.to.global.u64 	%rd1, %rd14;
	mov.u32 	%r106, %ctaid.x;
	setp.ge.u32 	%p11, %r106, %r57;
	@%p11 bra 	$L__BB0_33;

	cvta.to.global.u64 	%rd15, %rd12;
	add.s32 	%r61, %r58, 3;
	shr.s32 	%r62, %r61, 31;
	shr.u32 	%r63, %r62, 30;
	add.s32 	%r64, %r61, %r63;
	shr.s32 	%r2, %r64, 2;
	mov.u32 	%r3, %tid.x;
	shr.u32 	%r65, %r3, 5;
	mov.u32 	%r66, share_mem;
	mad.lo.s32 	%r5, %r65, 12, %r66;
	mov.u32 	%r6, %ntid.x;
	shr.u32 	%r7, %r6, 6;
	mad.lo.s32 	%r8, %r3, 12, %r66;
	mul.wide.u32 	%rd16, %r106, 4;
	add.s64 	%rd2, %rd15, %rd16;
	cvta.to.global.u64 	%rd17, %rd13;
	add.s64 	%rd3, %rd17, %rd16;
	cvta.to.global.u64 	%rd4, %rd9;
	cvta.to.global.u64 	%rd5, %rd10;
	cvta.to.global.u64 	%rd6, %rd11;
	setp.ge.s32 	%p12, %r3, %r2;
	mov.f32 	%f61, 0f00000000;

$L__BB0_2:
	mul.lo.s32 	%r67, %r106, %r58;
	cvt.u64.u32 	%rd7, %r67;
	mov.f32 	%f168, %f61;
	mov.f32 	%f169, %f61;
	mov.f32 	%f170, %f61;
	@%p12 bra 	$L__BB0_9;

	mov.u32 	%r107, %r3;
	mov.f32 	%f170, %f61;
	mov.f32 	%f169, %f61;
	mov.f32 	%f168, %f61;

$L__BB0_4:
	shl.b32 	%r12, %r107, 2;
	setp.ge.s32 	%p13, %r12, %r58;
	@%p13 bra 	$L__BB0_9;

	shl.b32 	%r101, %r107, 2;
	cvt.s64.s32 	%rd18, %r101;
	add.s64 	%rd19, %rd18, %rd7;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd8, %rd1, %rd20;
	ld.global.f32 	%f65, [%rd8];
	sub.f32 	%f66, %f65, %f168;
	add.f32 	%f170, %f170, 0f3F800000;
	div.rn.f32 	%f67, %f66, %f170;
	add.f32 	%f168, %f168, %f67;
	sub.f32 	%f68, %f65, %f168;
	fma.rn.f32 	%f169, %f66, %f68, %f169;
	add.s32 	%r68, %r101, 1;
	setp.ge.s32 	%p14, %r68, %r58;
	@%p14 bra 	$L__BB0_9;

	shl.b32 	%r104, %r107, 2;
	ld.global.f32 	%f69, [%rd8+4];
	sub.f32 	%f70, %f69, %f168;
	add.f32 	%f170, %f170, 0f3F800000;
	div.rn.f32 	%f71, %f70, %f170;
	add.f32 	%f168, %f168, %f71;
	sub.f32 	%f72, %f69, %f168;
	fma.rn.f32 	%f169, %f70, %f72, %f169;
	add.s32 	%r69, %r104, 2;
	setp.ge.s32 	%p15, %r69, %r58;
	@%p15 bra 	$L__BB0_9;

	shl.b32 	%r105, %r107, 2;
	ld.global.f32 	%f73, [%rd8+8];
	sub.f32 	%f74, %f73, %f168;
	add.f32 	%f170, %f170, 0f3F800000;
	div.rn.f32 	%f75, %f74, %f170;
	add.f32 	%f168, %f168, %f75;
	sub.f32 	%f76, %f73, %f168;
	fma.rn.f32 	%f169, %f74, %f76, %f169;
	add.s32 	%r70, %r105, 3;
	setp.ge.s32 	%p16, %r70, %r58;
	@%p16 bra 	$L__BB0_9;

	ld.global.f32 	%f77, [%rd8+12];
	sub.f32 	%f78, %f77, %f168;
	add.f32 	%f170, %f170, 0f3F800000;
	div.rn.f32 	%f79, %f78, %f170;
	add.f32 	%f168, %f168, %f79;
	sub.f32 	%f80, %f77, %f168;
	fma.rn.f32 	%f169, %f78, %f80, %f169;
	add.s32 	%r107, %r107, %r6;
	setp.lt.s32 	%p17, %r107, %r2;
	@%p17 bra 	$L__BB0_4;

$L__BB0_9:
	mov.b32 	%r113, %f168;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.down.b32 	%r15|%p1, %r113, %r72, %r71, %r73;
	mov.b32 	%r112, %f169;
	shfl.sync.down.b32 	%r17|%p2, %r112, %r72, %r71, %r73;
	mov.b32 	%r111, %f170;
	shfl.sync.down.b32 	%r74|%p18, %r111, %r72, %r71, %r73;
	mov.b32 	%f20, %r74;
	setp.eq.f32 	%p19, %f20, 0f00000000;
	@%p19 bra 	$L__BB0_11;

	mov.b32 	%f81, %r15;
	mov.b32 	%f82, %r17;
	add.f32 	%f83, %f169, %f82;
	sub.f32 	%f84, %f168, %f81;
	mul.f32 	%f85, %f84, %f84;
	mul.f32 	%f86, %f170, %f85;
	mul.f32 	%f87, %f86, %f20;
	add.f32 	%f21, %f170, %f20;
	div.rn.f32 	%f88, %f87, %f21;
	add.f32 	%f169, %f83, %f88;
	mul.f32 	%f89, %f81, %f20;
	fma.rn.f32 	%f90, %f168, %f170, %f89;
	div.rn.f32 	%f168, %f90, %f21;
	mov.b32 	%r113, %f168;
	mov.b32 	%r112, %f169;
	mov.b32 	%r111, %f21;
	mov.f32 	%f170, %f21;

$L__BB0_11:
	mov.u32 	%r76, 8;
	shfl.sync.down.b32 	%r25|%p3, %r113, %r76, %r71, %r73;
	shfl.sync.down.b32 	%r26|%p4, %r112, %r76, %r71, %r73;
	shfl.sync.down.b32 	%r78|%p20, %r111, %r76, %r71, %r73;
	mov.b32 	%f27, %r78;
	setp.eq.f32 	%p21, %f27, 0f00000000;
	@%p21 bra 	$L__BB0_13;

	mov.b32 	%f91, %r25;
	mov.b32 	%f92, %r26;
	add.f32 	%f93, %f169, %f92;
	sub.f32 	%f94, %f168, %f91;
	mul.f32 	%f95, %f94, %f94;
	mul.f32 	%f96, %f170, %f95;
	mul.f32 	%f97, %f96, %f27;
	add.f32 	%f28, %f170, %f27;
	div.rn.f32 	%f98, %f97, %f28;
	add.f32 	%f169, %f93, %f98;
	mul.f32 	%f99, %f91, %f27;
	fma.rn.f32 	%f100, %f168, %f170, %f99;
	div.rn.f32 	%f168, %f100, %f28;
	mov.b32 	%r113, %f168;
	mov.b32 	%r112, %f169;
	mov.b32 	%r111, %f28;
	mov.f32 	%f170, %f28;

$L__BB0_13:
	mov.u32 	%r80, 4;
	shfl.sync.down.b32 	%r33|%p5, %r113, %r80, %r71, %r73;
	shfl.sync.down.b32 	%r34|%p6, %r112, %r80, %r71, %r73;
	shfl.sync.down.b32 	%r82|%p22, %r111, %r80, %r71, %r73;
	mov.b32 	%f34, %r82;
	setp.eq.f32 	%p23, %f34, 0f00000000;
	@%p23 bra 	$L__BB0_15;

	mov.b32 	%f101, %r33;
	mov.b32 	%f102, %r34;
	add.f32 	%f103, %f169, %f102;
	sub.f32 	%f104, %f168, %f101;
	mul.f32 	%f105, %f104, %f104;
	mul.f32 	%f106, %f170, %f105;
	mul.f32 	%f107, %f106, %f34;
	add.f32 	%f35, %f170, %f34;
	div.rn.f32 	%f108, %f107, %f35;
	add.f32 	%f169, %f103, %f108;
	mul.f32 	%f109, %f101, %f34;
	fma.rn.f32 	%f110, %f168, %f170, %f109;
	div.rn.f32 	%f168, %f110, %f35;
	mov.b32 	%r113, %f168;
	mov.b32 	%r112, %f169;
	mov.b32 	%r111, %f35;
	mov.f32 	%f170, %f35;

$L__BB0_15:
	mov.u32 	%r84, 2;
	shfl.sync.down.b32 	%r41|%p7, %r113, %r84, %r71, %r73;
	shfl.sync.down.b32 	%r42|%p8, %r112, %r84, %r71, %r73;
	shfl.sync.down.b32 	%r86|%p24, %r111, %r84, %r71, %r73;
	mov.b32 	%f41, %r86;
	setp.eq.f32 	%p25, %f41, 0f00000000;
	@%p25 bra 	$L__BB0_17;

	mov.b32 	%f111, %r41;
	mov.b32 	%f112, %r42;
	add.f32 	%f113, %f169, %f112;
	sub.f32 	%f114, %f168, %f111;
	mul.f32 	%f115, %f114, %f114;
	mul.f32 	%f116, %f170, %f115;
	mul.f32 	%f117, %f116, %f41;
	add.f32 	%f42, %f170, %f41;
	div.rn.f32 	%f118, %f117, %f42;
	add.f32 	%f169, %f113, %f118;
	mul.f32 	%f119, %f111, %f41;
	fma.rn.f32 	%f120, %f168, %f170, %f119;
	div.rn.f32 	%f168, %f120, %f42;
	mov.b32 	%r113, %f168;
	mov.b32 	%r112, %f169;
	mov.b32 	%r111, %f42;
	mov.f32 	%f170, %f42;

$L__BB0_17:
	mov.u32 	%r88, 1;
	shfl.sync.down.b32 	%r49|%p9, %r113, %r88, %r71, %r73;
	shfl.sync.down.b32 	%r50|%p10, %r112, %r88, %r71, %r73;
	shfl.sync.down.b32 	%r90|%p26, %r111, %r88, %r71, %r73;
	mov.b32 	%f48, %r90;
	setp.eq.f32 	%p27, %f48, 0f00000000;
	@%p27 bra 	$L__BB0_19;

	mov.b32 	%f121, %r49;
	mov.b32 	%f122, %r50;
	add.f32 	%f123, %f169, %f122;
	sub.f32 	%f124, %f168, %f121;
	mul.f32 	%f125, %f124, %f124;
	mul.f32 	%f126, %f170, %f125;
	mul.f32 	%f127, %f126, %f48;
	add.f32 	%f49, %f170, %f48;
	div.rn.f32 	%f128, %f127, %f49;
	add.f32 	%f169, %f123, %f128;
	mul.f32 	%f129, %f121, %f48;
	fma.rn.f32 	%f130, %f168, %f170, %f129;
	div.rn.f32 	%f168, %f130, %f49;
	mov.f32 	%f170, %f49;

$L__BB0_19:
	and.b32  	%r97, %r3, 31;
	setp.ne.s32 	%p28, %r97, 0;
	@%p28 bra 	$L__BB0_21;

	st.shared.f32 	[%r5], %f168;
	st.shared.f32 	[%r5+4], %f169;
	st.shared.f32 	[%r5+8], %f170;

$L__BB0_21:
	setp.eq.s32 	%p29, %r7, 0;
	bar.sync 	0;
	@%p29 bra 	$L__BB0_27;

	mov.u32 	%r120, %r7;

$L__BB0_23:
	mov.u32 	%r51, %r120;
	setp.ge.u32 	%p30, %r3, %r51;
	@%p30 bra 	$L__BB0_26;

	mov.u32 	%r98, share_mem;
	add.s32 	%r91, %r51, %r3;
	mad.lo.s32 	%r52, %r91, 12, %r98;
	ld.shared.f32 	%f55, [%r52+8];
	setp.eq.f32 	%p31, %f55, 0f00000000;
	@%p31 bra 	$L__BB0_26;

	ld.shared.f32 	%f131, [%r8+8];
	add.f32 	%f132, %f131, %f55;
	ld.shared.f32 	%f133, [%r52+4];
	ld.shared.f32 	%f134, [%r8+4];
	add.f32 	%f135, %f134, %f133;
	ld.shared.f32 	%f136, [%r52];
	ld.shared.f32 	%f137, [%r8];
	sub.f32 	%f138, %f137, %f136;
	mul.f32 	%f139, %f138, %f138;
	mul.f32 	%f140, %f131, %f139;
	mul.f32 	%f141, %f55, %f140;
	div.rn.f32 	%f142, %f141, %f132;
	add.f32 	%f143, %f135, %f142;
	st.shared.f32 	[%r8+4], %f143;
	mul.f32 	%f144, %f55, %f136;
	fma.rn.f32 	%f145, %f131, %f137, %f144;
	div.rn.f32 	%f146, %f145, %f132;
	st.shared.f32 	[%r8], %f146;
	st.shared.f32 	[%r8+8], %f132;

$L__BB0_26:
	shr.s32 	%r120, %r51, 1;
	setp.gt.s32 	%p32, %r51, 1;
	@%p32 bra 	$L__BB0_23;

$L__BB0_27:
	setp.ne.s32 	%p33, %r3, 0;
	bar.sync 	0;
	@%p33 bra 	$L__BB0_29;

	ld.param.f32 	%f164, [GroupNormKernel_param_4];
	cvt.rn.f32.s32 	%f163, %r58;
	ld.shared.v2.f32 	{%f147, %f148}, [share_mem];
	st.global.f32 	[%rd2], %f147;
	div.rn.f32 	%f151, %f148, %f163;
	add.f32 	%f152, %f151, %f164;
	sqrt.rn.f32 	%f153, %f152;
	rcp.rn.f32 	%f154, %f153;
	st.shared.f32 	[share_mem+4], %f154;
	st.global.f32 	[%rd3], %f154;

$L__BB0_29:
	setp.ge.s32 	%p34, %r3, %r58;
	bar.sync 	0;
	@%p34 bra 	$L__BB0_32;

	ld.shared.v2.f32 	{%f155, %f156}, [share_mem];
	mov.u32 	%r121, %r3;

$L__BB0_31:
	ld.param.u32 	%r103, [GroupNormKernel_param_2];
	ld.param.u32 	%r102, [GroupNormKernel_param_3];
	cvt.u32.u64 	%r93, %rd7;
	add.s32 	%r94, %r121, %r93;
	div.s32 	%r95, %r94, %r102;
	rem.s32 	%r96, %r95, %r103;
	mul.wide.s32 	%rd21, %r94, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f32 	%f157, [%rd22];
	sub.f32 	%f158, %f157, %f155;
	mul.f32 	%f159, %f158, %f156;
	mul.wide.s32 	%rd23, %r96, 4;
	add.s64 	%rd24, %rd4, %rd23;
	ld.global.f32 	%f160, [%rd24];
	add.s64 	%rd25, %rd5, %rd23;
	ld.global.f32 	%f161, [%rd25];
	fma.rn.f32 	%f162, %f159, %f160, %f161;
	add.s64 	%rd26, %rd6, %rd21;
	st.global.f32 	[%rd26], %f162;
	add.s32 	%r121, %r121, %r6;
	setp.lt.s32 	%p35, %r121, %r58;
	@%p35 bra 	$L__BB0_31;

$L__BB0_32:
	ld.param.u32 	%r100, [GroupNormKernel_param_0];
	mov.u32 	%r99, %nctaid.x;
	add.s32 	%r106, %r106, %r99;
	setp.lt.u32 	%p36, %r106, %r100;
	@%p36 bra 	$L__BB0_2;

$L__BB0_33:
	ret;

}

