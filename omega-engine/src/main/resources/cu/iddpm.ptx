//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31833905
// Cuda compilation tools, release 11.8, V11.8.89
// Based on NVVM 7.0.1
//

.version 7.8
.target sm_52
.address_size 64

	// .globl	add_kernel
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;

.visible .entry add_kernel(
	.param .u64 add_kernel_param_0,
	.param .u64 add_kernel_param_1,
	.param .u64 add_kernel_param_2,
	.param .u64 add_kernel_param_3,
	.param .u64 add_kernel_param_4,
	.param .u64 add_kernel_param_5,
	.param .u32 add_kernel_param_6,
	.param .u32 add_kernel_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd1, [add_kernel_param_0];
	ld.param.u64 	%rd2, [add_kernel_param_1];
	ld.param.u64 	%rd3, [add_kernel_param_2];
	ld.param.u64 	%rd4, [add_kernel_param_3];
	ld.param.u64 	%rd5, [add_kernel_param_4];
	ld.param.u64 	%rd6, [add_kernel_param_5];
	ld.param.u32 	%r3, [add_kernel_param_6];
	ld.param.u32 	%r2, [add_kernel_param_7];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %nctaid.x;
	mov.u32 	%r6, %ctaid.y;
	mad.lo.s32 	%r7, %r6, %r5, %r4;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd7, %rd3;
	div.s32 	%r10, %r1, %r2;
	mul.wide.s32 	%rd8, %r10, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	cvt.rzi.s32.f32 	%r11, %f1;
	cvta.to.global.u64 	%rd10, %rd5;
	mul.wide.s32 	%rd11, %r11, 4;
	add.s64 	%rd12, %rd10, %rd11;
	cvta.to.global.u64 	%rd13, %rd1;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f2, [%rd15];
	ld.global.f32 	%f3, [%rd12];
	cvta.to.global.u64 	%rd16, %rd6;
	add.s64 	%rd17, %rd16, %rd11;
	cvta.to.global.u64 	%rd18, %rd2;
	add.s64 	%rd19, %rd18, %rd14;
	ld.global.f32 	%f4, [%rd19];
	ld.global.f32 	%f5, [%rd17];
	mul.f32 	%f6, %f5, %f4;
	fma.rn.f32 	%f7, %f3, %f2, %f6;
	cvta.to.global.u64 	%rd20, %rd4;
	add.s64 	%rd21, %rd20, %rd14;
	st.global.f32 	[%rd21], %f7;

$L__BB0_2:
	ret;

}
	// .globl	extract_into_tensor
.visible .entry extract_into_tensor(
	.param .u64 extract_into_tensor_param_0,
	.param .u64 extract_into_tensor_param_1,
	.param .u64 extract_into_tensor_param_2,
	.param .u32 extract_into_tensor_param_3,
	.param .u32 extract_into_tensor_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [extract_into_tensor_param_0];
	ld.param.u64 	%rd2, [extract_into_tensor_param_1];
	ld.param.u64 	%rd3, [extract_into_tensor_param_2];
	ld.param.u32 	%r3, [extract_into_tensor_param_3];
	ld.param.u32 	%r2, [extract_into_tensor_param_4];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %nctaid.x;
	mov.u32 	%r6, %ctaid.y;
	mad.lo.s32 	%r7, %r6, %r5, %r4;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd4, %rd2;
	div.s32 	%r10, %r1, %r2;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	cvt.rzi.s32.f32 	%r11, %f1;
	cvta.to.global.u64 	%rd7, %rd1;
	mul.wide.s32 	%rd8, %r11, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f2, [%rd9];
	cvta.to.global.u64 	%rd10, %rd3;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.f32 	[%rd12], %f2;

$L__BB1_2:
	ret;

}
	// .globl	model_variance
.visible .entry model_variance(
	.param .u64 model_variance_param_0,
	.param .u64 model_variance_param_1,
	.param .u64 model_variance_param_2,
	.param .u64 model_variance_param_3,
	.param .u32 model_variance_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [model_variance_param_0];
	ld.param.u64 	%rd2, [model_variance_param_1];
	ld.param.u64 	%rd3, [model_variance_param_2];
	ld.param.u64 	%rd4, [model_variance_param_3];
	ld.param.u32 	%r2, [model_variance_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd7];
	add.f32 	%f2, %f1, 0f3F800000;
	mov.f32 	%f3, 0f3F800000;
	mul.f32 	%f4, %f2, 0f3F000000;
	mov.f32 	%f5, 0f3F000000;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f6, [%rd9];
	sub.f32 	%f7, %f3, %f4;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f8, [%rd11];
	mul.f32 	%f9, %f8, %f7;
	fma.rn.f32 	%f10, %f6, %f4, %f9;
	mov.f32 	%f11, 0f3BBB989D;
	fma.rn.f32 	%f12, %f10, %f11, %f5;
	mov.f32 	%f13, 0f3FB8AA3B;
	mov.f32 	%f14, 0f437C0000;
	cvt.sat.f32.f32 	%f15, %f12;
	mov.f32 	%f16, 0f4B400001;
	fma.rm.f32 	%f17, %f15, %f14, %f16;
	add.f32 	%f18, %f17, 0fCB40007F;
	neg.f32 	%f19, %f18;
	fma.rn.f32 	%f20, %f10, %f13, %f19;
	mov.f32 	%f21, 0f32A57060;
	fma.rn.f32 	%f22, %f10, %f21, %f20;
	mov.b32 	%r9, %f17;
	shl.b32 	%r10, %r9, 23;
	mov.b32 	%f23, %r10;
	ex2.approx.ftz.f32 	%f24, %f22;
	mul.f32 	%f25, %f24, %f23;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	st.global.f32 	[%rd13], %f25;

$L__BB2_2:
	ret;

}
	// .globl	model_log_variance
.visible .entry model_log_variance(
	.param .u64 model_log_variance_param_0,
	.param .u64 model_log_variance_param_1,
	.param .u64 model_log_variance_param_2,
	.param .u64 model_log_variance_param_3,
	.param .u32 model_log_variance_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [model_log_variance_param_0];
	ld.param.u64 	%rd2, [model_log_variance_param_1];
	ld.param.u64 	%rd3, [model_log_variance_param_2];
	ld.param.u64 	%rd4, [model_log_variance_param_3];
	ld.param.u32 	%r2, [model_log_variance_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd7];
	add.f32 	%f2, %f1, 0f3F800000;
	mov.f32 	%f3, 0f3F800000;
	mul.f32 	%f4, %f2, 0f3F000000;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f5, [%rd9];
	sub.f32 	%f6, %f3, %f4;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f7, [%rd11];
	mul.f32 	%f8, %f7, %f6;
	fma.rn.f32 	%f9, %f5, %f4, %f8;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	st.global.f32 	[%rd13], %f9;

$L__BB3_2:
	ret;

}
	// .globl	sub_kernel
.visible .entry sub_kernel(
	.param .u64 sub_kernel_param_0,
	.param .u64 sub_kernel_param_1,
	.param .u64 sub_kernel_param_2,
	.param .u64 sub_kernel_param_3,
	.param .u64 sub_kernel_param_4,
	.param .u64 sub_kernel_param_5,
	.param .u32 sub_kernel_param_6,
	.param .u32 sub_kernel_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd1, [sub_kernel_param_0];
	ld.param.u64 	%rd2, [sub_kernel_param_1];
	ld.param.u64 	%rd3, [sub_kernel_param_2];
	ld.param.u64 	%rd4, [sub_kernel_param_3];
	ld.param.u64 	%rd5, [sub_kernel_param_4];
	ld.param.u64 	%rd6, [sub_kernel_param_5];
	ld.param.u32 	%r3, [sub_kernel_param_6];
	ld.param.u32 	%r2, [sub_kernel_param_7];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %nctaid.x;
	mov.u32 	%r6, %ctaid.y;
	mad.lo.s32 	%r7, %r6, %r5, %r4;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd7, %rd3;
	div.s32 	%r10, %r1, %r2;
	mul.wide.s32 	%rd8, %r10, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	cvt.rzi.s32.f32 	%r11, %f1;
	cvta.to.global.u64 	%rd10, %rd5;
	mul.wide.s32 	%rd11, %r11, 4;
	add.s64 	%rd12, %rd10, %rd11;
	cvta.to.global.u64 	%rd13, %rd1;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f2, [%rd15];
	ld.global.f32 	%f3, [%rd12];
	mul.f32 	%f4, %f3, %f2;
	cvta.to.global.u64 	%rd16, %rd6;
	add.s64 	%rd17, %rd16, %rd11;
	cvta.to.global.u64 	%rd18, %rd2;
	add.s64 	%rd19, %rd18, %rd14;
	ld.global.f32 	%f5, [%rd19];
	ld.global.f32 	%f6, [%rd17];
	mul.f32 	%f7, %f6, %f5;
	sub.f32 	%f8, %f4, %f7;
	cvta.to.global.u64 	%rd20, %rd4;
	add.s64 	%rd21, %rd20, %rd14;
	st.global.f32 	[%rd21], %f8;

$L__BB4_2:
	ret;

}
	// .globl	normal_kl
.visible .entry normal_kl(
	.param .u64 normal_kl_param_0,
	.param .u64 normal_kl_param_1,
	.param .u64 normal_kl_param_2,
	.param .u64 normal_kl_param_3,
	.param .u64 normal_kl_param_4,
	.param .u32 normal_kl_param_5
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<46>;
	.reg .f64 	%fd<31>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd1, [normal_kl_param_0];
	ld.param.u64 	%rd2, [normal_kl_param_1];
	ld.param.u64 	%rd3, [normal_kl_param_2];
	ld.param.u64 	%rd4, [normal_kl_param_3];
	ld.param.u64 	%rd5, [normal_kl_param_4];
	ld.param.u32 	%r5, [normal_kl_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %ctaid.y;
	mad.lo.s32 	%r9, %r8, %r7, %r6;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	setp.ge.s32 	%p2, %r1, %r5;
	@%p2 bra 	$L__BB5_15;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	add.s64 	%rd10, %rd9, %rd7;
	cvta.to.global.u64 	%rd11, %rd1;
	add.s64 	%rd12, %rd11, %rd7;
	cvta.to.global.u64 	%rd13, %rd3;
	add.s64 	%rd14, %rd13, %rd7;
	ld.global.f32 	%f1, [%rd10];
	ld.global.f32 	%f2, [%rd8];
	ld.global.f32 	%f4, [%rd14];
	ld.global.f32 	%f5, [%rd12];
	sub.f32 	%f3, %f5, %f4;
	cvt.f64.f32 	%fd1, %f3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.f64 	%fd12, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd12;
	}
	and.b32  	%r12, %r3, 2146435072;
	setp.eq.s32 	%p3, %r12, 1062207488;
	abs.f64 	%fd2, %fd1;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd2;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd12;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd29, [retval0+0];
	} // callseq 0
	setp.lt.s32 	%p4, %r2, 0;
	and.pred  	%p1, %p4, %p3;
	not.pred 	%p5, %p1;
	@%p5 bra 	$L__BB5_3;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd29;
	}
	xor.b32  	%r14, %r13, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd29;
	}
	mov.b64 	%fd29, {%r15, %r14};

$L__BB5_3:
	setp.eq.f32 	%p6, %f3, 0f00000000;
	@%p6 bra 	$L__BB5_7;
	bra.uni 	$L__BB5_4;

$L__BB5_7:
	selp.b32 	%r17, %r2, 0, %p3;
	mov.u32 	%r18, 0;
	or.b32  	%r19, %r17, 2146435072;
	setp.lt.s32 	%p10, %r3, 0;
	selp.b32 	%r20, %r19, %r17, %p10;
	mov.b64 	%fd29, {%r18, %r20};
	bra.uni 	$L__BB5_8;

$L__BB5_4:
	setp.gt.s32 	%p7, %r2, -1;
	@%p7 bra 	$L__BB5_8;

	mov.f64 	%fd13, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd14, %fd13;
	setp.eq.f64 	%p8, %fd14, 0d4000000000000000;
	@%p8 bra 	$L__BB5_8;

	mov.f64 	%fd29, 0dFFF8000000000000;

$L__BB5_8:
	add.f64 	%fd8, %fd1, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd8;
	}
	and.b32  	%r22, %r21, 2146435072;
	setp.ne.s32 	%p11, %r22, 2146435072;
	mov.f64 	%fd30, %fd29;
	@%p11 bra 	$L__BB5_14;

	setp.gtu.f64 	%p12, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd30, %fd8;
	@%p12 bra 	$L__BB5_14;

	mov.f64 	%fd16, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd16;
	}
	and.b32  	%r4, %r3, 2147483647;
	setp.eq.s32 	%p13, %r4, 2146435072;
	setp.eq.s32 	%p14, %r23, 0;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB5_13;
	bra.uni 	$L__BB5_11;

$L__BB5_13:
	setp.gt.f64 	%p22, %fd2, 0d3FF0000000000000;
	selp.b32 	%r30, 2146435072, 0, %p22;
	mov.u32 	%r31, 0;
	xor.b32  	%r32, %r30, 2146435072;
	setp.lt.s32 	%p23, %r3, 0;
	selp.b32 	%r33, %r32, %r30, %p23;
	setp.eq.f32 	%p24, %f3, 0fBF800000;
	selp.b32 	%r34, 1072693248, %r33, %p24;
	mov.b64 	%fd30, {%r31, %r34};
	bra.uni 	$L__BB5_14;

$L__BB5_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd1;
	}
	and.b32  	%r25, %r2, 2147483647;
	setp.ne.s32 	%p16, %r25, 2146435072;
	setp.ne.s32 	%p17, %r24, 0;
	or.pred  	%p18, %p16, %p17;
	mov.f64 	%fd30, %fd29;
	@%p18 bra 	$L__BB5_14;

	setp.gt.s32 	%p19, %r3, -1;
	selp.b32 	%r26, 2146435072, 0, %p19;
	mov.u32 	%r27, 0;
	setp.ne.s32 	%p20, %r4, 1071644672;
	and.pred  	%p21, %p20, %p1;
	or.b32  	%r28, %r26, -2147483648;
	selp.b32 	%r29, %r28, %r26, %p21;
	mov.b64 	%fd30, {%r27, %r29};

$L__BB5_14:
	sub.f32 	%f6, %f2, %f1;
	mov.f32 	%f7, 0f3F000000;
	mov.f32 	%f8, 0f3BBB989D;
	fma.rn.f32 	%f9, %f6, %f8, %f7;
	mov.f32 	%f10, 0f3FB8AA3B;
	mov.f32 	%f11, 0f437C0000;
	cvt.sat.f32.f32 	%f12, %f9;
	mov.f32 	%f13, 0f4B400001;
	fma.rm.f32 	%f14, %f12, %f11, %f13;
	setp.eq.f32 	%p25, %f3, 0f3F800000;
	selp.f64 	%fd17, 0d3FF0000000000000, %fd30, %p25;
	neg.f32 	%f15, %f1;
	fma.rn.f32 	%f16, %f15, %f8, %f7;
	cvt.sat.f32.f32 	%f17, %f16;
	fma.rm.f32 	%f18, %f17, %f11, %f13;
	add.f32 	%f19, %f18, 0fCB40007F;
	neg.f32 	%f20, %f19;
	fma.rn.f32 	%f21, %f15, %f10, %f20;
	mov.f32 	%f22, 0f32A57060;
	fma.rn.f32 	%f23, %f15, %f22, %f21;
	mov.b32 	%r35, %f18;
	shl.b32 	%r36, %r35, 23;
	mov.b32 	%f24, %r36;
	ex2.approx.ftz.f32 	%f25, %f23;
	mul.f32 	%f26, %f25, %f24;
	cvt.f64.f32 	%fd18, %f26;
	add.f32 	%f27, %f14, 0fCB40007F;
	neg.f32 	%f28, %f27;
	fma.rn.f32 	%f29, %f6, %f10, %f28;
	fma.rn.f32 	%f30, %f6, %f22, %f29;
	ex2.approx.ftz.f32 	%f31, %f30;
	mov.b32 	%r37, %f14;
	shl.b32 	%r38, %r37, 23;
	mov.b32 	%f32, %r38;
	mul.f32 	%f33, %f31, %f32;
	cvt.f64.f32 	%fd19, %f33;
	cvt.f64.f32 	%fd20, %f2;
	cvt.f64.f32 	%fd21, %f1;
	add.f64 	%fd22, %fd21, 0dBFF0000000000000;
	sub.f64 	%fd23, %fd22, %fd20;
	add.f64 	%fd24, %fd23, %fd19;
	fma.rn.f64 	%fd25, %fd17, %fd18, %fd24;
	mul.f64 	%fd26, %fd25, 0d3FE0000000000000;
	div.rn.f64 	%fd27, %fd26, 0d3FE62E42FEFA39EF;
	cvt.rn.f32.f64 	%f34, %fd27;
	cvta.to.global.u64 	%rd15, %rd5;
	add.s64 	%rd17, %rd15, %rd7;
	st.global.f32 	[%rd17], %f34;

$L__BB5_15:
	ret;

}
	// .globl	normal_kl_back
.visible .entry normal_kl_back(
	.param .u64 normal_kl_back_param_0,
	.param .u64 normal_kl_back_param_1,
	.param .u64 normal_kl_back_param_2,
	.param .u64 normal_kl_back_param_3,
	.param .u64 normal_kl_back_param_4,
	.param .u32 normal_kl_back_param_5
)
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<38>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd2, [normal_kl_back_param_0];
	ld.param.u64 	%rd3, [normal_kl_back_param_1];
	ld.param.u64 	%rd4, [normal_kl_back_param_2];
	ld.param.u64 	%rd5, [normal_kl_back_param_3];
	ld.param.u64 	%rd6, [normal_kl_back_param_4];
	ld.param.u32 	%r6, [normal_kl_back_param_5];
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %ctaid.y;
	mad.lo.s32 	%r10, %r9, %r8, %r7;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.s32 	%p2, %r1, %r6;
	@%p2 bra 	$L__BB6_15;

	cvta.to.global.u64 	%rd7, %rd3;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd5;
	add.s64 	%rd11, %rd10, %rd8;
	cvta.to.global.u64 	%rd12, %rd2;
	add.s64 	%rd13, %rd12, %rd8;
	cvta.to.global.u64 	%rd14, %rd4;
	add.s64 	%rd15, %rd14, %rd8;
	ld.global.f32 	%f1, [%rd11];
	ld.global.f32 	%f2, [%rd9];
	ld.global.f32 	%f4, [%rd15];
	ld.global.f32 	%f5, [%rd13];
	sub.f32 	%f3, %f5, %f4;
	cvt.f64.f32 	%fd1, %f3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	mov.f64 	%fd12, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd12;
	}
	and.b32  	%r4, %r3, 2146435072;
	setp.eq.s32 	%p3, %r4, 1062207488;
	abs.f64 	%fd2, %fd1;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd2;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd12;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd28, [retval0+0];
	} // callseq 1
	setp.lt.s32 	%p4, %r2, 0;
	and.pred  	%p1, %p4, %p3;
	not.pred 	%p5, %p1;
	@%p5 bra 	$L__BB6_3;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd28;
	}
	xor.b32  	%r14, %r13, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd28;
	}
	mov.b64 	%fd28, {%r15, %r14};

$L__BB6_3:
	setp.eq.f32 	%p6, %f3, 0f00000000;
	@%p6 bra 	$L__BB6_7;
	bra.uni 	$L__BB6_4;

$L__BB6_7:
	selp.b32 	%r16, %r2, 0, %p3;
	mov.u32 	%r17, 0;
	or.b32  	%r18, %r16, 2146435072;
	setp.lt.s32 	%p10, %r3, 0;
	selp.b32 	%r19, %r18, %r16, %p10;
	mov.b64 	%fd28, {%r17, %r19};
	bra.uni 	$L__BB6_8;

$L__BB6_4:
	setp.gt.s32 	%p7, %r2, -1;
	@%p7 bra 	$L__BB6_8;

	mov.f64 	%fd13, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd14, %fd13;
	setp.eq.f64 	%p8, %fd14, 0d4000000000000000;
	@%p8 bra 	$L__BB6_8;

	mov.f64 	%fd28, 0dFFF8000000000000;

$L__BB6_8:
	add.f64 	%fd8, %fd1, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd8;
	}
	and.b32  	%r21, %r20, 2146435072;
	setp.ne.s32 	%p11, %r21, 2146435072;
	mov.f64 	%fd29, %fd28;
	@%p11 bra 	$L__BB6_14;

	setp.gtu.f64 	%p12, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd29, %fd8;
	@%p12 bra 	$L__BB6_14;

	mov.f64 	%fd16, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd16;
	}
	and.b32  	%r5, %r3, 2147483647;
	setp.eq.s32 	%p13, %r5, 2146435072;
	setp.eq.s32 	%p14, %r22, 0;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB6_13;
	bra.uni 	$L__BB6_11;

$L__BB6_13:
	setp.gt.f64 	%p22, %fd2, 0d3FF0000000000000;
	selp.b32 	%r29, 2146435072, 0, %p22;
	mov.u32 	%r30, 0;
	xor.b32  	%r31, %r29, 2146435072;
	setp.lt.s32 	%p23, %r3, 0;
	selp.b32 	%r32, %r31, %r29, %p23;
	setp.eq.f32 	%p24, %f3, 0fBF800000;
	selp.b32 	%r33, 1072693248, %r32, %p24;
	mov.b64 	%fd29, {%r30, %r33};
	bra.uni 	$L__BB6_14;

$L__BB6_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd1;
	}
	and.b32  	%r24, %r2, 2147483647;
	setp.ne.s32 	%p16, %r24, 2146435072;
	setp.ne.s32 	%p17, %r23, 0;
	or.pred  	%p18, %p16, %p17;
	mov.f64 	%fd29, %fd28;
	@%p18 bra 	$L__BB6_14;

	setp.gt.s32 	%p19, %r3, -1;
	selp.b32 	%r25, 2146435072, 0, %p19;
	mov.u32 	%r26, 0;
	setp.ne.s32 	%p20, %r5, 1071644672;
	and.pred  	%p21, %p20, %p1;
	or.b32  	%r27, %r25, -2147483648;
	selp.b32 	%r28, %r27, %r25, %p21;
	mov.b64 	%fd29, {%r26, %r28};

$L__BB6_14:
	cvt.rn.f64.s32 	%fd17, %r6;
	mov.f64 	%fd18, 0d3FE71547641B167E;
	div.rn.f64 	%fd19, %fd18, %fd17;
	cvt.rn.f32.f64 	%f6, %fd19;
	sub.f32 	%f7, %f2, %f1;
	mov.f32 	%f8, 0f3F000000;
	mov.f32 	%f9, 0f3BBB989D;
	fma.rn.f32 	%f10, %f7, %f9, %f8;
	mov.f32 	%f11, 0f3FB8AA3B;
	mov.f32 	%f12, 0f437C0000;
	cvt.sat.f32.f32 	%f13, %f10;
	mov.f32 	%f14, 0f4B400001;
	fma.rm.f32 	%f15, %f13, %f12, %f14;
	setp.eq.f32 	%p25, %f3, 0f3F800000;
	selp.f64 	%fd20, 0d3FF0000000000000, %fd29, %p25;
	cvt.f64.f32 	%fd21, %f6;
	mul.f64 	%fd22, %fd20, %fd21;
	neg.f32 	%f16, %f1;
	fma.rn.f32 	%f17, %f16, %f9, %f8;
	cvt.sat.f32.f32 	%f18, %f17;
	fma.rm.f32 	%f19, %f18, %f12, %f14;
	add.f32 	%f20, %f19, 0fCB40007F;
	neg.f32 	%f21, %f20;
	fma.rn.f32 	%f22, %f16, %f11, %f21;
	mov.f32 	%f23, 0f32A57060;
	fma.rn.f32 	%f24, %f16, %f23, %f22;
	mov.b32 	%r34, %f19;
	shl.b32 	%r35, %r34, 23;
	mov.b32 	%f25, %r35;
	ex2.approx.ftz.f32 	%f26, %f24;
	mul.f32 	%f27, %f26, %f25;
	cvt.f64.f32 	%fd23, %f27;
	mul.f64 	%fd24, %fd22, %fd23;
	add.f32 	%f28, %f15, 0fCB40007F;
	neg.f32 	%f29, %f28;
	fma.rn.f32 	%f30, %f7, %f11, %f29;
	fma.rn.f32 	%f31, %f7, %f23, %f30;
	ex2.approx.ftz.f32 	%f32, %f31;
	mov.b32 	%r36, %f15;
	shl.b32 	%r37, %r36, 23;
	mov.b32 	%f33, %r37;
	mul.f32 	%f34, %f32, %f33;
	mul.f32 	%f35, %f34, %f6;
	sub.f32 	%f36, %f6, %f35;
	cvt.f64.f32 	%fd25, %f36;
	sub.f64 	%fd26, %fd25, %fd24;
	cvt.rn.f32.f64 	%f37, %fd26;
	cvta.to.global.u64 	%rd16, %rd6;
	shl.b64 	%rd17, %rd1, 2;
	add.s64 	%rd18, %rd16, %rd17;
	st.global.f32 	[%rd18], %f37;

$L__BB6_15:
	ret;

}
	// .globl	discretized_gaussian_log_likelihood
.visible .entry discretized_gaussian_log_likelihood(
	.param .u64 discretized_gaussian_log_likelihood_param_0,
	.param .u64 discretized_gaussian_log_likelihood_param_1,
	.param .u64 discretized_gaussian_log_likelihood_param_2,
	.param .u64 discretized_gaussian_log_likelihood_param_3,
	.param .u32 discretized_gaussian_log_likelihood_param_4
)
{
	.reg .pred 	%p<70>;
	.reg .f32 	%f<131>;
	.reg .b32 	%r<98>;
	.reg .f64 	%fd<241>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [discretized_gaussian_log_likelihood_param_0];
	ld.param.u64 	%rd3, [discretized_gaussian_log_likelihood_param_1];
	ld.param.u64 	%rd4, [discretized_gaussian_log_likelihood_param_2];
	ld.param.u64 	%rd5, [discretized_gaussian_log_likelihood_param_3];
	ld.param.u32 	%r15, [discretized_gaussian_log_likelihood_param_4];
	mov.u32 	%r16, %nctaid.x;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %ctaid.x;
	mad.lo.s32 	%r19, %r17, %r16, %r18;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r1, %r19, %r20, %r21;
	setp.ge.s32 	%p3, %r1, %r15;
	@%p3 bra 	$L__BB7_46;

	cvta.to.global.u64 	%rd6, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f32 	%f1, [%rd10];
	ld.global.f32 	%f2, [%rd8];
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f32 	%f24, [%rd12];
	cvt.f64.f32 	%fd40, %f24;
	mul.f64 	%fd1, %fd40, 0dBFE0000000000000;
	mov.f64 	%fd41, 0d4338000000000000;
	mov.f64 	%fd42, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd43, %fd1, %fd42, %fd41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r2, %temp}, %fd43;
	}
	mov.f64 	%fd44, 0dC338000000000000;
	add.rn.f64 	%fd45, %fd43, %fd44;
	mov.f64 	%fd46, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd47, %fd45, %fd46, %fd1;
	mov.f64 	%fd48, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd49, %fd45, %fd48, %fd47;
	mov.f64 	%fd50, 0d3E928AF3FCA213EA;
	mov.f64 	%fd51, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd52, %fd51, %fd49, %fd50;
	mov.f64 	%fd53, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd54, %fd52, %fd49, %fd53;
	mov.f64 	%fd55, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd56, %fd54, %fd49, %fd55;
	mov.f64 	%fd57, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd58, %fd56, %fd49, %fd57;
	mov.f64 	%fd59, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd60, %fd58, %fd49, %fd59;
	mov.f64 	%fd61, 0d3F81111111122322;
	fma.rn.f64 	%fd62, %fd60, %fd49, %fd61;
	mov.f64 	%fd63, 0d3FA55555555502A1;
	fma.rn.f64 	%fd64, %fd62, %fd49, %fd63;
	mov.f64 	%fd65, 0d3FC5555555555511;
	fma.rn.f64 	%fd66, %fd64, %fd49, %fd65;
	mov.f64 	%fd67, 0d3FE000000000000B;
	fma.rn.f64 	%fd68, %fd66, %fd49, %fd67;
	mov.f64 	%fd69, 0d3FF0000000000000;
	fma.rn.f64 	%fd70, %fd68, %fd49, %fd69;
	fma.rn.f64 	%fd71, %fd70, %fd49, %fd69;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd71;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd71;
	}
	shl.b32 	%r22, %r2, 20;
	add.s32 	%r23, %r4, %r22;
	mov.b64 	%fd232, {%r3, %r23};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd1;
	}
	mov.b32 	%f25, %r24;
	abs.f32 	%f3, %f25;
	setp.lt.f32 	%p4, %f3, 0f4086232B;
	@%p4 bra 	$L__BB7_4;

	setp.lt.f64 	%p5, %fd1, 0d0000000000000000;
	add.f64 	%fd72, %fd1, 0d7FF0000000000000;
	selp.f64 	%fd232, 0d0000000000000000, %fd72, %p5;
	setp.geu.f32 	%p6, %f3, 0f40874800;
	@%p6 bra 	$L__BB7_4;

	shr.u32 	%r25, %r2, 31;
	add.s32 	%r26, %r2, %r25;
	shr.s32 	%r27, %r26, 1;
	shl.b32 	%r28, %r27, 20;
	add.s32 	%r29, %r4, %r28;
	mov.b64 	%fd73, {%r3, %r29};
	sub.s32 	%r30, %r2, %r27;
	shl.b32 	%r31, %r30, 20;
	add.s32 	%r32, %r31, 1072693248;
	mov.u32 	%r33, 0;
	mov.b64 	%fd74, {%r33, %r32};
	mul.f64 	%fd232, %fd73, %fd74;

$L__BB7_4:
	cvt.rn.f32.f64 	%f26, %fd232;
	cvt.f64.f32 	%fd6, %f26;
	sub.f32 	%f27, %f2, %f1;
	cvt.f64.f32 	%fd7, %f27;
	add.f64 	%fd75, %fd7, 0d3F70101010101010;
	mul.f64 	%fd76, %fd75, %fd6;
	cvt.rn.f32.f64 	%f4, %fd76;
	cvt.f64.f32 	%fd8, %f4;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd8;
	}
	mov.f64 	%fd77, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r6}, %fd77;
	}
	and.b32  	%r7, %r6, 2146435072;
	setp.eq.s32 	%p7, %r7, 1073741824;
	abs.f64 	%fd9, %fd8;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd9;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd77;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd234, [retval0+0];
	} // callseq 2
	setp.lt.s32 	%p8, %r5, 0;
	and.pred  	%p1, %p8, %p7;
	not.pred 	%p9, %p1;
	@%p9 bra 	$L__BB7_6;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd234;
	}
	xor.b32  	%r35, %r34, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd234;
	}
	mov.b64 	%fd234, {%r36, %r35};

$L__BB7_6:
	setp.eq.f32 	%p10, %f4, 0f00000000;
	@%p10 bra 	$L__BB7_10;
	bra.uni 	$L__BB7_7;

$L__BB7_10:
	selp.b32 	%r37, %r5, 0, %p7;
	mov.u32 	%r38, 0;
	or.b32  	%r39, %r37, 2146435072;
	setp.lt.s32 	%p14, %r6, 0;
	selp.b32 	%r40, %r39, %r37, %p14;
	mov.b64 	%fd234, {%r38, %r40};
	bra.uni 	$L__BB7_11;

$L__BB7_7:
	setp.gt.s32 	%p11, %r5, -1;
	@%p11 bra 	$L__BB7_11;

	mov.f64 	%fd78, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd79, %fd78;
	setp.eq.f64 	%p12, %fd79, 0d4008000000000000;
	@%p12 bra 	$L__BB7_11;

	mov.f64 	%fd234, 0dFFF8000000000000;

$L__BB7_11:
	add.f64 	%fd15, %fd8, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd15;
	}
	and.b32  	%r42, %r41, 2146435072;
	setp.ne.s32 	%p15, %r42, 2146435072;
	mov.f64 	%fd235, %fd234;
	@%p15 bra 	$L__BB7_17;

	setp.gtu.f64 	%p16, %fd9, 0d7FF0000000000000;
	mov.f64 	%fd235, %fd15;
	@%p16 bra 	$L__BB7_17;

	mov.f64 	%fd81, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd81;
	}
	and.b32  	%r8, %r6, 2147483647;
	setp.eq.s32 	%p17, %r8, 2146435072;
	setp.eq.s32 	%p18, %r43, 0;
	and.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB7_16;
	bra.uni 	$L__BB7_14;

$L__BB7_16:
	setp.gt.f64 	%p26, %fd9, 0d3FF0000000000000;
	selp.b32 	%r50, 2146435072, 0, %p26;
	mov.u32 	%r51, 0;
	xor.b32  	%r52, %r50, 2146435072;
	setp.lt.s32 	%p27, %r6, 0;
	selp.b32 	%r53, %r52, %r50, %p27;
	setp.eq.f32 	%p28, %f4, 0fBF800000;
	selp.b32 	%r54, 1072693248, %r53, %p28;
	mov.b64 	%fd235, {%r51, %r54};
	bra.uni 	$L__BB7_17;

$L__BB7_14:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd8;
	}
	and.b32  	%r45, %r5, 2147483647;
	setp.ne.s32 	%p20, %r45, 2146435072;
	setp.ne.s32 	%p21, %r44, 0;
	or.pred  	%p22, %p20, %p21;
	mov.f64 	%fd235, %fd234;
	@%p22 bra 	$L__BB7_17;

	setp.gt.s32 	%p23, %r6, -1;
	selp.b32 	%r46, 2146435072, 0, %p23;
	mov.u32 	%r47, 0;
	setp.ne.s32 	%p24, %r8, 1071644672;
	and.pred  	%p25, %p24, %p1;
	or.b32  	%r48, %r46, -2147483648;
	selp.b32 	%r49, %r48, %r46, %p25;
	mov.b64 	%fd235, {%r47, %r49};

$L__BB7_17:
	mul.f64 	%fd82, %fd235, 0d3FA6E4E26D4801F7;
	setp.eq.f32 	%p29, %f4, 0f3F800000;
	selp.f64 	%fd83, 0d3FA6E4E26D4801F7, %fd82, %p29;
	add.f64 	%fd84, %fd83, %fd8;
	mul.f64 	%fd19, %fd84, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd19;
	}
	and.b32  	%r10, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd19;
	}
	mov.b64 	%fd20, {%r55, %r10};
	setp.ltu.f64 	%p30, %fd20, 0d3FE4F92224DD2F1A;
	@%p30 bra 	$L__BB7_19;
	bra.uni 	$L__BB7_18;

$L__BB7_19:
	mul.f64 	%fd126, %fd19, %fd19;
	mov.f64 	%fd127, 0d3F14359F420AFC3D;
	mov.f64 	%fd128, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd129, %fd128, %fd126, %fd127;
	mov.f64 	%fd130, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd131, %fd129, %fd126, %fd130;
	mov.f64 	%fd132, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd133, %fd131, %fd126, %fd132;
	mov.f64 	%fd134, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd135, %fd133, %fd126, %fd134;
	mov.f64 	%fd136, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd137, %fd135, %fd126, %fd136;
	mov.f64 	%fd138, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd139, %fd137, %fd126, %fd138;
	mov.f64 	%fd140, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd141, %fd139, %fd126, %fd140;
	mov.f64 	%fd142, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd143, %fd141, %fd126, %fd142;
	mov.f64 	%fd144, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd145, %fd143, %fd126, %fd144;
	mov.f64 	%fd146, 0dBFD5555555555550;
	fma.rn.f64 	%fd147, %fd145, %fd126, %fd146;
	mov.f64 	%fd148, 0d0000000000000000;
	fma.rn.f64 	%fd149, %fd147, %fd126, %fd148;
	fma.rn.f64 	%fd236, %fd149, %fd19, %fd19;
	bra.uni 	$L__BB7_20;

$L__BB7_18:
	add.f64 	%fd85, %fd20, %fd20;
	mov.f64 	%fd86, 0d4000000000000000;
	cvt.rn.f32.f64 	%f28, %fd85;
	mul.f32 	%f29, %f28, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f30, %f29;
	cvt.f64.f32 	%fd87, %f30;
	neg.f64 	%fd88, %fd87;
	mov.f64 	%fd89, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd90, %fd88, %fd89, %fd85;
	mov.f64 	%fd91, 0d3E928A27F89B6999;
	mov.f64 	%fd92, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd93, %fd92, %fd90, %fd91;
	mov.f64 	%fd94, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd95, %fd93, %fd90, %fd94;
	mov.f64 	%fd96, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd97, %fd95, %fd90, %fd96;
	mov.f64 	%fd98, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd99, %fd97, %fd90, %fd98;
	mov.f64 	%fd100, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd101, %fd99, %fd90, %fd100;
	mov.f64 	%fd102, 0d3F811111111173C4;
	fma.rn.f64 	%fd103, %fd101, %fd90, %fd102;
	mov.f64 	%fd104, 0d3FA555555555211A;
	fma.rn.f64 	%fd105, %fd103, %fd90, %fd104;
	mov.f64 	%fd106, 0d3FC5555555555540;
	fma.rn.f64 	%fd107, %fd105, %fd90, %fd106;
	mov.f64 	%fd108, 0d3FE0000000000005;
	fma.rn.f64 	%fd109, %fd107, %fd90, %fd108;
	mul.f64 	%fd110, %fd90, %fd109;
	fma.rn.f64 	%fd111, %fd110, %fd90, %fd90;
	ex2.approx.ftz.f32 	%f31, %f30;
	cvt.f64.f32 	%fd112, %f31;
	mov.f64 	%fd113, 0d3FF0000000000000;
	sub.f64 	%fd114, %fd113, %fd112;
	neg.f64 	%fd115, %fd111;
	fma.rn.f64 	%fd116, %fd115, %fd112, %fd114;
	sub.f64 	%fd117, %fd86, %fd116;
	rcp.approx.ftz.f64 	%fd118, %fd117;
	neg.f64 	%fd119, %fd117;
	fma.rn.f64 	%fd120, %fd119, %fd118, %fd113;
	fma.rn.f64 	%fd121, %fd120, %fd120, %fd120;
	fma.rn.f64 	%fd122, %fd121, %fd118, %fd118;
	neg.f64 	%fd123, %fd122;
	fma.rn.f64 	%fd124, %fd86, %fd123, %fd113;
	setp.gt.u32 	%p31, %r10, 1077088193;
	selp.f64 	%fd125, 0d3FF0000000000000, %fd124, %p31;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r56, %temp}, %fd125;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd125;
	}
	and.b32  	%r58, %r9, -2147483648;
	or.b32  	%r59, %r57, %r58;
	mov.b64 	%fd236, {%r56, %r59};

$L__BB7_20:
	add.f64 	%fd150, %fd236, 0d3FF0000000000000;
	mul.f64 	%fd151, %fd150, 0d3FE0000000000000;
	cvt.rn.f32.f64 	%f5, %fd151;
	add.f64 	%fd152, %fd7, 0dBF70101010101010;
	mul.f64 	%fd153, %fd152, %fd6;
	cvt.rn.f32.f64 	%f6, %fd153;
	cvt.f64.f32 	%fd24, %f6;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd24;
	}
	abs.f64 	%fd25, %fd24;
	mov.f64 	%fd154, 0d4008000000000000;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd25;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd154;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd238, [retval0+0];
	} // callseq 3
	setp.lt.s32 	%p32, %r11, 0;
	and.pred  	%p2, %p32, %p7;
	not.pred 	%p34, %p2;
	@%p34 bra 	$L__BB7_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd238;
	}
	xor.b32  	%r61, %r60, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r62, %temp}, %fd238;
	}
	mov.b64 	%fd238, {%r62, %r61};

$L__BB7_22:
	setp.eq.f32 	%p35, %f6, 0f00000000;
	@%p35 bra 	$L__BB7_26;
	bra.uni 	$L__BB7_23;

$L__BB7_26:
	selp.b32 	%r63, %r11, 0, %p7;
	mov.u32 	%r64, 0;
	or.b32  	%r65, %r63, 2146435072;
	setp.lt.s32 	%p39, %r6, 0;
	selp.b32 	%r66, %r65, %r63, %p39;
	mov.b64 	%fd238, {%r64, %r66};
	bra.uni 	$L__BB7_27;

$L__BB7_23:
	setp.gt.s32 	%p36, %r11, -1;
	@%p36 bra 	$L__BB7_27;

	mov.f64 	%fd155, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd156, %fd155;
	setp.eq.f64 	%p37, %fd156, 0d4008000000000000;
	@%p37 bra 	$L__BB7_27;

	mov.f64 	%fd238, 0dFFF8000000000000;

$L__BB7_27:
	add.f64 	%fd31, %fd24, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd31;
	}
	and.b32  	%r68, %r67, 2146435072;
	setp.ne.s32 	%p40, %r68, 2146435072;
	mov.f64 	%fd239, %fd238;
	@%p40 bra 	$L__BB7_33;

	setp.gtu.f64 	%p41, %fd25, 0d7FF0000000000000;
	mov.f64 	%fd239, %fd31;
	@%p41 bra 	$L__BB7_33;

	mov.f64 	%fd158, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r69, %temp}, %fd158;
	}
	and.b32  	%r12, %r6, 2147483647;
	setp.eq.s32 	%p42, %r12, 2146435072;
	setp.eq.s32 	%p43, %r69, 0;
	and.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB7_32;
	bra.uni 	$L__BB7_30;

$L__BB7_32:
	setp.gt.f64 	%p51, %fd25, 0d3FF0000000000000;
	selp.b32 	%r76, 2146435072, 0, %p51;
	mov.u32 	%r77, 0;
	xor.b32  	%r78, %r76, 2146435072;
	setp.lt.s32 	%p52, %r6, 0;
	selp.b32 	%r79, %r78, %r76, %p52;
	setp.eq.f32 	%p53, %f6, 0fBF800000;
	selp.b32 	%r80, 1072693248, %r79, %p53;
	mov.b64 	%fd239, {%r77, %r80};
	bra.uni 	$L__BB7_33;

$L__BB7_30:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r70, %temp}, %fd24;
	}
	and.b32  	%r71, %r11, 2147483647;
	setp.ne.s32 	%p45, %r71, 2146435072;
	setp.ne.s32 	%p46, %r70, 0;
	or.pred  	%p47, %p45, %p46;
	mov.f64 	%fd239, %fd238;
	@%p47 bra 	$L__BB7_33;

	setp.gt.s32 	%p48, %r6, -1;
	selp.b32 	%r72, 2146435072, 0, %p48;
	mov.u32 	%r73, 0;
	setp.ne.s32 	%p49, %r12, 1071644672;
	and.pred  	%p50, %p49, %p2;
	or.b32  	%r74, %r72, -2147483648;
	selp.b32 	%r75, %r74, %r72, %p50;
	mov.b64 	%fd239, {%r73, %r75};

$L__BB7_33:
	mul.f64 	%fd159, %fd239, 0d3FA6E4E26D4801F7;
	setp.eq.f32 	%p54, %f6, 0f3F800000;
	selp.f64 	%fd160, 0d3FA6E4E26D4801F7, %fd159, %p54;
	add.f64 	%fd161, %fd160, %fd24;
	mul.f64 	%fd35, %fd161, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd35;
	}
	and.b32  	%r14, %r13, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r81, %temp}, %fd35;
	}
	mov.b64 	%fd36, {%r81, %r14};
	setp.ltu.f64 	%p55, %fd36, 0d3FE4F92224DD2F1A;
	@%p55 bra 	$L__BB7_35;
	bra.uni 	$L__BB7_34;

$L__BB7_35:
	mul.f64 	%fd203, %fd35, %fd35;
	mov.f64 	%fd204, 0d3F14359F420AFC3D;
	mov.f64 	%fd205, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd206, %fd205, %fd203, %fd204;
	mov.f64 	%fd207, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd208, %fd206, %fd203, %fd207;
	mov.f64 	%fd209, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd210, %fd208, %fd203, %fd209;
	mov.f64 	%fd211, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd212, %fd210, %fd203, %fd211;
	mov.f64 	%fd213, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd214, %fd212, %fd203, %fd213;
	mov.f64 	%fd215, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd216, %fd214, %fd203, %fd215;
	mov.f64 	%fd217, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd218, %fd216, %fd203, %fd217;
	mov.f64 	%fd219, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd220, %fd218, %fd203, %fd219;
	mov.f64 	%fd221, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd222, %fd220, %fd203, %fd221;
	mov.f64 	%fd223, 0dBFD5555555555550;
	fma.rn.f64 	%fd224, %fd222, %fd203, %fd223;
	mov.f64 	%fd225, 0d0000000000000000;
	fma.rn.f64 	%fd226, %fd224, %fd203, %fd225;
	fma.rn.f64 	%fd240, %fd226, %fd35, %fd35;
	bra.uni 	$L__BB7_36;

$L__BB7_34:
	add.f64 	%fd162, %fd36, %fd36;
	mov.f64 	%fd163, 0d4000000000000000;
	cvt.rn.f32.f64 	%f32, %fd162;
	mul.f32 	%f33, %f32, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f34, %f33;
	cvt.f64.f32 	%fd164, %f34;
	neg.f64 	%fd165, %fd164;
	mov.f64 	%fd166, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd167, %fd165, %fd166, %fd162;
	mov.f64 	%fd168, 0d3E928A27F89B6999;
	mov.f64 	%fd169, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd170, %fd169, %fd167, %fd168;
	mov.f64 	%fd171, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd172, %fd170, %fd167, %fd171;
	mov.f64 	%fd173, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd174, %fd172, %fd167, %fd173;
	mov.f64 	%fd175, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd176, %fd174, %fd167, %fd175;
	mov.f64 	%fd177, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd178, %fd176, %fd167, %fd177;
	mov.f64 	%fd179, 0d3F811111111173C4;
	fma.rn.f64 	%fd180, %fd178, %fd167, %fd179;
	mov.f64 	%fd181, 0d3FA555555555211A;
	fma.rn.f64 	%fd182, %fd180, %fd167, %fd181;
	mov.f64 	%fd183, 0d3FC5555555555540;
	fma.rn.f64 	%fd184, %fd182, %fd167, %fd183;
	mov.f64 	%fd185, 0d3FE0000000000005;
	fma.rn.f64 	%fd186, %fd184, %fd167, %fd185;
	mul.f64 	%fd187, %fd167, %fd186;
	fma.rn.f64 	%fd188, %fd187, %fd167, %fd167;
	ex2.approx.ftz.f32 	%f35, %f34;
	cvt.f64.f32 	%fd189, %f35;
	mov.f64 	%fd190, 0d3FF0000000000000;
	sub.f64 	%fd191, %fd190, %fd189;
	neg.f64 	%fd192, %fd188;
	fma.rn.f64 	%fd193, %fd192, %fd189, %fd191;
	sub.f64 	%fd194, %fd163, %fd193;
	rcp.approx.ftz.f64 	%fd195, %fd194;
	neg.f64 	%fd196, %fd194;
	fma.rn.f64 	%fd197, %fd196, %fd195, %fd190;
	fma.rn.f64 	%fd198, %fd197, %fd197, %fd197;
	fma.rn.f64 	%fd199, %fd198, %fd195, %fd195;
	neg.f64 	%fd200, %fd199;
	fma.rn.f64 	%fd201, %fd163, %fd200, %fd190;
	setp.gt.u32 	%p56, %r14, 1077088193;
	selp.f64 	%fd202, 0d3FF0000000000000, %fd201, %p56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r82, %temp}, %fd202;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r83}, %fd202;
	}
	and.b32  	%r84, %r13, -2147483648;
	or.b32  	%r85, %r83, %r84;
	mov.b64 	%fd240, {%r82, %r85};

$L__BB7_36:
	max.f32 	%f36, %f5, 0f2B8CBCCC;
	setp.lt.f32 	%p57, %f36, 0f00800000;
	mul.f32 	%f37, %f36, 0f4B000000;
	selp.f32 	%f7, %f37, %f36, %p57;
	selp.f32 	%f38, 0fC1B80000, 0f00000000, %p57;
	mov.b32 	%r86, %f7;
	add.s32 	%r87, %r86, -1059760811;
	and.b32  	%r88, %r87, -8388608;
	sub.s32 	%r89, %r86, %r88;
	mov.b32 	%f39, %r89;
	cvt.rn.f32.s32 	%f40, %r88;
	mov.f32 	%f41, 0f34000000;
	fma.rn.f32 	%f42, %f40, %f41, %f38;
	add.f32 	%f43, %f39, 0fBF800000;
	mov.f32 	%f44, 0f3E1039F6;
	mov.f32 	%f45, 0fBE055027;
	fma.rn.f32 	%f46, %f45, %f43, %f44;
	mov.f32 	%f47, 0fBDF8CDCC;
	fma.rn.f32 	%f48, %f46, %f43, %f47;
	mov.f32 	%f49, 0f3E0F2955;
	fma.rn.f32 	%f50, %f48, %f43, %f49;
	mov.f32 	%f51, 0fBE2AD8B9;
	fma.rn.f32 	%f52, %f50, %f43, %f51;
	mov.f32 	%f53, 0f3E4CED0B;
	fma.rn.f32 	%f54, %f52, %f43, %f53;
	mov.f32 	%f55, 0fBE7FFF22;
	fma.rn.f32 	%f56, %f54, %f43, %f55;
	mov.f32 	%f57, 0f3EAAAA78;
	fma.rn.f32 	%f58, %f56, %f43, %f57;
	mov.f32 	%f59, 0fBF000000;
	fma.rn.f32 	%f60, %f58, %f43, %f59;
	mul.f32 	%f61, %f43, %f60;
	fma.rn.f32 	%f62, %f61, %f43, %f43;
	mov.f32 	%f63, 0f3F317218;
	fma.rn.f32 	%f127, %f42, %f63, %f62;
	setp.lt.u32 	%p58, %r86, 2139095040;
	@%p58 bra 	$L__BB7_38;

	mov.f32 	%f64, 0f7F800000;
	fma.rn.f32 	%f127, %f7, %f64, %f64;

$L__BB7_38:
	setp.eq.f32 	%p59, %f7, 0f00000000;
	selp.f32 	%f130, 0fFF800000, %f127, %p59;
	setp.lt.f32 	%p60, %f2, 0fBF7FBE77;
	@%p60 bra 	$L__BB7_45;

	add.f64 	%fd227, %fd240, 0d3FF0000000000000;
	mul.f64 	%fd228, %fd227, 0d3FE0000000000000;
	cvt.rn.f32.f64 	%f65, %fd228;
	mov.f32 	%f66, 0f3F800000;
	sub.f32 	%f67, %f66, %f65;
	setp.lt.f32 	%p61, %f67, 0f2B8CBCCC;
	selp.f32 	%f68, 0f2B8CBCCC, %f67, %p61;
	sub.f32 	%f12, %f5, %f65;
	setp.lt.f32 	%p62, %f68, 0f00800000;
	mul.f32 	%f69, %f68, 0f4B000000;
	selp.f32 	%f13, %f69, %f68, %p62;
	selp.f32 	%f70, 0fC1B80000, 0f00000000, %p62;
	mov.b32 	%r90, %f13;
	add.s32 	%r91, %r90, -1059760811;
	and.b32  	%r92, %r91, -8388608;
	sub.s32 	%r93, %r90, %r92;
	mov.b32 	%f71, %r93;
	cvt.rn.f32.s32 	%f72, %r92;
	mov.f32 	%f73, 0f34000000;
	fma.rn.f32 	%f74, %f72, %f73, %f70;
	add.f32 	%f75, %f71, 0fBF800000;
	mov.f32 	%f76, 0f3E1039F6;
	mov.f32 	%f77, 0fBE055027;
	fma.rn.f32 	%f78, %f77, %f75, %f76;
	mov.f32 	%f79, 0fBDF8CDCC;
	fma.rn.f32 	%f80, %f78, %f75, %f79;
	mov.f32 	%f81, 0f3E0F2955;
	fma.rn.f32 	%f82, %f80, %f75, %f81;
	mov.f32 	%f83, 0fBE2AD8B9;
	fma.rn.f32 	%f84, %f82, %f75, %f83;
	mov.f32 	%f85, 0f3E4CED0B;
	fma.rn.f32 	%f86, %f84, %f75, %f85;
	mov.f32 	%f87, 0fBE7FFF22;
	fma.rn.f32 	%f88, %f86, %f75, %f87;
	mov.f32 	%f89, 0f3EAAAA78;
	fma.rn.f32 	%f90, %f88, %f75, %f89;
	mov.f32 	%f91, 0fBF000000;
	fma.rn.f32 	%f92, %f90, %f75, %f91;
	mul.f32 	%f93, %f75, %f92;
	fma.rn.f32 	%f94, %f93, %f75, %f75;
	mov.f32 	%f95, 0f3F317218;
	fma.rn.f32 	%f128, %f74, %f95, %f94;
	setp.lt.u32 	%p63, %r90, 2139095040;
	@%p63 bra 	$L__BB7_41;

	mov.f32 	%f96, 0f7F800000;
	fma.rn.f32 	%f128, %f13, %f96, %f96;

$L__BB7_41:
	setp.eq.f32 	%p64, %f13, 0f00000000;
	selp.f32 	%f130, 0fFF800000, %f128, %p64;
	cvt.f64.f32 	%fd229, %f2;
	setp.gt.f64 	%p65, %fd229, 0d3FEFF7CED916872B;
	@%p65 bra 	$L__BB7_45;

	setp.lt.f32 	%p66, %f12, 0f2B8CBCCC;
	selp.f32 	%f97, 0f2B8CBCCC, %f12, %p66;
	setp.lt.f32 	%p67, %f97, 0f00800000;
	mul.f32 	%f98, %f97, 0f4B000000;
	selp.f32 	%f18, %f98, %f97, %p67;
	selp.f32 	%f99, 0fC1B80000, 0f00000000, %p67;
	mov.b32 	%r94, %f18;
	add.s32 	%r95, %r94, -1059760811;
	and.b32  	%r96, %r95, -8388608;
	sub.s32 	%r97, %r94, %r96;
	mov.b32 	%f100, %r97;
	cvt.rn.f32.s32 	%f101, %r96;
	mov.f32 	%f102, 0f34000000;
	fma.rn.f32 	%f103, %f101, %f102, %f99;
	add.f32 	%f104, %f100, 0fBF800000;
	mov.f32 	%f105, 0f3E1039F6;
	mov.f32 	%f106, 0fBE055027;
	fma.rn.f32 	%f107, %f106, %f104, %f105;
	mov.f32 	%f108, 0fBDF8CDCC;
	fma.rn.f32 	%f109, %f107, %f104, %f108;
	mov.f32 	%f110, 0f3E0F2955;
	fma.rn.f32 	%f111, %f109, %f104, %f110;
	mov.f32 	%f112, 0fBE2AD8B9;
	fma.rn.f32 	%f113, %f111, %f104, %f112;
	mov.f32 	%f114, 0f3E4CED0B;
	fma.rn.f32 	%f115, %f113, %f104, %f114;
	mov.f32 	%f116, 0fBE7FFF22;
	fma.rn.f32 	%f117, %f115, %f104, %f116;
	mov.f32 	%f118, 0f3EAAAA78;
	fma.rn.f32 	%f119, %f117, %f104, %f118;
	mov.f32 	%f120, 0fBF000000;
	fma.rn.f32 	%f121, %f119, %f104, %f120;
	mul.f32 	%f122, %f104, %f121;
	fma.rn.f32 	%f123, %f122, %f104, %f104;
	mov.f32 	%f124, 0f3F317218;
	fma.rn.f32 	%f129, %f103, %f124, %f123;
	setp.lt.u32 	%p68, %r94, 2139095040;
	@%p68 bra 	$L__BB7_44;

	mov.f32 	%f125, 0f7F800000;
	fma.rn.f32 	%f129, %f18, %f125, %f125;

$L__BB7_44:
	setp.eq.f32 	%p69, %f18, 0f00000000;
	selp.f32 	%f130, 0fFF800000, %f129, %p69;

$L__BB7_45:
	cvt.f64.f32 	%fd230, %f130;
	div.rn.f64 	%fd231, %fd230, 0dBFE62E42FEFA39EF;
	cvt.rn.f32.f64 	%f126, %fd231;
	cvta.to.global.u64 	%rd13, %rd5;
	shl.b64 	%rd14, %rd1, 2;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f32 	[%rd15], %f126;

$L__BB7_46:
	ret;

}
	// .globl	discretized_gaussian_log_likelihood_back
.visible .entry discretized_gaussian_log_likelihood_back(
	.param .u64 discretized_gaussian_log_likelihood_back_param_0,
	.param .u64 discretized_gaussian_log_likelihood_back_param_1,
	.param .u64 discretized_gaussian_log_likelihood_back_param_2,
	.param .u64 discretized_gaussian_log_likelihood_back_param_3,
	.param .u32 discretized_gaussian_log_likelihood_back_param_4
)
{
	.reg .pred 	%p<251>;
	.reg .f32 	%f<98>;
	.reg .b32 	%r<283>;
	.reg .f64 	%fd<671>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [discretized_gaussian_log_likelihood_back_param_0];
	ld.param.u64 	%rd3, [discretized_gaussian_log_likelihood_back_param_1];
	ld.param.u64 	%rd4, [discretized_gaussian_log_likelihood_back_param_2];
	ld.param.u32 	%r36, [discretized_gaussian_log_likelihood_back_param_4];
	mov.u32 	%r37, %nctaid.x;
	mov.u32 	%r38, %ctaid.y;
	mov.u32 	%r39, %ctaid.x;
	mad.lo.s32 	%r40, %r38, %r37, %r39;
	mov.u32 	%r41, %ntid.x;
	mov.u32 	%r42, %tid.x;
	mad.lo.s32 	%r1, %r40, %r41, %r42;
	setp.ge.s32 	%p7, %r1, %r36;
	@%p7 bra 	$L__BB8_161;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f32 	%f17, [%rd10];
	ld.global.f32 	%f1, [%rd8];
	sub.f32 	%f18, %f1, %f17;
	cvta.to.global.u64 	%rd11, %rd4;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f32 	%f19, [%rd12];
	mul.f32 	%f20, %f19, 0fBF000000;
	mov.f32 	%f21, 0f3F000000;
	mov.f32 	%f22, 0f3BBB989D;
	fma.rn.f32 	%f23, %f20, %f22, %f21;
	mov.f32 	%f24, 0f3FB8AA3B;
	mov.f32 	%f25, 0f437C0000;
	cvt.sat.f32.f32 	%f26, %f23;
	mov.f32 	%f27, 0f4B400001;
	fma.rm.f32 	%f28, %f26, %f25, %f27;
	add.f32 	%f29, %f28, 0fCB40007F;
	neg.f32 	%f30, %f29;
	fma.rn.f32 	%f31, %f20, %f24, %f30;
	mov.f32 	%f32, 0f32A57060;
	fma.rn.f32 	%f33, %f20, %f32, %f31;
	mov.b32 	%r43, %f28;
	shl.b32 	%r44, %r43, 23;
	mov.b32 	%f34, %r44;
	ex2.approx.ftz.f32 	%f35, %f33;
	mul.f32 	%f2, %f35, %f34;
	cvt.f64.f32 	%fd1, %f2;
	cvt.f64.f32 	%fd2, %f18;
	add.f64 	%fd3, %fd2, 0d3F70101010101010;
	mul.f64 	%fd128, %fd3, %fd1;
	cvt.rn.f32.f64 	%f3, %fd128;
	cvt.f64.f32 	%fd4, %f3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd4;
	}
	mov.f64 	%fd129, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd129;
	}
	and.b32  	%r4, %r3, 2146435072;
	setp.eq.s32 	%p8, %r4, 1073741824;
	abs.f64 	%fd5, %fd4;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd5;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd129;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd642, [retval0+0];
	} // callseq 4
	setp.lt.s32 	%p9, %r2, 0;
	and.pred  	%p1, %p9, %p8;
	not.pred 	%p10, %p1;
	mov.f64 	%fd635, %fd642;
	@%p10 bra 	$L__BB8_3;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd642;
	}
	xor.b32  	%r46, %r45, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd642;
	}
	mov.b64 	%fd635, {%r47, %r46};

$L__BB8_3:
	setp.eq.f32 	%p11, %f3, 0f00000000;
	@%p11 bra 	$L__BB8_7;
	bra.uni 	$L__BB8_4;

$L__BB8_7:
	selp.b32 	%r48, %r2, 0, %p8;
	mov.u32 	%r49, 0;
	or.b32  	%r50, %r48, 2146435072;
	setp.lt.s32 	%p15, %r3, 0;
	selp.b32 	%r51, %r50, %r48, %p15;
	mov.b64 	%fd635, {%r49, %r51};
	bra.uni 	$L__BB8_8;

$L__BB8_4:
	setp.gt.s32 	%p12, %r2, -1;
	@%p12 bra 	$L__BB8_8;

	mov.f64 	%fd130, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd131, %fd130;
	setp.eq.f64 	%p13, %fd131, 0d4008000000000000;
	@%p13 bra 	$L__BB8_8;

	mov.f64 	%fd635, 0dFFF8000000000000;

$L__BB8_8:
	add.f64 	%fd11, %fd4, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd11;
	}
	and.b32  	%r5, %r52, 2146435072;
	setp.ne.s32 	%p16, %r5, 2146435072;
	mov.f64 	%fd636, %fd635;
	@%p16 bra 	$L__BB8_14;

	setp.gtu.f64 	%p17, %fd5, 0d7FF0000000000000;
	mov.f64 	%fd636, %fd11;
	@%p17 bra 	$L__BB8_14;

	mov.f64 	%fd133, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd133;
	}
	and.b32  	%r6, %r3, 2147483647;
	setp.eq.s32 	%p18, %r6, 2146435072;
	setp.eq.s32 	%p19, %r53, 0;
	and.pred  	%p20, %p18, %p19;
	@%p20 bra 	$L__BB8_13;
	bra.uni 	$L__BB8_11;

$L__BB8_13:
	setp.gt.f64 	%p27, %fd5, 0d3FF0000000000000;
	selp.b32 	%r60, 2146435072, 0, %p27;
	mov.u32 	%r61, 0;
	xor.b32  	%r62, %r60, 2146435072;
	setp.lt.s32 	%p28, %r3, 0;
	selp.b32 	%r63, %r62, %r60, %p28;
	setp.eq.f32 	%p29, %f3, 0fBF800000;
	selp.b32 	%r64, 1072693248, %r63, %p29;
	mov.b64 	%fd636, {%r61, %r64};
	bra.uni 	$L__BB8_14;

$L__BB8_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd4;
	}
	and.b32  	%r55, %r2, 2147483647;
	setp.ne.s32 	%p21, %r55, 2146435072;
	setp.ne.s32 	%p22, %r54, 0;
	or.pred  	%p23, %p21, %p22;
	mov.f64 	%fd636, %fd635;
	@%p23 bra 	$L__BB8_14;

	setp.gt.s32 	%p24, %r3, -1;
	selp.b32 	%r56, 2146435072, 0, %p24;
	mov.u32 	%r57, 0;
	setp.ne.s32 	%p25, %r6, 1071644672;
	and.pred  	%p26, %p25, %p1;
	or.b32  	%r58, %r56, -2147483648;
	selp.b32 	%r59, %r58, %r56, %p26;
	mov.b64 	%fd636, {%r57, %r59};

$L__BB8_14:
	mul.f64 	%fd134, %fd636, 0d3FA6E4E26D4801F7;
	setp.eq.f32 	%p30, %f3, 0f3F800000;
	selp.f64 	%fd135, 0d3FA6E4E26D4801F7, %fd134, %p30;
	add.f64 	%fd136, %fd135, %fd4;
	mul.f64 	%fd15, %fd136, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd15;
	}
	and.b32  	%r8, %r7, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r65, %temp}, %fd15;
	}
	mov.b64 	%fd16, {%r65, %r8};
	setp.ltu.f64 	%p31, %fd16, 0d3FE4F92224DD2F1A;
	@%p31 bra 	$L__BB8_16;
	bra.uni 	$L__BB8_15;

$L__BB8_16:
	mul.f64 	%fd178, %fd15, %fd15;
	mov.f64 	%fd179, 0d3F14359F420AFC3D;
	mov.f64 	%fd180, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd181, %fd180, %fd178, %fd179;
	mov.f64 	%fd182, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd183, %fd181, %fd178, %fd182;
	mov.f64 	%fd184, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd185, %fd183, %fd178, %fd184;
	mov.f64 	%fd186, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd187, %fd185, %fd178, %fd186;
	mov.f64 	%fd188, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd189, %fd187, %fd178, %fd188;
	mov.f64 	%fd190, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd191, %fd189, %fd178, %fd190;
	mov.f64 	%fd192, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd193, %fd191, %fd178, %fd192;
	mov.f64 	%fd194, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd195, %fd193, %fd178, %fd194;
	mov.f64 	%fd196, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd197, %fd195, %fd178, %fd196;
	mov.f64 	%fd198, 0dBFD5555555555550;
	fma.rn.f64 	%fd199, %fd197, %fd178, %fd198;
	mov.f64 	%fd200, 0d0000000000000000;
	fma.rn.f64 	%fd201, %fd199, %fd178, %fd200;
	fma.rn.f64 	%fd637, %fd201, %fd15, %fd15;
	bra.uni 	$L__BB8_17;

$L__BB8_15:
	add.f64 	%fd137, %fd16, %fd16;
	mov.f64 	%fd138, 0d4000000000000000;
	cvt.rn.f32.f64 	%f36, %fd137;
	mul.f32 	%f37, %f36, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f38, %f37;
	cvt.f64.f32 	%fd139, %f38;
	neg.f64 	%fd140, %fd139;
	mov.f64 	%fd141, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd142, %fd140, %fd141, %fd137;
	mov.f64 	%fd143, 0d3E928A27F89B6999;
	mov.f64 	%fd144, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd145, %fd144, %fd142, %fd143;
	mov.f64 	%fd146, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd147, %fd145, %fd142, %fd146;
	mov.f64 	%fd148, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd149, %fd147, %fd142, %fd148;
	mov.f64 	%fd150, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd151, %fd149, %fd142, %fd150;
	mov.f64 	%fd152, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd153, %fd151, %fd142, %fd152;
	mov.f64 	%fd154, 0d3F811111111173C4;
	fma.rn.f64 	%fd155, %fd153, %fd142, %fd154;
	mov.f64 	%fd156, 0d3FA555555555211A;
	fma.rn.f64 	%fd157, %fd155, %fd142, %fd156;
	mov.f64 	%fd158, 0d3FC5555555555540;
	fma.rn.f64 	%fd159, %fd157, %fd142, %fd158;
	mov.f64 	%fd160, 0d3FE0000000000005;
	fma.rn.f64 	%fd161, %fd159, %fd142, %fd160;
	mul.f64 	%fd162, %fd142, %fd161;
	fma.rn.f64 	%fd163, %fd162, %fd142, %fd142;
	ex2.approx.ftz.f32 	%f39, %f38;
	cvt.f64.f32 	%fd164, %f39;
	mov.f64 	%fd165, 0d3FF0000000000000;
	sub.f64 	%fd166, %fd165, %fd164;
	neg.f64 	%fd167, %fd163;
	fma.rn.f64 	%fd168, %fd167, %fd164, %fd166;
	sub.f64 	%fd169, %fd138, %fd168;
	rcp.approx.ftz.f64 	%fd170, %fd169;
	neg.f64 	%fd171, %fd169;
	fma.rn.f64 	%fd172, %fd171, %fd170, %fd165;
	fma.rn.f64 	%fd173, %fd172, %fd172, %fd172;
	fma.rn.f64 	%fd174, %fd173, %fd170, %fd170;
	neg.f64 	%fd175, %fd174;
	fma.rn.f64 	%fd176, %fd138, %fd175, %fd165;
	setp.gt.u32 	%p32, %r8, 1077088193;
	selp.f64 	%fd177, 0d3FF0000000000000, %fd176, %p32;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r66, %temp}, %fd177;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd177;
	}
	and.b32  	%r68, %r7, -2147483648;
	or.b32  	%r69, %r67, %r68;
	mov.b64 	%fd637, {%r66, %r69};

$L__BB8_17:
	sub.f32 	%f94, %f1, %f17;
	cvt.f64.f32 	%fd633, %f94;
	add.f64 	%fd202, %fd637, 0d3FF0000000000000;
	mul.f64 	%fd203, %fd202, 0d3FE0000000000000;
	cvt.rn.f32.f64 	%f4, %fd203;
	add.f64 	%fd20, %fd633, 0dBF70101010101010;
	mul.f64 	%fd204, %fd20, %fd1;
	cvt.rn.f32.f64 	%f5, %fd204;
	cvt.f64.f32 	%fd21, %f5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd21;
	}
	abs.f64 	%fd22, %fd21;
	mov.f64 	%fd205, 0d4008000000000000;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd22;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd205;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd649, [retval0+0];
	} // callseq 5
	setp.lt.s32 	%p33, %r9, 0;
	and.pred  	%p2, %p33, %p8;
	not.pred 	%p35, %p2;
	mov.f64 	%fd639, %fd649;
	@%p35 bra 	$L__BB8_19;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd649;
	}
	xor.b32  	%r71, %r70, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r72, %temp}, %fd649;
	}
	mov.b64 	%fd639, {%r72, %r71};

$L__BB8_19:
	setp.eq.f32 	%p36, %f5, 0f00000000;
	@%p36 bra 	$L__BB8_23;
	bra.uni 	$L__BB8_20;

$L__BB8_23:
	selp.b32 	%r73, %r9, 0, %p8;
	mov.u32 	%r74, 0;
	or.b32  	%r75, %r73, 2146435072;
	setp.lt.s32 	%p40, %r3, 0;
	selp.b32 	%r76, %r75, %r73, %p40;
	mov.b64 	%fd639, {%r74, %r76};
	bra.uni 	$L__BB8_24;

$L__BB8_20:
	setp.gt.s32 	%p37, %r9, -1;
	@%p37 bra 	$L__BB8_24;

	mov.f64 	%fd206, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd207, %fd206;
	setp.eq.f64 	%p38, %fd207, 0d4008000000000000;
	@%p38 bra 	$L__BB8_24;

	mov.f64 	%fd639, 0dFFF8000000000000;

$L__BB8_24:
	add.f64 	%fd28, %fd21, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd28;
	}
	and.b32  	%r10, %r77, 2146435072;
	setp.ne.s32 	%p41, %r10, 2146435072;
	mov.f64 	%fd640, %fd639;
	@%p41 bra 	$L__BB8_30;

	setp.gtu.f64 	%p42, %fd22, 0d7FF0000000000000;
	mov.f64 	%fd640, %fd28;
	@%p42 bra 	$L__BB8_30;

	mov.f64 	%fd209, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r78, %temp}, %fd209;
	}
	and.b32  	%r11, %r3, 2147483647;
	setp.eq.s32 	%p43, %r11, 2146435072;
	setp.eq.s32 	%p44, %r78, 0;
	and.pred  	%p45, %p43, %p44;
	@%p45 bra 	$L__BB8_29;
	bra.uni 	$L__BB8_27;

$L__BB8_29:
	setp.gt.f64 	%p52, %fd22, 0d3FF0000000000000;
	selp.b32 	%r85, 2146435072, 0, %p52;
	mov.u32 	%r86, 0;
	xor.b32  	%r87, %r85, 2146435072;
	setp.lt.s32 	%p53, %r3, 0;
	selp.b32 	%r88, %r87, %r85, %p53;
	setp.eq.f32 	%p54, %f5, 0fBF800000;
	selp.b32 	%r89, 1072693248, %r88, %p54;
	mov.b64 	%fd640, {%r86, %r89};
	bra.uni 	$L__BB8_30;

$L__BB8_27:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r79, %temp}, %fd21;
	}
	and.b32  	%r80, %r9, 2147483647;
	setp.ne.s32 	%p46, %r80, 2146435072;
	setp.ne.s32 	%p47, %r79, 0;
	or.pred  	%p48, %p46, %p47;
	mov.f64 	%fd640, %fd639;
	@%p48 bra 	$L__BB8_30;

	setp.gt.s32 	%p49, %r3, -1;
	selp.b32 	%r81, 2146435072, 0, %p49;
	mov.u32 	%r82, 0;
	setp.ne.s32 	%p50, %r11, 1071644672;
	and.pred  	%p51, %p50, %p2;
	or.b32  	%r83, %r81, -2147483648;
	selp.b32 	%r84, %r83, %r81, %p51;
	mov.b64 	%fd640, {%r82, %r84};

$L__BB8_30:
	mul.f64 	%fd210, %fd640, 0d3FA6E4E26D4801F7;
	setp.eq.f32 	%p55, %f5, 0f3F800000;
	selp.f64 	%fd211, 0d3FA6E4E26D4801F7, %fd210, %p55;
	add.f64 	%fd212, %fd211, %fd21;
	mul.f64 	%fd32, %fd212, 0d3FE9884533D43651;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd32;
	}
	and.b32  	%r13, %r12, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r90, %temp}, %fd32;
	}
	mov.b64 	%fd33, {%r90, %r13};
	setp.ltu.f64 	%p56, %fd33, 0d3FE4F92224DD2F1A;
	@%p56 bra 	$L__BB8_32;
	bra.uni 	$L__BB8_31;

$L__BB8_32:
	mul.f64 	%fd254, %fd32, %fd32;
	mov.f64 	%fd255, 0d3F14359F420AFC3D;
	mov.f64 	%fd256, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd257, %fd256, %fd254, %fd255;
	mov.f64 	%fd258, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd259, %fd257, %fd254, %fd258;
	mov.f64 	%fd260, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd261, %fd259, %fd254, %fd260;
	mov.f64 	%fd262, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd263, %fd261, %fd254, %fd262;
	mov.f64 	%fd264, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd265, %fd263, %fd254, %fd264;
	mov.f64 	%fd266, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd267, %fd265, %fd254, %fd266;
	mov.f64 	%fd268, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd269, %fd267, %fd254, %fd268;
	mov.f64 	%fd270, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd271, %fd269, %fd254, %fd270;
	mov.f64 	%fd272, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd273, %fd271, %fd254, %fd272;
	mov.f64 	%fd274, 0dBFD5555555555550;
	fma.rn.f64 	%fd275, %fd273, %fd254, %fd274;
	mov.f64 	%fd276, 0d0000000000000000;
	fma.rn.f64 	%fd277, %fd275, %fd254, %fd276;
	fma.rn.f64 	%fd641, %fd277, %fd32, %fd32;
	bra.uni 	$L__BB8_33;

$L__BB8_31:
	add.f64 	%fd213, %fd33, %fd33;
	mov.f64 	%fd214, 0d4000000000000000;
	cvt.rn.f32.f64 	%f40, %fd213;
	mul.f32 	%f41, %f40, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f42, %f41;
	cvt.f64.f32 	%fd215, %f42;
	neg.f64 	%fd216, %fd215;
	mov.f64 	%fd217, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd218, %fd216, %fd217, %fd213;
	mov.f64 	%fd219, 0d3E928A27F89B6999;
	mov.f64 	%fd220, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd221, %fd220, %fd218, %fd219;
	mov.f64 	%fd222, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd223, %fd221, %fd218, %fd222;
	mov.f64 	%fd224, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd225, %fd223, %fd218, %fd224;
	mov.f64 	%fd226, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd227, %fd225, %fd218, %fd226;
	mov.f64 	%fd228, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd229, %fd227, %fd218, %fd228;
	mov.f64 	%fd230, 0d3F811111111173C4;
	fma.rn.f64 	%fd231, %fd229, %fd218, %fd230;
	mov.f64 	%fd232, 0d3FA555555555211A;
	fma.rn.f64 	%fd233, %fd231, %fd218, %fd232;
	mov.f64 	%fd234, 0d3FC5555555555540;
	fma.rn.f64 	%fd235, %fd233, %fd218, %fd234;
	mov.f64 	%fd236, 0d3FE0000000000005;
	fma.rn.f64 	%fd237, %fd235, %fd218, %fd236;
	mul.f64 	%fd238, %fd218, %fd237;
	fma.rn.f64 	%fd239, %fd238, %fd218, %fd218;
	ex2.approx.ftz.f32 	%f43, %f42;
	cvt.f64.f32 	%fd240, %f43;
	mov.f64 	%fd241, 0d3FF0000000000000;
	sub.f64 	%fd242, %fd241, %fd240;
	neg.f64 	%fd243, %fd239;
	fma.rn.f64 	%fd244, %fd243, %fd240, %fd242;
	sub.f64 	%fd245, %fd214, %fd244;
	rcp.approx.ftz.f64 	%fd246, %fd245;
	neg.f64 	%fd247, %fd245;
	fma.rn.f64 	%fd248, %fd247, %fd246, %fd241;
	fma.rn.f64 	%fd249, %fd248, %fd248, %fd248;
	fma.rn.f64 	%fd250, %fd249, %fd246, %fd246;
	neg.f64 	%fd251, %fd250;
	fma.rn.f64 	%fd252, %fd214, %fd251, %fd241;
	setp.gt.u32 	%p57, %r13, 1077088193;
	selp.f64 	%fd253, 0d3FF0000000000000, %fd252, %p57;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r91, %temp}, %fd253;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r92}, %fd253;
	}
	and.b32  	%r93, %r12, -2147483648;
	or.b32  	%r94, %r92, %r93;
	mov.b64 	%fd641, {%r91, %r94};

$L__BB8_33:
	ld.param.u32 	%r275, [discretized_gaussian_log_likelihood_back_param_4];
	add.f64 	%fd278, %fd641, 0d3FF0000000000000;
	mul.f64 	%fd279, %fd278, 0d3FE0000000000000;
	cvt.rn.f32.f64 	%f6, %fd279;
	sub.f32 	%f7, %f4, %f6;
	cvt.rn.f32.s32 	%f44, %r275;
	mov.f32 	%f45, 0fBF800000;
	div.rn.f32 	%f46, %f45, %f44;
	div.rn.f32 	%f8, %f46, 0f3F317218;
	setp.lt.f32 	%p58, %f1, 0fBF7FBE77;
	@%p58 bra 	$L__BB8_128;
	bra.uni 	$L__BB8_34;

$L__BB8_128:
	setp.leu.f32 	%p200, %f4, 0f2B8CBCCC;
	mov.f64 	%fd663, 0d0000000000000000;
	@%p200 bra 	$L__BB8_130;

	div.rn.f32 	%f81, %f8, %f4;
	cvt.f64.f32 	%fd547, %f81;
	mul.f64 	%fd663, %fd547, 0d3FE0000000000000;

$L__BB8_130:
	@%p10 bra 	$L__BB8_132;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r230}, %fd642;
	}
	xor.b32  	%r231, %r230, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r232, %temp}, %fd642;
	}
	mov.b64 	%fd642, {%r232, %r231};

$L__BB8_132:
	@%p11 bra 	$L__BB8_136;
	bra.uni 	$L__BB8_133;

$L__BB8_136:
	selp.b32 	%r233, %r2, 0, %p8;
	mov.u32 	%r234, 0;
	or.b32  	%r235, %r233, 2146435072;
	setp.lt.s32 	%p206, %r3, 0;
	selp.b32 	%r236, %r235, %r233, %p206;
	mov.b64 	%fd642, {%r234, %r236};
	bra.uni 	$L__BB8_137;

$L__BB8_34:
	cvt.f64.f32 	%fd280, %f1;
	setp.gt.f64 	%p59, %fd280, 0d3FEFF7CED916872B;
	@%p59 bra 	$L__BB8_96;
	bra.uni 	$L__BB8_35;

$L__BB8_96:
	cvt.f64.f32 	%fd455, %f6;
	mov.f64 	%fd456, 0d3FF0000000000000;
	sub.f64 	%fd81, %fd456, %fd455;
	setp.leu.f64 	%p153, %fd81, 0d3D71979980000000;
	mov.f32 	%f96, 0f00000000;
	@%p153 bra 	$L__BB8_98;

	cvt.f64.f32 	%fd457, %f8;
	div.rn.f64 	%fd458, %fd457, %fd81;
	cvt.rn.f32.f64 	%f96, %fd458;

$L__BB8_98:
	@%p35 bra 	$L__BB8_100;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r185}, %fd649;
	}
	xor.b32  	%r186, %r185, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r187, %temp}, %fd649;
	}
	mov.b64 	%fd649, {%r187, %r186};

$L__BB8_100:
	@%p36 bra 	$L__BB8_104;
	bra.uni 	$L__BB8_101;

$L__BB8_104:
	selp.b32 	%r188, %r9, 0, %p8;
	mov.u32 	%r189, 0;
	or.b32  	%r190, %r188, 2146435072;
	setp.lt.s32 	%p159, %r3, 0;
	selp.b32 	%r191, %r190, %r188, %p159;
	mov.b64 	%fd649, {%r189, %r191};
	bra.uni 	$L__BB8_105;

$L__BB8_35:
	setp.leu.f32 	%p60, %f7, 0f2B8CBCCC;
	mov.f32 	%f95, 0f00000000;
	@%p60 bra 	$L__BB8_37;

	div.rn.f32 	%f95, %f8, %f7;

$L__BB8_37:
	@%p10 bra 	$L__BB8_39;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r95}, %fd642;
	}
	xor.b32  	%r96, %r95, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r97, %temp}, %fd642;
	}
	mov.b64 	%fd642, {%r97, %r96};

$L__BB8_39:
	@%p11 bra 	$L__BB8_43;
	bra.uni 	$L__BB8_40;

$L__BB8_43:
	selp.b32 	%r98, %r2, 0, %p8;
	mov.u32 	%r99, 0;
	or.b32  	%r100, %r98, 2146435072;
	setp.lt.s32 	%p66, %r3, 0;
	selp.b32 	%r101, %r100, %r98, %p66;
	mov.b64 	%fd642, {%r99, %r101};
	bra.uni 	$L__BB8_44;

$L__BB8_133:
	setp.gt.s32 	%p203, %r2, -1;
	@%p203 bra 	$L__BB8_137;

	mov.f64 	%fd548, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd549, %fd548;
	setp.eq.f64 	%p204, %fd549, 0d4008000000000000;
	@%p204 bra 	$L__BB8_137;

	mov.f64 	%fd642, 0dFFF8000000000000;

$L__BB8_137:
	mov.f64 	%fd666, %fd642;
	@%p16 bra 	$L__BB8_143;

	setp.gtu.f64 	%p208, %fd5, 0d7FF0000000000000;
	mov.f64 	%fd666, %fd11;
	@%p208 bra 	$L__BB8_143;

	mov.f64 	%fd551, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r237, %temp}, %fd551;
	}
	and.b32  	%r30, %r3, 2147483647;
	setp.eq.s32 	%p209, %r30, 2146435072;
	setp.eq.s32 	%p210, %r237, 0;
	and.pred  	%p211, %p209, %p210;
	@%p211 bra 	$L__BB8_142;
	bra.uni 	$L__BB8_140;

$L__BB8_142:
	setp.gt.f64 	%p218, %fd5, 0d3FF0000000000000;
	selp.b32 	%r244, 2146435072, 0, %p218;
	mov.u32 	%r245, 0;
	xor.b32  	%r246, %r244, 2146435072;
	setp.lt.s32 	%p219, %r3, 0;
	selp.b32 	%r247, %r246, %r244, %p219;
	setp.eq.f32 	%p220, %f3, 0fBF800000;
	selp.b32 	%r248, 1072693248, %r247, %p220;
	mov.b64 	%fd666, {%r245, %r248};
	bra.uni 	$L__BB8_143;

$L__BB8_101:
	setp.gt.s32 	%p156, %r9, -1;
	@%p156 bra 	$L__BB8_105;

	mov.f64 	%fd459, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd460, %fd459;
	setp.eq.f64 	%p157, %fd460, 0d4008000000000000;
	@%p157 bra 	$L__BB8_105;

	mov.f64 	%fd649, 0dFFF8000000000000;

$L__BB8_105:
	mov.f64 	%fd658, %fd649;
	@%p41 bra 	$L__BB8_111;

	setp.gtu.f64 	%p161, %fd22, 0d7FF0000000000000;
	mov.f64 	%fd658, %fd28;
	@%p161 bra 	$L__BB8_111;

	mov.f64 	%fd462, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r192, %temp}, %fd462;
	}
	and.b32  	%r24, %r3, 2147483647;
	setp.eq.s32 	%p162, %r24, 2146435072;
	setp.eq.s32 	%p163, %r192, 0;
	and.pred  	%p164, %p162, %p163;
	@%p164 bra 	$L__BB8_110;
	bra.uni 	$L__BB8_108;

$L__BB8_110:
	setp.gt.f64 	%p171, %fd22, 0d3FF0000000000000;
	selp.b32 	%r199, 2146435072, 0, %p171;
	mov.u32 	%r200, 0;
	xor.b32  	%r201, %r199, 2146435072;
	setp.lt.s32 	%p172, %r3, 0;
	selp.b32 	%r202, %r201, %r199, %p172;
	setp.eq.f32 	%p173, %f5, 0fBF800000;
	selp.b32 	%r203, 1072693248, %r202, %p173;
	mov.b64 	%fd658, {%r200, %r203};
	bra.uni 	$L__BB8_111;

$L__BB8_40:
	setp.gt.s32 	%p63, %r2, -1;
	@%p63 bra 	$L__BB8_44;

	mov.f64 	%fd281, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd282, %fd281;
	setp.eq.f64 	%p64, %fd282, 0d4008000000000000;
	@%p64 bra 	$L__BB8_44;

	mov.f64 	%fd642, 0dFFF8000000000000;

$L__BB8_44:
	mov.f64 	%fd644, %fd642;
	@%p16 bra 	$L__BB8_50;

	setp.gtu.f64 	%p68, %fd5, 0d7FF0000000000000;
	mov.f64 	%fd644, %fd11;
	@%p68 bra 	$L__BB8_50;

	mov.f64 	%fd284, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r102, %temp}, %fd284;
	}
	and.b32  	%r14, %r3, 2147483647;
	setp.eq.s32 	%p69, %r14, 2146435072;
	setp.eq.s32 	%p70, %r102, 0;
	and.pred  	%p71, %p69, %p70;
	@%p71 bra 	$L__BB8_49;
	bra.uni 	$L__BB8_47;

$L__BB8_49:
	setp.gt.f64 	%p78, %fd5, 0d3FF0000000000000;
	selp.b32 	%r109, 2146435072, 0, %p78;
	mov.u32 	%r110, 0;
	xor.b32  	%r111, %r109, 2146435072;
	setp.lt.s32 	%p79, %r3, 0;
	selp.b32 	%r112, %r111, %r109, %p79;
	setp.eq.f32 	%p80, %f3, 0fBF800000;
	selp.b32 	%r113, 1072693248, %r112, %p80;
	mov.b64 	%fd644, {%r110, %r113};
	bra.uni 	$L__BB8_50;

$L__BB8_140:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r238, %temp}, %fd4;
	}
	and.b32  	%r239, %r2, 2147483647;
	setp.ne.s32 	%p212, %r239, 2146435072;
	setp.ne.s32 	%p213, %r238, 0;
	or.pred  	%p214, %p212, %p213;
	mov.f64 	%fd666, %fd642;
	@%p214 bra 	$L__BB8_143;

	setp.gt.s32 	%p215, %r3, -1;
	selp.b32 	%r240, 2146435072, 0, %p215;
	mov.u32 	%r241, 0;
	setp.ne.s32 	%p216, %r30, 1071644672;
	and.pred  	%p217, %p216, %p1;
	or.b32  	%r242, %r240, -2147483648;
	selp.b32 	%r243, %r242, %r240, %p217;
	mov.b64 	%fd666, {%r241, %r243};

$L__BB8_143:
	mul.f64 	%fd552, %fd666, 0d3FA6E4E26D4801F7;
	selp.f64 	%fd553, 0d3FA6E4E26D4801F7, %fd552, %p30;
	add.f64 	%fd554, %fd553, %fd4;
	mul.f64 	%fd113, %fd554, 0d3FE9884520000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r31}, %fd113;
	}
	and.b32  	%r32, %r31, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r249, %temp}, %fd113;
	}
	mov.b64 	%fd114, {%r249, %r32};
	setp.ltu.f64 	%p222, %fd114, 0d3FE4F92224DD2F1A;
	@%p222 bra 	$L__BB8_145;
	bra.uni 	$L__BB8_144;

$L__BB8_145:
	mul.f64 	%fd596, %fd113, %fd113;
	mov.f64 	%fd597, 0d3F14359F420AFC3D;
	mov.f64 	%fd598, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd599, %fd598, %fd596, %fd597;
	mov.f64 	%fd600, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd601, %fd599, %fd596, %fd600;
	mov.f64 	%fd602, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd603, %fd601, %fd596, %fd602;
	mov.f64 	%fd604, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd605, %fd603, %fd596, %fd604;
	mov.f64 	%fd606, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd607, %fd605, %fd596, %fd606;
	mov.f64 	%fd608, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd609, %fd607, %fd596, %fd608;
	mov.f64 	%fd610, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd611, %fd609, %fd596, %fd610;
	mov.f64 	%fd612, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd613, %fd611, %fd596, %fd612;
	mov.f64 	%fd614, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd615, %fd613, %fd596, %fd614;
	mov.f64 	%fd616, 0dBFD5555555555550;
	fma.rn.f64 	%fd617, %fd615, %fd596, %fd616;
	mov.f64 	%fd618, 0d0000000000000000;
	fma.rn.f64 	%fd619, %fd617, %fd596, %fd618;
	fma.rn.f64 	%fd667, %fd619, %fd113, %fd113;
	bra.uni 	$L__BB8_146;

$L__BB8_144:
	add.f64 	%fd555, %fd114, %fd114;
	mov.f64 	%fd556, 0d4000000000000000;
	cvt.rn.f32.f64 	%f82, %fd555;
	mul.f32 	%f83, %f82, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f84, %f83;
	cvt.f64.f32 	%fd557, %f84;
	neg.f64 	%fd558, %fd557;
	mov.f64 	%fd559, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd560, %fd558, %fd559, %fd555;
	mov.f64 	%fd561, 0d3E928A27F89B6999;
	mov.f64 	%fd562, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd563, %fd562, %fd560, %fd561;
	mov.f64 	%fd564, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd565, %fd563, %fd560, %fd564;
	mov.f64 	%fd566, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd567, %fd565, %fd560, %fd566;
	mov.f64 	%fd568, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd569, %fd567, %fd560, %fd568;
	mov.f64 	%fd570, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd571, %fd569, %fd560, %fd570;
	mov.f64 	%fd572, 0d3F811111111173C4;
	fma.rn.f64 	%fd573, %fd571, %fd560, %fd572;
	mov.f64 	%fd574, 0d3FA555555555211A;
	fma.rn.f64 	%fd575, %fd573, %fd560, %fd574;
	mov.f64 	%fd576, 0d3FC5555555555540;
	fma.rn.f64 	%fd577, %fd575, %fd560, %fd576;
	mov.f64 	%fd578, 0d3FE0000000000005;
	fma.rn.f64 	%fd579, %fd577, %fd560, %fd578;
	mul.f64 	%fd580, %fd560, %fd579;
	fma.rn.f64 	%fd581, %fd580, %fd560, %fd560;
	ex2.approx.ftz.f32 	%f85, %f84;
	cvt.f64.f32 	%fd582, %f85;
	mov.f64 	%fd583, 0d3FF0000000000000;
	sub.f64 	%fd584, %fd583, %fd582;
	neg.f64 	%fd585, %fd581;
	fma.rn.f64 	%fd586, %fd585, %fd582, %fd584;
	sub.f64 	%fd587, %fd556, %fd586;
	rcp.approx.ftz.f64 	%fd588, %fd587;
	neg.f64 	%fd589, %fd587;
	fma.rn.f64 	%fd590, %fd589, %fd588, %fd583;
	fma.rn.f64 	%fd591, %fd590, %fd590, %fd590;
	fma.rn.f64 	%fd592, %fd591, %fd588, %fd588;
	neg.f64 	%fd593, %fd592;
	fma.rn.f64 	%fd594, %fd556, %fd593, %fd583;
	setp.gt.u32 	%p223, %r32, 1077088193;
	selp.f64 	%fd595, 0d3FF0000000000000, %fd594, %p223;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r250, %temp}, %fd595;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r251}, %fd595;
	}
	and.b32  	%r252, %r31, -2147483648;
	or.b32  	%r253, %r251, %r252;
	mov.b64 	%fd667, {%r250, %r253};

$L__BB8_146:
	setp.lt.s32 	%p250, %r2, 0;
	cvt.rn.f32.f64 	%f86, %fd667;
	mul.f32 	%f87, %f86, %f86;
	mov.f32 	%f88, 0f3F800000;
	sub.f32 	%f89, %f88, %f87;
	cvt.f64.f32 	%fd620, %f89;
	mul.f64 	%fd118, %fd663, %fd620;
	mov.f64 	%fd621, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd621;
	}
	and.b32  	%r34, %r33, 2146435072;
	setp.eq.s32 	%p224, %r34, 1062207488;
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd5;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd621;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd669, [retval0+0];
	} // callseq 9
	and.pred  	%p6, %p250, %p224;
	not.pred 	%p226, %p6;
	@%p226 bra 	$L__BB8_148;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r254}, %fd669;
	}
	xor.b32  	%r255, %r254, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r256, %temp}, %fd669;
	}
	mov.b64 	%fd669, {%r256, %r255};

$L__BB8_148:
	@%p11 bra 	$L__BB8_152;
	bra.uni 	$L__BB8_149;

$L__BB8_152:
	selp.b32 	%r257, %r2, 0, %p224;
	mov.u32 	%r258, 0;
	or.b32  	%r259, %r257, 2146435072;
	setp.lt.s32 	%p231, %r33, 0;
	selp.b32 	%r260, %r259, %r257, %p231;
	mov.b64 	%fd669, {%r258, %r260};
	bra.uni 	$L__BB8_153;

$L__BB8_149:
	setp.gt.s32 	%p228, %r2, -1;
	@%p228 bra 	$L__BB8_153;

	mov.f64 	%fd622, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd623, %fd622;
	setp.eq.f64 	%p229, %fd623, 0d4000000000000000;
	@%p229 bra 	$L__BB8_153;

	mov.f64 	%fd669, 0dFFF8000000000000;

$L__BB8_153:
	add.f64 	%fd124, %fd4, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r261}, %fd124;
	}
	and.b32  	%r262, %r261, 2146435072;
	setp.ne.s32 	%p232, %r262, 2146435072;
	mov.f64 	%fd670, %fd669;
	@%p232 bra 	$L__BB8_159;

	setp.gtu.f64 	%p233, %fd5, 0d7FF0000000000000;
	mov.f64 	%fd670, %fd124;
	@%p233 bra 	$L__BB8_159;

	mov.f64 	%fd625, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r263, %temp}, %fd625;
	}
	and.b32  	%r35, %r33, 2147483647;
	setp.eq.s32 	%p234, %r35, 2146435072;
	setp.eq.s32 	%p235, %r263, 0;
	and.pred  	%p236, %p234, %p235;
	@%p236 bra 	$L__BB8_158;
	bra.uni 	$L__BB8_156;

$L__BB8_158:
	setp.gt.f64 	%p243, %fd5, 0d3FF0000000000000;
	selp.b32 	%r270, 2146435072, 0, %p243;
	mov.u32 	%r271, 0;
	xor.b32  	%r272, %r270, 2146435072;
	setp.lt.s32 	%p244, %r33, 0;
	selp.b32 	%r273, %r272, %r270, %p244;
	setp.eq.f32 	%p245, %f3, 0fBF800000;
	selp.b32 	%r274, 1072693248, %r273, %p245;
	mov.b64 	%fd670, {%r271, %r274};
	bra.uni 	$L__BB8_159;

$L__BB8_156:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r264, %temp}, %fd4;
	}
	and.b32  	%r265, %r2, 2147483647;
	setp.ne.s32 	%p237, %r265, 2146435072;
	setp.ne.s32 	%p238, %r264, 0;
	or.pred  	%p239, %p237, %p238;
	mov.f64 	%fd670, %fd669;
	@%p239 bra 	$L__BB8_159;

	setp.gt.s32 	%p240, %r33, -1;
	selp.b32 	%r266, 2146435072, 0, %p240;
	mov.u32 	%r267, 0;
	setp.ne.s32 	%p241, %r35, 1071644672;
	and.pred  	%p242, %p241, %p6;
	or.b32  	%r268, %r266, -2147483648;
	selp.b32 	%r269, %r268, %r266, %p242;
	mov.b64 	%fd670, {%r267, %r269};

$L__BB8_159:
	mul.f64 	%fd626, %fd118, 0d3FE9884520000000;
	cvt.rn.f32.f64 	%f90, %fd626;
	cvt.f64.f32 	%fd627, %f90;
	mul.f64 	%fd628, %fd670, 0d3FC12BA9D1F60179;
	selp.f64 	%fd629, 0d3FC12BA9D1F60179, %fd628, %p30;
	fma.rn.f64 	%fd630, %fd629, %fd627, %fd627;
	cvt.rn.f32.f64 	%f91, %fd630;
	cvt.f64.f32 	%fd631, %f91;
	mul.f64 	%fd632, %fd3, %fd631;
	cvt.rn.f32.f64 	%f97, %fd632;
	bra.uni 	$L__BB8_160;

$L__BB8_108:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r193, %temp}, %fd21;
	}
	and.b32  	%r194, %r9, 2147483647;
	setp.ne.s32 	%p165, %r194, 2146435072;
	setp.ne.s32 	%p166, %r193, 0;
	or.pred  	%p167, %p165, %p166;
	mov.f64 	%fd658, %fd649;
	@%p167 bra 	$L__BB8_111;

	setp.gt.s32 	%p168, %r3, -1;
	selp.b32 	%r195, 2146435072, 0, %p168;
	mov.u32 	%r196, 0;
	setp.ne.s32 	%p169, %r24, 1071644672;
	and.pred  	%p170, %p169, %p2;
	or.b32  	%r197, %r195, -2147483648;
	selp.b32 	%r198, %r197, %r195, %p170;
	mov.b64 	%fd658, {%r196, %r198};

$L__BB8_111:
	mul.f64 	%fd463, %fd658, 0d3FA6E4E26D4801F7;
	selp.f64 	%fd464, 0d3FA6E4E26D4801F7, %fd463, %p55;
	add.f64 	%fd465, %fd464, %fd21;
	mul.f64 	%fd89, %fd465, 0d3FE9884520000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r25}, %fd89;
	}
	and.b32  	%r26, %r25, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r204, %temp}, %fd89;
	}
	mov.b64 	%fd90, {%r204, %r26};
	setp.ltu.f64 	%p175, %fd90, 0d3FE4F92224DD2F1A;
	@%p175 bra 	$L__BB8_113;
	bra.uni 	$L__BB8_112;

$L__BB8_113:
	mul.f64 	%fd507, %fd89, %fd89;
	mov.f64 	%fd508, 0d3F14359F420AFC3D;
	mov.f64 	%fd509, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd510, %fd509, %fd507, %fd508;
	mov.f64 	%fd511, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd512, %fd510, %fd507, %fd511;
	mov.f64 	%fd513, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd514, %fd512, %fd507, %fd513;
	mov.f64 	%fd515, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd516, %fd514, %fd507, %fd515;
	mov.f64 	%fd517, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd518, %fd516, %fd507, %fd517;
	mov.f64 	%fd519, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd520, %fd518, %fd507, %fd519;
	mov.f64 	%fd521, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd522, %fd520, %fd507, %fd521;
	mov.f64 	%fd523, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd524, %fd522, %fd507, %fd523;
	mov.f64 	%fd525, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd526, %fd524, %fd507, %fd525;
	mov.f64 	%fd527, 0dBFD5555555555550;
	fma.rn.f64 	%fd528, %fd526, %fd507, %fd527;
	mov.f64 	%fd529, 0d0000000000000000;
	fma.rn.f64 	%fd530, %fd528, %fd507, %fd529;
	fma.rn.f64 	%fd659, %fd530, %fd89, %fd89;
	bra.uni 	$L__BB8_114;

$L__BB8_112:
	add.f64 	%fd466, %fd90, %fd90;
	mov.f64 	%fd467, 0d4000000000000000;
	cvt.rn.f32.f64 	%f71, %fd466;
	mul.f32 	%f72, %f71, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f73, %f72;
	cvt.f64.f32 	%fd468, %f73;
	neg.f64 	%fd469, %fd468;
	mov.f64 	%fd470, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd471, %fd469, %fd470, %fd466;
	mov.f64 	%fd472, 0d3E928A27F89B6999;
	mov.f64 	%fd473, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd474, %fd473, %fd471, %fd472;
	mov.f64 	%fd475, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd476, %fd474, %fd471, %fd475;
	mov.f64 	%fd477, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd478, %fd476, %fd471, %fd477;
	mov.f64 	%fd479, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd480, %fd478, %fd471, %fd479;
	mov.f64 	%fd481, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd482, %fd480, %fd471, %fd481;
	mov.f64 	%fd483, 0d3F811111111173C4;
	fma.rn.f64 	%fd484, %fd482, %fd471, %fd483;
	mov.f64 	%fd485, 0d3FA555555555211A;
	fma.rn.f64 	%fd486, %fd484, %fd471, %fd485;
	mov.f64 	%fd487, 0d3FC5555555555540;
	fma.rn.f64 	%fd488, %fd486, %fd471, %fd487;
	mov.f64 	%fd489, 0d3FE0000000000005;
	fma.rn.f64 	%fd490, %fd488, %fd471, %fd489;
	mul.f64 	%fd491, %fd471, %fd490;
	fma.rn.f64 	%fd492, %fd491, %fd471, %fd471;
	ex2.approx.ftz.f32 	%f74, %f73;
	cvt.f64.f32 	%fd493, %f74;
	mov.f64 	%fd494, 0d3FF0000000000000;
	sub.f64 	%fd495, %fd494, %fd493;
	neg.f64 	%fd496, %fd492;
	fma.rn.f64 	%fd497, %fd496, %fd493, %fd495;
	sub.f64 	%fd498, %fd467, %fd497;
	rcp.approx.ftz.f64 	%fd499, %fd498;
	neg.f64 	%fd500, %fd498;
	fma.rn.f64 	%fd501, %fd500, %fd499, %fd494;
	fma.rn.f64 	%fd502, %fd501, %fd501, %fd501;
	fma.rn.f64 	%fd503, %fd502, %fd499, %fd499;
	neg.f64 	%fd504, %fd503;
	fma.rn.f64 	%fd505, %fd467, %fd504, %fd494;
	setp.gt.u32 	%p176, %r26, 1077088193;
	selp.f64 	%fd506, 0d3FF0000000000000, %fd505, %p176;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r205, %temp}, %fd506;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r206}, %fd506;
	}
	and.b32  	%r207, %r25, -2147483648;
	or.b32  	%r208, %r206, %r207;
	mov.b64 	%fd659, {%r205, %r208};

$L__BB8_114:
	setp.lt.s32 	%p249, %r9, 0;
	cvt.f64.f32 	%fd531, %f96;
	mul.f64 	%fd532, %fd531, 0dBFE0000000000000;
	cvt.rn.f32.f64 	%f75, %fd659;
	mul.f32 	%f76, %f75, %f75;
	mov.f32 	%f77, 0f3F800000;
	sub.f32 	%f78, %f77, %f76;
	cvt.f64.f32 	%fd533, %f78;
	mul.f64 	%fd94, %fd532, %fd533;
	mov.f64 	%fd534, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd534;
	}
	and.b32  	%r28, %r27, 2146435072;
	setp.eq.s32 	%p177, %r28, 1062207488;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd22;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd534;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd661, [retval0+0];
	} // callseq 8
	and.pred  	%p5, %p249, %p177;
	not.pred 	%p179, %p5;
	@%p179 bra 	$L__BB8_116;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r209}, %fd661;
	}
	xor.b32  	%r210, %r209, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r211, %temp}, %fd661;
	}
	mov.b64 	%fd661, {%r211, %r210};

$L__BB8_116:
	@%p36 bra 	$L__BB8_120;
	bra.uni 	$L__BB8_117;

$L__BB8_120:
	selp.b32 	%r212, %r9, 0, %p177;
	mov.u32 	%r213, 0;
	or.b32  	%r214, %r212, 2146435072;
	setp.lt.s32 	%p184, %r27, 0;
	selp.b32 	%r215, %r214, %r212, %p184;
	mov.b64 	%fd661, {%r213, %r215};
	bra.uni 	$L__BB8_121;

$L__BB8_117:
	setp.gt.s32 	%p181, %r9, -1;
	@%p181 bra 	$L__BB8_121;

	mov.f64 	%fd535, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd536, %fd535;
	setp.eq.f64 	%p182, %fd536, 0d4000000000000000;
	@%p182 bra 	$L__BB8_121;

	mov.f64 	%fd661, 0dFFF8000000000000;

$L__BB8_121:
	add.f64 	%fd100, %fd21, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r216}, %fd100;
	}
	and.b32  	%r217, %r216, 2146435072;
	setp.ne.s32 	%p185, %r217, 2146435072;
	mov.f64 	%fd662, %fd661;
	@%p185 bra 	$L__BB8_127;

	setp.gtu.f64 	%p186, %fd22, 0d7FF0000000000000;
	mov.f64 	%fd662, %fd100;
	@%p186 bra 	$L__BB8_127;

	mov.f64 	%fd538, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r218, %temp}, %fd538;
	}
	and.b32  	%r29, %r27, 2147483647;
	setp.eq.s32 	%p187, %r29, 2146435072;
	setp.eq.s32 	%p188, %r218, 0;
	and.pred  	%p189, %p187, %p188;
	@%p189 bra 	$L__BB8_126;
	bra.uni 	$L__BB8_124;

$L__BB8_126:
	setp.gt.f64 	%p196, %fd22, 0d3FF0000000000000;
	selp.b32 	%r225, 2146435072, 0, %p196;
	mov.u32 	%r226, 0;
	xor.b32  	%r227, %r225, 2146435072;
	setp.lt.s32 	%p197, %r27, 0;
	selp.b32 	%r228, %r227, %r225, %p197;
	setp.eq.f32 	%p198, %f5, 0fBF800000;
	selp.b32 	%r229, 1072693248, %r228, %p198;
	mov.b64 	%fd662, {%r226, %r229};
	bra.uni 	$L__BB8_127;

$L__BB8_47:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r103, %temp}, %fd4;
	}
	and.b32  	%r104, %r2, 2147483647;
	setp.ne.s32 	%p72, %r104, 2146435072;
	setp.ne.s32 	%p73, %r103, 0;
	or.pred  	%p74, %p72, %p73;
	mov.f64 	%fd644, %fd642;
	@%p74 bra 	$L__BB8_50;

	setp.gt.s32 	%p75, %r3, -1;
	selp.b32 	%r105, 2146435072, 0, %p75;
	mov.u32 	%r106, 0;
	setp.ne.s32 	%p76, %r14, 1071644672;
	and.pred  	%p77, %p76, %p1;
	or.b32  	%r107, %r105, -2147483648;
	selp.b32 	%r108, %r107, %r105, %p77;
	mov.b64 	%fd644, {%r106, %r108};

$L__BB8_50:
	mul.f64 	%fd285, %fd644, 0d3FA6E4E26D4801F7;
	selp.f64 	%fd286, 0d3FA6E4E26D4801F7, %fd285, %p30;
	add.f64 	%fd287, %fd286, %fd4;
	mul.f64 	%fd44, %fd287, 0d3FE9884520000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd44;
	}
	and.b32  	%r16, %r15, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r114, %temp}, %fd44;
	}
	mov.b64 	%fd45, {%r114, %r16};
	setp.ltu.f64 	%p82, %fd45, 0d3FE4F92224DD2F1A;
	@%p82 bra 	$L__BB8_52;
	bra.uni 	$L__BB8_51;

$L__BB8_52:
	mul.f64 	%fd329, %fd44, %fd44;
	mov.f64 	%fd330, 0d3F14359F420AFC3D;
	mov.f64 	%fd331, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd332, %fd331, %fd329, %fd330;
	mov.f64 	%fd333, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd334, %fd332, %fd329, %fd333;
	mov.f64 	%fd335, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd336, %fd334, %fd329, %fd335;
	mov.f64 	%fd337, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd338, %fd336, %fd329, %fd337;
	mov.f64 	%fd339, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd340, %fd338, %fd329, %fd339;
	mov.f64 	%fd341, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd342, %fd340, %fd329, %fd341;
	mov.f64 	%fd343, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd344, %fd342, %fd329, %fd343;
	mov.f64 	%fd345, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd346, %fd344, %fd329, %fd345;
	mov.f64 	%fd347, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd348, %fd346, %fd329, %fd347;
	mov.f64 	%fd349, 0dBFD5555555555550;
	fma.rn.f64 	%fd350, %fd348, %fd329, %fd349;
	mov.f64 	%fd351, 0d0000000000000000;
	fma.rn.f64 	%fd352, %fd350, %fd329, %fd351;
	fma.rn.f64 	%fd645, %fd352, %fd44, %fd44;
	bra.uni 	$L__BB8_53;

$L__BB8_51:
	add.f64 	%fd288, %fd45, %fd45;
	mov.f64 	%fd289, 0d4000000000000000;
	cvt.rn.f32.f64 	%f48, %fd288;
	mul.f32 	%f49, %f48, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f50, %f49;
	cvt.f64.f32 	%fd290, %f50;
	neg.f64 	%fd291, %fd290;
	mov.f64 	%fd292, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd293, %fd291, %fd292, %fd288;
	mov.f64 	%fd294, 0d3E928A27F89B6999;
	mov.f64 	%fd295, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd296, %fd295, %fd293, %fd294;
	mov.f64 	%fd297, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd298, %fd296, %fd293, %fd297;
	mov.f64 	%fd299, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd300, %fd298, %fd293, %fd299;
	mov.f64 	%fd301, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd302, %fd300, %fd293, %fd301;
	mov.f64 	%fd303, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd304, %fd302, %fd293, %fd303;
	mov.f64 	%fd305, 0d3F811111111173C4;
	fma.rn.f64 	%fd306, %fd304, %fd293, %fd305;
	mov.f64 	%fd307, 0d3FA555555555211A;
	fma.rn.f64 	%fd308, %fd306, %fd293, %fd307;
	mov.f64 	%fd309, 0d3FC5555555555540;
	fma.rn.f64 	%fd310, %fd308, %fd293, %fd309;
	mov.f64 	%fd311, 0d3FE0000000000005;
	fma.rn.f64 	%fd312, %fd310, %fd293, %fd311;
	mul.f64 	%fd313, %fd293, %fd312;
	fma.rn.f64 	%fd314, %fd313, %fd293, %fd293;
	ex2.approx.ftz.f32 	%f51, %f50;
	cvt.f64.f32 	%fd315, %f51;
	mov.f64 	%fd316, 0d3FF0000000000000;
	sub.f64 	%fd317, %fd316, %fd315;
	neg.f64 	%fd318, %fd314;
	fma.rn.f64 	%fd319, %fd318, %fd315, %fd317;
	sub.f64 	%fd320, %fd289, %fd319;
	rcp.approx.ftz.f64 	%fd321, %fd320;
	neg.f64 	%fd322, %fd320;
	fma.rn.f64 	%fd323, %fd322, %fd321, %fd316;
	fma.rn.f64 	%fd324, %fd323, %fd323, %fd323;
	fma.rn.f64 	%fd325, %fd324, %fd321, %fd321;
	neg.f64 	%fd326, %fd325;
	fma.rn.f64 	%fd327, %fd289, %fd326, %fd316;
	setp.gt.u32 	%p83, %r16, 1077088193;
	selp.f64 	%fd328, 0d3FF0000000000000, %fd327, %p83;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r115, %temp}, %fd328;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r116}, %fd328;
	}
	and.b32  	%r117, %r15, -2147483648;
	or.b32  	%r118, %r116, %r117;
	mov.b64 	%fd645, {%r115, %r118};

$L__BB8_53:
	setp.lt.s32 	%p247, %r2, 0;
	mov.f64 	%fd353, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r17}, %fd353;
	}
	and.b32  	%r18, %r17, 2146435072;
	setp.eq.s32 	%p84, %r18, 1062207488;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd5;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd353;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd647, [retval0+0];
	} // callseq 6
	and.pred  	%p3, %p247, %p84;
	not.pred 	%p86, %p3;
	@%p86 bra 	$L__BB8_55;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r119}, %fd647;
	}
	xor.b32  	%r120, %r119, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r121, %temp}, %fd647;
	}
	mov.b64 	%fd647, {%r121, %r120};

$L__BB8_55:
	@%p11 bra 	$L__BB8_59;
	bra.uni 	$L__BB8_56;

$L__BB8_59:
	selp.b32 	%r122, %r2, 0, %p84;
	mov.u32 	%r123, 0;
	or.b32  	%r124, %r122, 2146435072;
	setp.lt.s32 	%p91, %r17, 0;
	selp.b32 	%r125, %r124, %r122, %p91;
	mov.b64 	%fd647, {%r123, %r125};
	bra.uni 	$L__BB8_60;

$L__BB8_56:
	setp.gt.s32 	%p88, %r2, -1;
	@%p88 bra 	$L__BB8_60;

	mov.f64 	%fd354, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd355, %fd354;
	setp.eq.f64 	%p89, %fd355, 0d4000000000000000;
	@%p89 bra 	$L__BB8_60;

	mov.f64 	%fd647, 0dFFF8000000000000;

$L__BB8_60:
	add.f64 	%fd54, %fd4, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r126}, %fd54;
	}
	and.b32  	%r127, %r126, 2146435072;
	setp.ne.s32 	%p92, %r127, 2146435072;
	mov.f64 	%fd648, %fd647;
	@%p92 bra 	$L__BB8_66;

	setp.gtu.f64 	%p93, %fd5, 0d7FF0000000000000;
	mov.f64 	%fd648, %fd54;
	@%p93 bra 	$L__BB8_66;

	mov.f64 	%fd357, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r128, %temp}, %fd357;
	}
	and.b32  	%r19, %r17, 2147483647;
	setp.eq.s32 	%p94, %r19, 2146435072;
	setp.eq.s32 	%p95, %r128, 0;
	and.pred  	%p96, %p94, %p95;
	@%p96 bra 	$L__BB8_65;
	bra.uni 	$L__BB8_63;

$L__BB8_65:
	setp.gt.f64 	%p103, %fd5, 0d3FF0000000000000;
	selp.b32 	%r135, 2146435072, 0, %p103;
	mov.u32 	%r136, 0;
	xor.b32  	%r137, %r135, 2146435072;
	setp.lt.s32 	%p104, %r17, 0;
	selp.b32 	%r138, %r137, %r135, %p104;
	setp.eq.f32 	%p105, %f3, 0fBF800000;
	selp.b32 	%r139, 1072693248, %r138, %p105;
	mov.b64 	%fd648, {%r136, %r139};
	bra.uni 	$L__BB8_66;

$L__BB8_124:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r219, %temp}, %fd21;
	}
	and.b32  	%r220, %r9, 2147483647;
	setp.ne.s32 	%p190, %r220, 2146435072;
	setp.ne.s32 	%p191, %r219, 0;
	or.pred  	%p192, %p190, %p191;
	mov.f64 	%fd662, %fd661;
	@%p192 bra 	$L__BB8_127;

	setp.gt.s32 	%p193, %r27, -1;
	selp.b32 	%r221, 2146435072, 0, %p193;
	mov.u32 	%r222, 0;
	setp.ne.s32 	%p194, %r29, 1071644672;
	and.pred  	%p195, %p194, %p5;
	or.b32  	%r223, %r221, -2147483648;
	selp.b32 	%r224, %r223, %r221, %p195;
	mov.b64 	%fd662, {%r222, %r224};

$L__BB8_127:
	mul.f64 	%fd539, %fd94, 0d3FE9884520000000;
	cvt.rn.f32.f64 	%f79, %fd539;
	cvt.f64.f32 	%fd540, %f79;
	mul.f64 	%fd541, %fd662, 0d3FC12BA9D1F60179;
	selp.f64 	%fd542, 0d3FC12BA9D1F60179, %fd541, %p55;
	fma.rn.f64 	%fd543, %fd542, %fd540, %fd540;
	cvt.rn.f32.f64 	%f80, %fd543;
	cvt.f64.f32 	%fd544, %f80;
	mul.f64 	%fd545, %fd20, %fd544;
	cvt.rn.f32.f64 	%f97, %fd545;
	bra.uni 	$L__BB8_160;

$L__BB8_63:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r129, %temp}, %fd4;
	}
	and.b32  	%r130, %r2, 2147483647;
	setp.ne.s32 	%p97, %r130, 2146435072;
	setp.ne.s32 	%p98, %r129, 0;
	or.pred  	%p99, %p97, %p98;
	mov.f64 	%fd648, %fd647;
	@%p99 bra 	$L__BB8_66;

	setp.gt.s32 	%p100, %r17, -1;
	selp.b32 	%r131, 2146435072, 0, %p100;
	mov.u32 	%r132, 0;
	setp.ne.s32 	%p101, %r19, 1071644672;
	and.pred  	%p102, %p101, %p3;
	or.b32  	%r133, %r131, -2147483648;
	selp.b32 	%r134, %r133, %r131, %p102;
	mov.b64 	%fd648, {%r132, %r134};

$L__BB8_66:
	cvt.rn.f32.f64 	%f52, %fd645;
	mul.f32 	%f53, %f52, %f52;
	mov.f32 	%f54, 0f3F800000;
	sub.f32 	%f55, %f54, %f53;
	cvt.f64.f32 	%fd358, %f55;
	cvt.f64.f32 	%fd359, %f95;
	mul.f64 	%fd360, %fd359, 0d3FE0000000000000;
	mul.f64 	%fd361, %fd360, %fd358;
	mul.f64 	%fd362, %fd361, 0d3FE9884520000000;
	cvt.rn.f32.f64 	%f56, %fd362;
	cvt.f64.f32 	%fd363, %f56;
	mul.f64 	%fd364, %fd648, 0d3FC12BA9D1F60179;
	selp.f64 	%fd365, 0d3FC12BA9D1F60179, %fd364, %p30;
	fma.rn.f64 	%fd366, %fd365, %fd363, %fd363;
	cvt.rn.f32.f64 	%f57, %fd366;
	cvt.f64.f32 	%fd367, %f57;
	mul.f64 	%fd58, %fd3, %fd367;
	@%p35 bra 	$L__BB8_68;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r140}, %fd649;
	}
	xor.b32  	%r141, %r140, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r142, %temp}, %fd649;
	}
	mov.b64 	%fd649, {%r142, %r141};

$L__BB8_68:
	@%p36 bra 	$L__BB8_72;
	bra.uni 	$L__BB8_69;

$L__BB8_72:
	selp.b32 	%r143, %r9, 0, %p8;
	mov.u32 	%r144, 0;
	or.b32  	%r145, %r143, 2146435072;
	setp.lt.s32 	%p112, %r3, 0;
	selp.b32 	%r146, %r145, %r143, %p112;
	mov.b64 	%fd649, {%r144, %r146};
	bra.uni 	$L__BB8_73;

$L__BB8_69:
	setp.gt.s32 	%p109, %r9, -1;
	@%p109 bra 	$L__BB8_73;

	mov.f64 	%fd368, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd369, %fd368;
	setp.eq.f64 	%p110, %fd369, 0d4008000000000000;
	@%p110 bra 	$L__BB8_73;

	mov.f64 	%fd649, 0dFFF8000000000000;

$L__BB8_73:
	mov.f64 	%fd651, %fd649;
	@%p41 bra 	$L__BB8_79;

	setp.gtu.f64 	%p114, %fd22, 0d7FF0000000000000;
	mov.f64 	%fd651, %fd28;
	@%p114 bra 	$L__BB8_79;

	mov.f64 	%fd371, 0d4008000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r147, %temp}, %fd371;
	}
	and.b32  	%r20, %r3, 2147483647;
	setp.eq.s32 	%p115, %r20, 2146435072;
	setp.eq.s32 	%p116, %r147, 0;
	and.pred  	%p117, %p115, %p116;
	@%p117 bra 	$L__BB8_78;
	bra.uni 	$L__BB8_76;

$L__BB8_78:
	setp.gt.f64 	%p124, %fd22, 0d3FF0000000000000;
	selp.b32 	%r154, 2146435072, 0, %p124;
	mov.u32 	%r155, 0;
	xor.b32  	%r156, %r154, 2146435072;
	setp.lt.s32 	%p125, %r3, 0;
	selp.b32 	%r157, %r156, %r154, %p125;
	setp.eq.f32 	%p126, %f5, 0fBF800000;
	selp.b32 	%r158, 1072693248, %r157, %p126;
	mov.b64 	%fd651, {%r155, %r158};
	bra.uni 	$L__BB8_79;

$L__BB8_76:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r148, %temp}, %fd21;
	}
	and.b32  	%r149, %r9, 2147483647;
	setp.ne.s32 	%p118, %r149, 2146435072;
	setp.ne.s32 	%p119, %r148, 0;
	or.pred  	%p120, %p118, %p119;
	mov.f64 	%fd651, %fd649;
	@%p120 bra 	$L__BB8_79;

	setp.gt.s32 	%p121, %r3, -1;
	selp.b32 	%r150, 2146435072, 0, %p121;
	mov.u32 	%r151, 0;
	setp.ne.s32 	%p122, %r20, 1071644672;
	and.pred  	%p123, %p122, %p2;
	or.b32  	%r152, %r150, -2147483648;
	selp.b32 	%r153, %r152, %r150, %p123;
	mov.b64 	%fd651, {%r151, %r153};

$L__BB8_79:
	mul.f64 	%fd372, %fd651, 0d3FA6E4E26D4801F7;
	selp.f64 	%fd373, 0d3FA6E4E26D4801F7, %fd372, %p55;
	add.f64 	%fd374, %fd373, %fd21;
	mul.f64 	%fd66, %fd374, 0d3FE9884520000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd66;
	}
	and.b32  	%r22, %r21, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r159, %temp}, %fd66;
	}
	mov.b64 	%fd67, {%r159, %r22};
	setp.ltu.f64 	%p128, %fd67, 0d3FE4F92224DD2F1A;
	@%p128 bra 	$L__BB8_81;
	bra.uni 	$L__BB8_80;

$L__BB8_81:
	mul.f64 	%fd416, %fd66, %fd66;
	mov.f64 	%fd417, 0d3F14359F420AFC3D;
	mov.f64 	%fd418, 0dBEF0BC46E2F5E964;
	fma.rn.f64 	%fd419, %fd418, %fd416, %fd417;
	mov.f64 	%fd420, 0dBF2DF9F0728C5D84;
	fma.rn.f64 	%fd421, %fd419, %fd416, %fd420;
	mov.f64 	%fd422, 0d3F4337D1CEC4F033;
	fma.rn.f64 	%fd423, %fd421, %fd416, %fd422;
	mov.f64 	%fd424, 0dBF57D6E9674335B3;
	fma.rn.f64 	%fd425, %fd423, %fd416, %fd424;
	mov.f64 	%fd426, 0d3F6D6D000D7AAD3D;
	fma.rn.f64 	%fd427, %fd425, %fd416, %fd426;
	mov.f64 	%fd428, 0dBF8226E1F3CF1EF5;
	fma.rn.f64 	%fd429, %fd427, %fd416, %fd428;
	mov.f64 	%fd430, 0d3F9664F47EC0C8CF;
	fma.rn.f64 	%fd431, %fd429, %fd416, %fd430;
	mov.f64 	%fd432, 0dBFABA1BA1B80AB40;
	fma.rn.f64 	%fd433, %fd431, %fd416, %fd432;
	mov.f64 	%fd434, 0d3FC111111110FA4A;
	fma.rn.f64 	%fd435, %fd433, %fd416, %fd434;
	mov.f64 	%fd436, 0dBFD5555555555550;
	fma.rn.f64 	%fd437, %fd435, %fd416, %fd436;
	mov.f64 	%fd438, 0d0000000000000000;
	fma.rn.f64 	%fd439, %fd437, %fd416, %fd438;
	fma.rn.f64 	%fd652, %fd439, %fd66, %fd66;
	bra.uni 	$L__BB8_82;

$L__BB8_80:
	add.f64 	%fd375, %fd67, %fd67;
	mov.f64 	%fd376, 0d4000000000000000;
	cvt.rn.f32.f64 	%f58, %fd375;
	mul.f32 	%f59, %f58, 0f3FB8AA3B;
	cvt.rni.f32.f32 	%f60, %f59;
	cvt.f64.f32 	%fd377, %f60;
	neg.f64 	%fd378, %fd377;
	mov.f64 	%fd379, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd380, %fd378, %fd379, %fd375;
	mov.f64 	%fd381, 0d3E928A27F89B6999;
	mov.f64 	%fd382, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd383, %fd382, %fd380, %fd381;
	mov.f64 	%fd384, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd385, %fd383, %fd380, %fd384;
	mov.f64 	%fd386, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd387, %fd385, %fd380, %fd386;
	mov.f64 	%fd388, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd389, %fd387, %fd380, %fd388;
	mov.f64 	%fd390, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd391, %fd389, %fd380, %fd390;
	mov.f64 	%fd392, 0d3F811111111173C4;
	fma.rn.f64 	%fd393, %fd391, %fd380, %fd392;
	mov.f64 	%fd394, 0d3FA555555555211A;
	fma.rn.f64 	%fd395, %fd393, %fd380, %fd394;
	mov.f64 	%fd396, 0d3FC5555555555540;
	fma.rn.f64 	%fd397, %fd395, %fd380, %fd396;
	mov.f64 	%fd398, 0d3FE0000000000005;
	fma.rn.f64 	%fd399, %fd397, %fd380, %fd398;
	mul.f64 	%fd400, %fd380, %fd399;
	fma.rn.f64 	%fd401, %fd400, %fd380, %fd380;
	ex2.approx.ftz.f32 	%f61, %f60;
	cvt.f64.f32 	%fd402, %f61;
	mov.f64 	%fd403, 0d3FF0000000000000;
	sub.f64 	%fd404, %fd403, %fd402;
	neg.f64 	%fd405, %fd401;
	fma.rn.f64 	%fd406, %fd405, %fd402, %fd404;
	sub.f64 	%fd407, %fd376, %fd406;
	rcp.approx.ftz.f64 	%fd408, %fd407;
	neg.f64 	%fd409, %fd407;
	fma.rn.f64 	%fd410, %fd409, %fd408, %fd403;
	fma.rn.f64 	%fd411, %fd410, %fd410, %fd410;
	fma.rn.f64 	%fd412, %fd411, %fd408, %fd408;
	neg.f64 	%fd413, %fd412;
	fma.rn.f64 	%fd414, %fd376, %fd413, %fd403;
	setp.gt.u32 	%p129, %r22, 1077088193;
	selp.f64 	%fd415, 0d3FF0000000000000, %fd414, %p129;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r160, %temp}, %fd415;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r161}, %fd415;
	}
	and.b32  	%r162, %r21, -2147483648;
	or.b32  	%r163, %r161, %r162;
	mov.b64 	%fd652, {%r160, %r163};

$L__BB8_82:
	setp.lt.s32 	%p248, %r9, 0;
	mul.f64 	%fd441, %fd359, 0dBFE0000000000000;
	cvt.rn.f32.f64 	%f62, %fd652;
	mul.f32 	%f63, %f62, %f62;
	mov.f32 	%f64, 0f3F800000;
	sub.f32 	%f65, %f64, %f63;
	cvt.f64.f32 	%fd442, %f65;
	mul.f64 	%fd71, %fd441, %fd442;
	mov.f64 	%fd443, 0d4000000000000000;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd22;
	.param .b64 param1;
	st.param.f64 	[param1+0], %fd443;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64 	%fd654, [retval0+0];
	} // callseq 7
	and.pred  	%p4, %p248, %p84;
	not.pred 	%p132, %p4;
	@%p132 bra 	$L__BB8_84;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r164}, %fd654;
	}
	xor.b32  	%r165, %r164, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r166, %temp}, %fd654;
	}
	mov.b64 	%fd654, {%r166, %r165};

$L__BB8_84:
	@%p36 bra 	$L__BB8_88;
	bra.uni 	$L__BB8_85;

$L__BB8_88:
	selp.b32 	%r167, %r9, 0, %p84;
	mov.u32 	%r168, 0;
	or.b32  	%r169, %r167, 2146435072;
	setp.lt.s32 	%p137, %r17, 0;
	selp.b32 	%r170, %r169, %r167, %p137;
	mov.b64 	%fd654, {%r168, %r170};
	bra.uni 	$L__BB8_89;

$L__BB8_85:
	setp.gt.s32 	%p134, %r9, -1;
	@%p134 bra 	$L__BB8_89;

	mov.f64 	%fd444, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd445, %fd444;
	setp.eq.f64 	%p135, %fd445, 0d4000000000000000;
	@%p135 bra 	$L__BB8_89;

	mov.f64 	%fd654, 0dFFF8000000000000;

$L__BB8_89:
	add.f64 	%fd77, %fd21, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r171}, %fd77;
	}
	and.b32  	%r172, %r171, 2146435072;
	setp.ne.s32 	%p138, %r172, 2146435072;
	mov.f64 	%fd655, %fd654;
	@%p138 bra 	$L__BB8_95;

	setp.gtu.f64 	%p139, %fd22, 0d7FF0000000000000;
	mov.f64 	%fd655, %fd77;
	@%p139 bra 	$L__BB8_95;

	mov.f64 	%fd447, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r173, %temp}, %fd447;
	}
	and.b32  	%r23, %r17, 2147483647;
	setp.eq.s32 	%p140, %r23, 2146435072;
	setp.eq.s32 	%p141, %r173, 0;
	and.pred  	%p142, %p140, %p141;
	@%p142 bra 	$L__BB8_94;
	bra.uni 	$L__BB8_92;

$L__BB8_94:
	setp.gt.f64 	%p149, %fd22, 0d3FF0000000000000;
	selp.b32 	%r180, 2146435072, 0, %p149;
	mov.u32 	%r181, 0;
	xor.b32  	%r182, %r180, 2146435072;
	setp.lt.s32 	%p150, %r17, 0;
	selp.b32 	%r183, %r182, %r180, %p150;
	setp.eq.f32 	%p151, %f5, 0fBF800000;
	selp.b32 	%r184, 1072693248, %r183, %p151;
	mov.b64 	%fd655, {%r181, %r184};
	bra.uni 	$L__BB8_95;

$L__BB8_92:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r174, %temp}, %fd21;
	}
	and.b32  	%r175, %r9, 2147483647;
	setp.ne.s32 	%p143, %r175, 2146435072;
	setp.ne.s32 	%p144, %r174, 0;
	or.pred  	%p145, %p143, %p144;
	mov.f64 	%fd655, %fd654;
	@%p145 bra 	$L__BB8_95;

	setp.gt.s32 	%p146, %r17, -1;
	selp.b32 	%r176, 2146435072, 0, %p146;
	mov.u32 	%r177, 0;
	setp.ne.s32 	%p147, %r23, 1071644672;
	and.pred  	%p148, %p147, %p4;
	or.b32  	%r178, %r176, -2147483648;
	selp.b32 	%r179, %r178, %r176, %p148;
	mov.b64 	%fd655, {%r177, %r179};

$L__BB8_95:
	mul.f64 	%fd448, %fd71, 0d3FE9884520000000;
	cvt.rn.f32.f64 	%f66, %fd448;
	cvt.f64.f32 	%fd449, %f66;
	mul.f64 	%fd450, %fd655, 0d3FC12BA9D1F60179;
	selp.f64 	%fd451, 0d3FC12BA9D1F60179, %fd450, %p55;
	fma.rn.f64 	%fd452, %fd451, %fd449, %fd449;
	cvt.rn.f32.f64 	%f67, %fd452;
	cvt.f64.f32 	%fd453, %f67;
	mul.f64 	%fd454, %fd20, %fd453;
	cvt.rn.f32.f64 	%f68, %fd454;
	cvt.rn.f32.f64 	%f69, %fd58;
	add.f32 	%f97, %f69, %f68;

$L__BB8_160:
	mov.u32 	%r282, %ctaid.x;
	mov.u32 	%r281, %nctaid.x;
	mov.u32 	%r280, %ctaid.y;
	mov.u32 	%r279, %tid.x;
	mov.u32 	%r278, %ntid.x;
	mad.lo.s32 	%r277, %r280, %r281, %r282;
	mad.lo.s32 	%r276, %r277, %r278, %r279;
	cvt.s64.s32 	%rd17, %r276;
	ld.param.u64 	%rd16, [discretized_gaussian_log_likelihood_back_param_3];
	mul.f32 	%f92, %f2, %f97;
	mul.f32 	%f93, %f92, 0fBF000000;
	cvta.to.global.u64 	%rd13, %rd16;
	shl.b64 	%rd14, %rd17, 2;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f32 	[%rd15], %f93;

$L__BB8_161:
	ret;

}
	// .globl	where_kernel
.visible .entry where_kernel(
	.param .u64 where_kernel_param_0,
	.param .u64 where_kernel_param_1,
	.param .u64 where_kernel_param_2,
	.param .u64 where_kernel_param_3,
	.param .u32 where_kernel_param_4,
	.param .u32 where_kernel_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd3, [where_kernel_param_0];
	ld.param.u64 	%rd4, [where_kernel_param_1];
	ld.param.u64 	%rd5, [where_kernel_param_2];
	ld.param.u64 	%rd6, [where_kernel_param_3];
	ld.param.u32 	%r3, [where_kernel_param_4];
	ld.param.u32 	%r2, [where_kernel_param_5];
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ctaid.x;
	mad.lo.s32 	%r7, %r5, %r4, %r6;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB9_4;

	cvta.to.global.u64 	%rd7, %rd6;
	div.s32 	%r10, %r1, %r2;
	cvta.to.global.u64 	%rd8, %rd5;
	mul.wide.s32 	%rd9, %r10, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f32 	%f1, [%rd10];
	cvt.rzi.s32.f32 	%r11, %f1;
	setp.eq.s32 	%p2, %r11, 0;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd2, %rd7, %rd11;
	@%p2 bra 	$L__BB9_3;

	cvta.to.global.u64 	%rd12, %rd4;
	shl.b64 	%rd13, %rd1, 2;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f2, [%rd14];
	st.global.f32 	[%rd2], %f2;
	bra.uni 	$L__BB9_4;

$L__BB9_3:
	cvta.to.global.u64 	%rd15, %rd3;
	shl.b64 	%rd16, %rd1, 2;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.f32 	%f3, [%rd17];
	st.global.f32 	[%rd2], %f3;

$L__BB9_4:
	ret;

}
	// .globl	var_back
.visible .entry var_back(
	.param .u64 var_back_param_0,
	.param .u64 var_back_param_1,
	.param .u64 var_back_param_2,
	.param .u64 var_back_param_3,
	.param .u32 var_back_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [var_back_param_0];
	ld.param.u64 	%rd2, [var_back_param_1];
	ld.param.u64 	%rd3, [var_back_param_2];
	ld.param.u64 	%rd4, [var_back_param_3];
	ld.param.u32 	%r2, [var_back_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB10_2;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd1;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd7];
	mul.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f4, [%rd11];
	mul.f32 	%f5, %f2, %f4;
	sub.f32 	%f6, %f3, %f5;
	mul.f32 	%f7, %f6, 0f3F000000;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	st.global.f32 	[%rd13], %f7;

$L__BB10_2:
	ret;

}
	// .globl	mean_kernel
.visible .entry mean_kernel(
	.param .u64 mean_kernel_param_0,
	.param .u64 mean_kernel_param_1,
	.param .u32 mean_kernel_param_2,
	.param .u32 mean_kernel_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd9, [mean_kernel_param_0];
	ld.param.u64 	%rd8, [mean_kernel_param_1];
	ld.param.u32 	%r12, [mean_kernel_param_2];
	ld.param.u32 	%r11, [mean_kernel_param_3];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r13, %nctaid.x;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r13, %r15;
	mov.u32 	%r17, %ntid.x;
	mov.u32 	%r18, %tid.x;
	mad.lo.s32 	%r1, %r16, %r17, %r18;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB11_9;

	setp.lt.s32 	%p2, %r11, 1;
	mov.f32 	%f26, 0f00000000;
	@%p2 bra 	$L__BB11_8;

	add.s32 	%r20, %r11, -1;
	and.b32  	%r27, %r11, 3;
	setp.lt.u32 	%p3, %r20, 3;
	mov.f32 	%f26, 0f00000000;
	mov.u32 	%r26, 0;
	@%p3 bra 	$L__BB11_5;

	sub.s32 	%r25, %r11, %r27;
	mul.lo.s32 	%r22, %r11, %r1;
	mul.wide.s32 	%rd10, %r22, 4;
	add.s64 	%rd11, %rd1, %rd10;
	add.s64 	%rd16, %rd11, 8;

$L__BB11_4:
	ld.global.f32 	%f12, [%rd16+-8];
	add.f32 	%f13, %f26, %f12;
	ld.global.f32 	%f14, [%rd16+-4];
	add.f32 	%f15, %f13, %f14;
	ld.global.f32 	%f16, [%rd16];
	add.f32 	%f17, %f15, %f16;
	ld.global.f32 	%f18, [%rd16+4];
	add.f32 	%f26, %f17, %f18;
	add.s32 	%r26, %r26, 4;
	add.s64 	%rd16, %rd16, 16;
	add.s32 	%r25, %r25, -4;
	setp.ne.s32 	%p4, %r25, 0;
	@%p4 bra 	$L__BB11_4;

$L__BB11_5:
	setp.eq.s32 	%p5, %r27, 0;
	@%p5 bra 	$L__BB11_8;

	mad.lo.s32 	%r23, %r11, %r1, %r26;
	mul.wide.s32 	%rd12, %r23, 4;
	add.s64 	%rd17, %rd1, %rd12;

$L__BB11_7:
	.pragma "nounroll";
	ld.global.f32 	%f19, [%rd17];
	add.f32 	%f26, %f26, %f19;
	add.s64 	%rd17, %rd17, 4;
	add.s32 	%r27, %r27, -1;
	setp.ne.s32 	%p6, %r27, 0;
	@%p6 bra 	$L__BB11_7;

$L__BB11_8:
	cvt.rn.f32.s32 	%f20, %r11;
	div.rn.f32 	%f21, %f26, %f20;
	cvta.to.global.u64 	%rd13, %rd8;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.f32 	[%rd15], %f21;

$L__BB11_9:
	ret;

}
	// .globl	get_score_from_velocity
.visible .entry get_score_from_velocity(
	.param .u64 get_score_from_velocity_param_0,
	.param .u64 get_score_from_velocity_param_1,
	.param .f32 get_score_from_velocity_param_2,
	.param .u64 get_score_from_velocity_param_3,
	.param .u32 get_score_from_velocity_param_4,
	.param .u32 get_score_from_velocity_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [get_score_from_velocity_param_0];
	ld.param.u64 	%rd2, [get_score_from_velocity_param_1];
	ld.param.f32 	%f1, [get_score_from_velocity_param_2];
	ld.param.u64 	%rd3, [get_score_from_velocity_param_3];
	ld.param.u32 	%r2, [get_score_from_velocity_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB12_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mov.f32 	%f2, 0f3F800000;
	sub.f32 	%f3, %f2, %f1;
	neg.f32 	%f4, %f3;
	mul.f32 	%f5, %f3, %f1;
	fma.rn.f32 	%f6, %f1, %f1, %f5;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f7, [%rd6];
	mul.f32 	%f8, %f7, %f4;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f9, [%rd8];
	sub.f32 	%f10, %f8, %f9;
	div.rn.f32 	%f11, %f10, %f6;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd5;
	st.global.f32 	[%rd10], %f11;

$L__BB12_2:
	ret;

}
	// .globl	p_sample
.visible .entry p_sample(
	.param .u64 p_sample_param_0,
	.param .u64 p_sample_param_1,
	.param .u64 p_sample_param_2,
	.param .u64 p_sample_param_3,
	.param .f32 p_sample_param_4,
	.param .f32 p_sample_param_5,
	.param .u64 p_sample_param_6,
	.param .u32 p_sample_param_7,
	.param .u32 p_sample_param_8
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [p_sample_param_0];
	ld.param.u64 	%rd2, [p_sample_param_1];
	ld.param.u64 	%rd3, [p_sample_param_2];
	ld.param.u64 	%rd4, [p_sample_param_3];
	ld.param.f32 	%f1, [p_sample_param_4];
	ld.param.f32 	%f2, [p_sample_param_5];
	ld.param.u64 	%rd5, [p_sample_param_6];
	ld.param.u32 	%r2, [p_sample_param_7];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB13_2;

	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	cvta.to.global.u64 	%rd9, %rd3;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f32 	%f3, [%rd10];
	mul.f32 	%f4, %f1, 0fBF000000;
	ld.global.f32 	%f5, [%rd8];
	fma.rn.f32 	%f6, %f4, %f3, %f5;
	cvta.to.global.u64 	%rd11, %rd2;
	add.s64 	%rd12, %rd11, %rd7;
	ld.global.f32 	%f7, [%rd12];
	fma.rn.f32 	%f8, %f6, %f2, %f7;
	cvta.to.global.u64 	%rd13, %rd4;
	add.s64 	%rd14, %rd13, %rd7;
	ld.global.f32 	%f9, [%rd14];
	sqrt.rn.f32 	%f10, %f1;
	fma.rn.f32 	%f11, %f10, %f9, %f8;
	cvta.to.global.u64 	%rd15, %rd5;
	add.s64 	%rd16, %rd15, %rd7;
	st.global.f32 	[%rd16], %f11;

$L__BB13_2:
	ret;

}
	// .globl	p_sample_last
.visible .entry p_sample_last(
	.param .u64 p_sample_last_param_0,
	.param .u64 p_sample_last_param_1,
	.param .u64 p_sample_last_param_2,
	.param .f32 p_sample_last_param_3,
	.param .f32 p_sample_last_param_4,
	.param .u64 p_sample_last_param_5,
	.param .u32 p_sample_last_param_6,
	.param .u32 p_sample_last_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [p_sample_last_param_0];
	ld.param.u64 	%rd2, [p_sample_last_param_1];
	ld.param.u64 	%rd3, [p_sample_last_param_2];
	ld.param.f32 	%f1, [p_sample_last_param_3];
	ld.param.f32 	%f2, [p_sample_last_param_4];
	ld.param.u64 	%rd4, [p_sample_last_param_5];
	ld.param.u32 	%r2, [p_sample_last_param_6];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %nctaid.x;
	mov.u32 	%r5, %ctaid.y;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB14_2;

	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f3, [%rd9];
	mul.f32 	%f4, %f1, 0fBF000000;
	ld.global.f32 	%f5, [%rd7];
	fma.rn.f32 	%f6, %f4, %f3, %f5;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd11, %rd10, %rd6;
	ld.global.f32 	%f7, [%rd11];
	fma.rn.f32 	%f8, %f6, %f2, %f7;
	cvta.to.global.u64 	%rd12, %rd4;
	add.s64 	%rd13, %rd12, %rd6;
	st.global.f32 	[%rd13], %f8;

$L__BB14_2:
	ret;

}
	// .globl	q_sample
.visible .entry q_sample(
	.param .u64 q_sample_param_0,
	.param .u64 q_sample_param_1,
	.param .u64 q_sample_param_2,
	.param .u64 q_sample_param_3,
	.param .u64 q_sample_param_4,
	.param .u32 q_sample_param_5,
	.param .u32 q_sample_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd1, [q_sample_param_0];
	ld.param.u64 	%rd2, [q_sample_param_1];
	ld.param.u64 	%rd3, [q_sample_param_2];
	ld.param.u64 	%rd4, [q_sample_param_3];
	ld.param.u64 	%rd5, [q_sample_param_4];
	ld.param.u32 	%r3, [q_sample_param_5];
	ld.param.u32 	%r2, [q_sample_param_6];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %nctaid.x;
	mov.u32 	%r6, %ctaid.y;
	mad.lo.s32 	%r7, %r6, %r5, %r4;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB15_2;

	cvta.to.global.u64 	%rd6, %rd1;
	div.s32 	%r10, %r1, %r2;
	cvta.to.global.u64 	%rd7, %rd3;
	mul.wide.s32 	%rd8, %r10, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	mov.f32 	%f2, 0f3F800000;
	sub.f32 	%f3, %f2, %f1;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.f32 	%f4, [%rd11];
	cvta.to.global.u64 	%rd12, %rd2;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.f32 	%f5, [%rd13];
	mul.f32 	%f6, %f1, %f5;
	fma.rn.f32 	%f7, %f3, %f4, %f6;
	cvta.to.global.u64 	%rd14, %rd4;
	add.s64 	%rd15, %rd14, %rd10;
	st.global.f32 	[%rd15], %f7;
	ld.global.f32 	%f8, [%rd13];
	ld.global.f32 	%f9, [%rd11];
	sub.f32 	%f10, %f8, %f9;
	cvta.to.global.u64 	%rd16, %rd5;
	add.s64 	%rd17, %rd16, %rd10;
	st.global.f32 	[%rd17], %f10;

$L__BB15_2:
	ret;

}
	// .globl	q_sample_no_target
.visible .entry q_sample_no_target(
	.param .u64 q_sample_no_target_param_0,
	.param .u64 q_sample_no_target_param_1,
	.param .u64 q_sample_no_target_param_2,
	.param .u64 q_sample_no_target_param_3,
	.param .u32 q_sample_no_target_param_4,
	.param .u32 q_sample_no_target_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [q_sample_no_target_param_0];
	ld.param.u64 	%rd2, [q_sample_no_target_param_1];
	ld.param.u64 	%rd3, [q_sample_no_target_param_2];
	ld.param.u64 	%rd4, [q_sample_no_target_param_3];
	ld.param.u32 	%r3, [q_sample_no_target_param_4];
	ld.param.u32 	%r2, [q_sample_no_target_param_5];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %nctaid.x;
	mov.u32 	%r6, %ctaid.y;
	mad.lo.s32 	%r7, %r6, %r5, %r4;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB16_2;

	cvta.to.global.u64 	%rd5, %rd3;
	div.s32 	%r10, %r1, %r2;
	mul.wide.s32 	%rd6, %r10, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd7];
	mov.f32 	%f2, 0f3F800000;
	sub.f32 	%f3, %f2, %f1;
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f32 	%f4, [%rd10];
	cvta.to.global.u64 	%rd11, %rd2;
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.f32 	%f5, [%rd12];
	mul.f32 	%f6, %f1, %f5;
	fma.rn.f32 	%f7, %f3, %f4, %f6;
	cvta.to.global.u64 	%rd13, %rd4;
	add.s64 	%rd14, %rd13, %rd9;
	st.global.f32 	[%rd14], %f7;

$L__BB16_2:
	ret;

}
.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<138>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd12;
	}
	shr.u32 	%r51, %r50, 20;
	setp.ne.s32 	%p1, %r51, 0;
	@%p1 bra 	$L__BB17_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd14;
	}
	shr.u32 	%r16, %r50, 20;
	add.s32 	%r51, %r16, -54;

$L__BB17_2:
	add.s32 	%r52, %r51, -1023;
	and.b32  	%r17, %r50, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd135, {%r49, %r18};
	setp.lt.u32 	%p2, %r18, 1073127583;
	@%p2 bra 	$L__BB17_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd135;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd135;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd135, {%r19, %r21};
	add.s32 	%r52, %r51, -1022;

$L__BB17_4:
	add.f64 	%fd15, %fd135, 0d3FF0000000000000;
	mov.f64 	%fd16, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd17, %fd15;
	neg.f64 	%fd18, %fd15;
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd16;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd17;
	add.f64 	%fd22, %fd135, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r52, -2147483648;
	mov.u32 	%r26, -2147483648;
	mov.u32 	%r27, 1127219200;
	mov.b64 	%fd79, {%r25, %r27};
	mov.b64 	%fd80, {%r26, %r27};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	shl.b32 	%r29, %r28, 1;
	setp.gt.u32 	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32 	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd16;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd16;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd136, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	%f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32 	%p4, %f1, 0f4086232B;
	@%p4 bra 	$L__BB17_7;

	setp.lt.f64 	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64 	%fd136, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32 	%p6, %f1, 0f40874800;
	@%p6 bra 	$L__BB17_7;

	mov.f64 	%fd134, 0d4338000000000000;
	mov.f64 	%fd133, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd132;
	}
	shr.u32 	%r36, %r48, 31;
	add.s32 	%r37, %r48, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r15, %r39;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r48, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd136, %fd130, %fd131;

$L__BB17_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd136;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.eq.s32 	%p7, %r46, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd136;
	}
	setp.eq.s32 	%p8, %r47, 0;
	and.pred  	%p9, %p8, %p7;
	@%p9 bra 	$L__BB17_9;

	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;

$L__BB17_9:
	st.param.f64 	[func_retval0+0], %fd136;
	ret;

}

