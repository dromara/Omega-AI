//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31833905
// Cuda compilation tools, release 11.8, V11.8.89
// Based on NVVM 7.0.1
//

.version 7.8
.target sm_52
.address_size 64

	// .globl	layernorm_forward_kernel
// _ZZ24layernorm_forward_kernelE10shared_sum has been demoted
// _ZZ24layernorm_forward_kernelE11shared_sum2 has been demoted
// _ZZ25layernorm_forward_kernel5E10shared_sum has been demoted
// _ZZ25layernorm_forward_kernel5E11shared_sum2 has been demoted
// _ZZ28layernorm_forward_np_kernel5E10shared_sum has been demoted
// _ZZ28layernorm_forward_np_kernel5E11shared_sum2 has been demoted
.extern .shared .align 16 .b8 shared[];

.visible .entry layernorm_forward_kernel(
	.param .u64 layernorm_forward_kernel_param_0,
	.param .u64 layernorm_forward_kernel_param_1,
	.param .u64 layernorm_forward_kernel_param_2,
	.param .u64 layernorm_forward_kernel_param_3,
	.param .u64 layernorm_forward_kernel_param_4,
	.param .u64 layernorm_forward_kernel_param_5,
	.param .u32 layernorm_forward_kernel_param_6,
	.param .u32 layernorm_forward_kernel_param_7
)
{
	.reg .pred 	%p<33>;
	.reg .f32 	%f<79>;
	.reg .b32 	%r<82>;
	.reg .b64 	%rd<27>;
	// demoted variable
	.shared .align 4 .b8 _ZZ24layernorm_forward_kernelE10shared_sum[128];
	// demoted variable
	.shared .align 4 .b8 _ZZ24layernorm_forward_kernelE11shared_sum2[128];

	ld.param.u64 	%rd5, [layernorm_forward_kernel_param_0];
	ld.param.u64 	%rd6, [layernorm_forward_kernel_param_1];
	ld.param.u64 	%rd7, [layernorm_forward_kernel_param_2];
	ld.param.u64 	%rd8, [layernorm_forward_kernel_param_3];
	ld.param.u64 	%rd9, [layernorm_forward_kernel_param_4];
	ld.param.u64 	%rd10, [layernorm_forward_kernel_param_5];
	ld.param.u32 	%r9, [layernorm_forward_kernel_param_7];
	mov.u32 	%r1, %ntid.x;
	shr.u32 	%r2, %r1, 5;
	mov.u32 	%r81, %tid.x;
	and.b32  	%r4, %r81, 31;
	mov.u32 	%r10, %ctaid.x;
	mul.lo.s32 	%r11, %r10, %r9;
	cvt.s64.s32 	%rd1, %r11;
	setp.ge.s32 	%p1, %r81, %r9;
	mov.f32 	%f77, 0f00000000;
	mov.f32 	%f75, %f77;
	mov.f32 	%f76, %f77;
	@%p1 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd2, %rd8;
	mov.u32 	%r80, %r81;

$L__BB0_2:
	cvt.s64.s32 	%rd11, %r80;
	add.s64 	%rd12, %rd11, %rd1;
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.f32 	%f17, [%rd14];
	add.f32 	%f75, %f75, %f17;
	fma.rn.f32 	%f76, %f17, %f17, %f76;
	add.s32 	%r80, %r80, %r1;
	setp.lt.s32 	%p2, %r80, %r9;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	mov.b32 	%r12, %f75;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p3, %r12, %r14, %r13, %r15;
	mov.b32 	%f19, %r16;
	add.f32 	%f20, %f75, %f19;
	mov.b32 	%r17, %f20;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p4, %r17, %r18, %r13, %r15;
	mov.b32 	%f21, %r19;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r20, %f22;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p5, %r20, %r21, %r13, %r15;
	mov.b32 	%f23, %r22;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r23, %f24;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p6, %r23, %r24, %r13, %r15;
	mov.b32 	%f25, %r25;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r26, %f26;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p7, %r26, %r27, %r13, %r15;
	mov.b32 	%f27, %r28;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r29, %f76;
	shfl.sync.bfly.b32 	%r30|%p8, %r29, %r14, %r13, %r15;
	mov.b32 	%f29, %r30;
	add.f32 	%f30, %f76, %f29;
	mov.b32 	%r31, %f30;
	shfl.sync.bfly.b32 	%r32|%p9, %r31, %r18, %r13, %r15;
	mov.b32 	%f31, %r32;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r33, %f32;
	shfl.sync.bfly.b32 	%r34|%p10, %r33, %r21, %r13, %r15;
	mov.b32 	%f33, %r34;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r35, %f34;
	shfl.sync.bfly.b32 	%r36|%p11, %r35, %r24, %r13, %r15;
	mov.b32 	%f35, %r36;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r37, %f36;
	shfl.sync.bfly.b32 	%r38|%p12, %r37, %r27, %r13, %r15;
	mov.b32 	%f37, %r38;
	add.f32 	%f38, %f36, %f37;
	shr.u32 	%r39, %r81, 3;
	and.b32  	%r40, %r39, 536870908;
	mov.u32 	%r41, _ZZ24layernorm_forward_kernelE10shared_sum;
	add.s32 	%r42, %r41, %r40;
	st.shared.f32 	[%r42], %f28;
	mov.u32 	%r43, _ZZ24layernorm_forward_kernelE11shared_sum2;
	add.s32 	%r44, %r43, %r40;
	st.shared.f32 	[%r44], %f38;
	bar.sync 	0;
	setp.ge.u32 	%p13, %r4, %r2;
	@%p13 bra 	$L__BB0_5;

	shl.b32 	%r45, %r4, 2;
	add.s32 	%r47, %r41, %r45;
	ld.shared.f32 	%f77, [%r47];

$L__BB0_5:
	mov.f32 	%f78, 0f00000000;
	@%p13 bra 	$L__BB0_7;

	shl.b32 	%r48, %r4, 2;
	add.s32 	%r50, %r43, %r48;
	ld.shared.f32 	%f78, [%r50];

$L__BB0_7:
	mov.b32 	%r51, %f77;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p15, %r51, %r53, %r52, %r54;
	mov.b32 	%f40, %r55;
	add.f32 	%f41, %f77, %f40;
	mov.b32 	%r56, %f41;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p16, %r56, %r57, %r52, %r54;
	mov.b32 	%f42, %r58;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r59, %f43;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p17, %r59, %r60, %r52, %r54;
	mov.b32 	%f44, %r61;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r62, %f45;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p18, %r62, %r63, %r52, %r54;
	mov.b32 	%f46, %r64;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r65, %f47;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p19, %r65, %r66, %r52, %r54;
	mov.b32 	%f48, %r67;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r68, %f78;
	shfl.sync.bfly.b32 	%r69|%p20, %r68, %r53, %r52, %r54;
	mov.b32 	%f50, %r69;
	add.f32 	%f51, %f78, %f50;
	mov.b32 	%r70, %f51;
	shfl.sync.bfly.b32 	%r71|%p21, %r70, %r57, %r52, %r54;
	mov.b32 	%f52, %r71;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r72, %f53;
	shfl.sync.bfly.b32 	%r73|%p22, %r72, %r60, %r52, %r54;
	mov.b32 	%f54, %r73;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r74, %f55;
	shfl.sync.bfly.b32 	%r75|%p23, %r74, %r63, %r52, %r54;
	mov.b32 	%f56, %r75;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r76, %f57;
	shfl.sync.bfly.b32 	%r77|%p24, %r76, %r66, %r52, %r54;
	mov.b32 	%f58, %r77;
	add.f32 	%f59, %f57, %f58;
	cvt.rn.f32.s32 	%f60, %r9;
	div.rn.f32 	%f11, %f49, %f60;
	div.rn.f32 	%f61, %f59, %f60;
	mul.f32 	%f62, %f11, %f11;
	sub.f32 	%f63, %f61, %f62;
	add.f32 	%f64, %f63, 0f3727C5AC;
	rsqrt.approx.f32 	%f12, %f64;
	setp.eq.s64 	%p25, %rd6, 0;
	setp.ne.s32 	%p26, %r81, 0;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB0_9;

	mul.wide.s32 	%rd16, %r10, 4;
	add.s64 	%rd15, %rd6, %rd16;
	// begin inline asm
	st.global.cs.f32 [%rd15], %f11;
	// end inline asm

$L__BB0_9:
	setp.eq.s64 	%p29, %rd7, 0;
	or.pred  	%p30, %p26, %p29;
	@%p30 bra 	$L__BB0_11;

	mul.wide.s32 	%rd18, %r10, 4;
	add.s64 	%rd17, %rd7, %rd18;
	// begin inline asm
	st.global.cs.f32 [%rd17], %f12;
	// end inline asm

$L__BB0_11:
	@%p1 bra 	$L__BB0_14;

	cvta.to.global.u64 	%rd3, %rd10;
	cvta.to.global.u64 	%rd4, %rd9;

$L__BB0_13:
	cvt.s64.s32 	%rd21, %r81;
	add.s64 	%rd22, %rd21, %rd1;
	shl.b64 	%rd23, %rd22, 2;
	add.s64 	%rd19, %rd8, %rd23;
	// begin inline asm
	ld.global.cs.f32 %f67, [%rd19];
	// end inline asm
	sub.f32 	%f69, %f67, %f11;
	mul.f32 	%f70, %f12, %f69;
	add.s64 	%rd20, %rd5, %rd23;
	mul.wide.s32 	%rd24, %r81, 4;
	add.s64 	%rd25, %rd4, %rd24;
	ld.global.nc.f32 	%f71, [%rd25];
	add.s64 	%rd26, %rd3, %rd24;
	ld.global.nc.f32 	%f72, [%rd26];
	fma.rn.f32 	%f68, %f71, %f70, %f72;
	// begin inline asm
	st.global.cs.f32 [%rd20], %f68;
	// end inline asm
	add.s32 	%r81, %r81, %r1;
	setp.lt.s32 	%p32, %r81, %r9;
	@%p32 bra 	$L__BB0_13;

$L__BB0_14:
	ret;

}
	// .globl	layernorm_backward_kernel
.visible .entry layernorm_backward_kernel(
	.param .u64 layernorm_backward_kernel_param_0,
	.param .u64 layernorm_backward_kernel_param_1,
	.param .u64 layernorm_backward_kernel_param_2,
	.param .u64 layernorm_backward_kernel_param_3,
	.param .u64 layernorm_backward_kernel_param_4,
	.param .u64 layernorm_backward_kernel_param_5,
	.param .u64 layernorm_backward_kernel_param_6,
	.param .u64 layernorm_backward_kernel_param_7,
	.param .u64 layernorm_backward_kernel_param_8,
	.param .u32 layernorm_backward_kernel_param_9,
	.param .u32 layernorm_backward_kernel_param_10,
	.param .u32 layernorm_backward_kernel_param_11
)
{
	.reg .pred 	%p<35>;
	.reg .f32 	%f<123>;
	.reg .b32 	%r<128>;
	.reg .b64 	%rd<84>;


	ld.param.u64 	%rd19, [layernorm_backward_kernel_param_0];
	ld.param.u64 	%rd26, [layernorm_backward_kernel_param_3];
	ld.param.u64 	%rd22, [layernorm_backward_kernel_param_4];
	ld.param.u64 	%rd23, [layernorm_backward_kernel_param_5];
	ld.param.u64 	%rd27, [layernorm_backward_kernel_param_6];
	ld.param.u64 	%rd24, [layernorm_backward_kernel_param_7];
	ld.param.u64 	%rd25, [layernorm_backward_kernel_param_8];
	ld.param.u32 	%r41, [layernorm_backward_kernel_param_9];
	ld.param.u32 	%r42, [layernorm_backward_kernel_param_10];
	ld.param.u32 	%r43, [layernorm_backward_kernel_param_11];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd23;
	cvta.to.global.u64 	%rd3, %rd27;
	cvta.to.global.u64 	%rd4, %rd26;
	mov.u32 	%r1, WARP_SZ;
	mov.u32 	%r2, %tid.x;
	div.u32 	%r44, %r2, %r1;
	mov.u32 	%r3, %ntid.x;
	div.u32 	%r4, %r3, %r1;
	mov.u32 	%r45, %ctaid.x;
	mad.lo.s32 	%r122, %r4, %r45, %r44;
	mul.lo.s32 	%r46, %r44, %r1;
	sub.s32 	%r6, %r2, %r46;
	mov.u32 	%r7, %nctaid.x;
	setp.ge.s32 	%p1, %r2, %r43;
	@%p1 bra 	$L__BB1_6;

	shl.b32 	%r8, %r3, 2;
	mov.u32 	%r121, %r2;

$L__BB1_2:
	.pragma "nounroll";
	shl.b32 	%r47, %r121, 2;
	mov.u32 	%r48, shared;
	add.s32 	%r10, %r48, %r47;
	mov.u32 	%r49, 0;
	st.shared.u32 	[%r10], %r49;
	shl.b32 	%r50, %r43, 2;
	add.s32 	%r11, %r10, %r50;
	st.shared.u32 	[%r11], %r49;
	add.s32 	%r12, %r121, %r3;
	setp.ge.s32 	%p2, %r12, %r43;
	@%p2 bra 	$L__BB1_6;

	add.s32 	%r13, %r10, %r8;
	st.shared.u32 	[%r13], %r49;
	add.s32 	%r14, %r11, %r8;
	st.shared.u32 	[%r14], %r49;
	add.s32 	%r15, %r12, %r3;
	setp.ge.s32 	%p3, %r15, %r43;
	@%p3 bra 	$L__BB1_6;

	add.s32 	%r16, %r13, %r8;
	mov.u32 	%r52, 0;
	st.shared.u32 	[%r16], %r52;
	add.s32 	%r17, %r14, %r8;
	st.shared.u32 	[%r17], %r52;
	add.s32 	%r18, %r15, %r3;
	setp.ge.s32 	%p4, %r18, %r43;
	@%p4 bra 	$L__BB1_6;

	add.s32 	%r53, %r16, %r8;
	st.shared.u32 	[%r53], %r52;
	add.s32 	%r55, %r17, %r8;
	st.shared.u32 	[%r55], %r52;
	add.s32 	%r121, %r18, %r3;
	setp.lt.s32 	%p5, %r121, %r43;
	@%p5 bra 	$L__BB1_2;

$L__BB1_6:
	mov.u32 	%r57, shared;
	bar.sync 	0;
	mul.lo.s32 	%r22, %r42, %r41;
	setp.ge.s32 	%p6, %r122, %r22;
	@%p6 bra 	$L__BB1_20;

	cvt.rn.f32.s32 	%f1, %r43;
	sub.s32 	%r58, %r2, %r6;
	add.s32 	%r59, %r58, %r43;
	not.b32 	%r60, %r2;
	add.s32 	%r61, %r59, %r60;
	div.u32 	%r23, %r61, %r1;
	add.s32 	%r62, %r23, 1;
	and.b32  	%r24, %r62, 3;
	cvt.s64.s32 	%rd5, %r6;
	mul.wide.s32 	%rd28, %r6, 4;
	add.s64 	%rd6, %rd3, %rd28;
	add.s32 	%r25, %r6, %r1;
	cvt.s64.s32 	%rd7, %r25;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd8, %rd6, %rd11;
	add.s32 	%r26, %r25, %r1;
	cvt.s64.s32 	%rd9, %r26;
	add.s64 	%rd10, %rd8, %rd11;
	add.s32 	%r27, %r26, %r1;
	cvta.to.global.u64 	%rd12, %rd19;
	cvta.to.global.u64 	%rd13, %rd25;
	cvta.to.global.u64 	%rd14, %rd24;
	mul.lo.s32 	%r28, %r4, %r7;

$L__BB1_8:
	rem.s32 	%r63, %r122, %r42;
	sub.s32 	%r64, %r122, %r63;
	mul.lo.s32 	%r65, %r64, %r43;
	cvt.s64.s32 	%rd29, %r65;
	mul.lo.s32 	%r66, %r63, %r43;
	cvt.s64.s32 	%rd30, %r66;
	add.s64 	%rd15, %rd29, %rd30;
	mul.wide.s32 	%rd31, %r122, 4;
	add.s64 	%rd32, %rd14, %rd31;
	ld.global.f32 	%f2, [%rd32];
	add.s64 	%rd33, %rd13, %rd31;
	ld.global.f32 	%f3, [%rd33];
	setp.ge.s32 	%p7, %r6, %r43;
	mov.f32 	%f121, 0f00000000;
	mov.f32 	%f122, %f121;
	@%p7 bra 	$L__BB1_16;

	setp.eq.s32 	%p8, %r24, 0;
	mov.f32 	%f122, 0f00000000;
	mov.u32 	%r123, %r6;
	mov.f32 	%f121, %f122;
	@%p8 bra 	$L__BB1_13;

	setp.eq.s32 	%p9, %r24, 1;
	add.s64 	%rd34, %rd15, %rd5;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.f32 	%f29, [%rd36];
	sub.f32 	%f30, %f29, %f2;
	mul.f32 	%f31, %f3, %f30;
	add.s64 	%rd37, %rd1, %rd35;
	ld.global.f32 	%f32, [%rd37];
	ld.global.f32 	%f33, [%rd6];
	mul.f32 	%f34, %f33, %f32;
	add.f32 	%f121, %f34, 0f00000000;
	fma.rn.f32 	%f122, %f31, %f34, 0f00000000;
	mov.u32 	%r123, %r25;
	@%p9 bra 	$L__BB1_13;

	setp.eq.s32 	%p10, %r24, 2;
	add.s64 	%rd38, %rd15, %rd7;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f35, [%rd40];
	sub.f32 	%f36, %f35, %f2;
	mul.f32 	%f37, %f3, %f36;
	add.s64 	%rd41, %rd1, %rd39;
	ld.global.f32 	%f38, [%rd41];
	ld.global.f32 	%f39, [%rd8];
	mul.f32 	%f40, %f39, %f38;
	add.f32 	%f121, %f121, %f40;
	fma.rn.f32 	%f122, %f37, %f40, %f122;
	mov.u32 	%r123, %r26;
	@%p10 bra 	$L__BB1_13;

	add.s64 	%rd42, %rd15, %rd9;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.f32 	%f41, [%rd44];
	sub.f32 	%f42, %f41, %f2;
	mul.f32 	%f43, %f3, %f42;
	add.s64 	%rd45, %rd1, %rd43;
	ld.global.f32 	%f44, [%rd45];
	ld.global.f32 	%f45, [%rd10];
	mul.f32 	%f46, %f45, %f44;
	add.f32 	%f121, %f121, %f46;
	fma.rn.f32 	%f122, %f43, %f46, %f122;
	mov.u32 	%r123, %r27;

$L__BB1_13:
	setp.lt.u32 	%p11, %r23, 3;
	@%p11 bra 	$L__BB1_16;

$L__BB1_15:
	cvt.s64.s32 	%rd46, %r123;
	add.s64 	%rd47, %rd15, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd2, %rd48;
	ld.global.f32 	%f47, [%rd49];
	sub.f32 	%f48, %f47, %f2;
	mul.f32 	%f49, %f3, %f48;
	mul.wide.s32 	%rd50, %r123, 4;
	add.s64 	%rd51, %rd3, %rd50;
	add.s64 	%rd52, %rd1, %rd48;
	ld.global.f32 	%f50, [%rd52];
	ld.global.f32 	%f51, [%rd51];
	mul.f32 	%f52, %f51, %f50;
	add.f32 	%f53, %f121, %f52;
	fma.rn.f32 	%f54, %f49, %f52, %f122;
	add.s64 	%rd53, %rd49, %rd11;
	ld.global.f32 	%f55, [%rd53];
	sub.f32 	%f56, %f55, %f2;
	mul.f32 	%f57, %f3, %f56;
	add.s64 	%rd54, %rd51, %rd11;
	add.s64 	%rd55, %rd52, %rd11;
	ld.global.f32 	%f58, [%rd55];
	ld.global.f32 	%f59, [%rd54];
	mul.f32 	%f60, %f59, %f58;
	add.f32 	%f61, %f53, %f60;
	fma.rn.f32 	%f62, %f57, %f60, %f54;
	add.s32 	%r67, %r123, %r1;
	add.s32 	%r68, %r67, %r1;
	add.s64 	%rd56, %rd53, %rd11;
	ld.global.f32 	%f63, [%rd56];
	sub.f32 	%f64, %f63, %f2;
	mul.f32 	%f65, %f3, %f64;
	add.s64 	%rd57, %rd54, %rd11;
	add.s64 	%rd58, %rd55, %rd11;
	ld.global.f32 	%f66, [%rd58];
	ld.global.f32 	%f67, [%rd57];
	mul.f32 	%f68, %f67, %f66;
	add.f32 	%f69, %f61, %f68;
	fma.rn.f32 	%f70, %f65, %f68, %f62;
	add.s32 	%r69, %r68, %r1;
	add.s64 	%rd59, %rd56, %rd11;
	ld.global.f32 	%f71, [%rd59];
	sub.f32 	%f72, %f71, %f2;
	mul.f32 	%f73, %f3, %f72;
	add.s64 	%rd60, %rd57, %rd11;
	add.s64 	%rd61, %rd58, %rd11;
	ld.global.f32 	%f74, [%rd61];
	ld.global.f32 	%f75, [%rd60];
	mul.f32 	%f76, %f75, %f74;
	add.f32 	%f121, %f69, %f76;
	fma.rn.f32 	%f122, %f73, %f76, %f70;
	add.s32 	%r123, %r69, %r1;
	setp.lt.s32 	%p12, %r123, %r43;
	@%p12 bra 	$L__BB1_15;

$L__BB1_16:
	mov.b32 	%r70, %f121;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p13, %r70, %r72, %r71, %r73;
	mov.b32 	%f77, %r74;
	add.f32 	%f78, %f121, %f77;
	mov.b32 	%r75, %f78;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p14, %r75, %r76, %r71, %r73;
	mov.b32 	%f79, %r77;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r78, %f80;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p15, %r78, %r79, %r71, %r73;
	mov.b32 	%f81, %r80;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r81, %f82;
	mov.u32 	%r82, 2;
	shfl.sync.bfly.b32 	%r83|%p16, %r81, %r82, %r71, %r73;
	mov.b32 	%f83, %r83;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r84, %f84;
	mov.u32 	%r85, 1;
	shfl.sync.bfly.b32 	%r86|%p17, %r84, %r85, %r71, %r73;
	mov.b32 	%f85, %r86;
	mov.b32 	%r87, %f122;
	shfl.sync.bfly.b32 	%r88|%p18, %r87, %r72, %r71, %r73;
	mov.b32 	%f86, %r88;
	add.f32 	%f87, %f122, %f86;
	mov.b32 	%r89, %f87;
	shfl.sync.bfly.b32 	%r90|%p19, %r89, %r76, %r71, %r73;
	mov.b32 	%f88, %r90;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r91, %f89;
	shfl.sync.bfly.b32 	%r92|%p20, %r91, %r79, %r71, %r73;
	mov.b32 	%f90, %r92;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r93, %f91;
	shfl.sync.bfly.b32 	%r94|%p21, %r93, %r82, %r71, %r73;
	mov.b32 	%f92, %r94;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r95, %f93;
	shfl.sync.bfly.b32 	%r96|%p22, %r95, %r85, %r71, %r73;
	mov.b32 	%f94, %r96;
	add.f32 	%f20, %f93, %f94;
	add.f32 	%f21, %f84, %f85;
	@%p7 bra 	$L__BB1_19;

	div.rn.f32 	%f22, %f20, %f1;
	div.rn.f32 	%f23, %f21, %f1;
	mov.u32 	%r125, %r6;

$L__BB1_18:
	cvt.s64.s32 	%rd64, %r125;
	add.s64 	%rd65, %rd15, %rd64;
	shl.b64 	%rd66, %rd65, 2;
	add.s64 	%rd62, %rd22, %rd66;
	// begin inline asm
	ld.global.cs.f32 %f95, [%rd62];
	// end inline asm
	add.s64 	%rd63, %rd23, %rd66;
	// begin inline asm
	ld.global.cs.f32 %f96, [%rd63];
	// end inline asm
	sub.f32 	%f97, %f96, %f2;
	mul.f32 	%f98, %f3, %f97;
	mul.wide.s32 	%rd67, %r125, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.global.f32 	%f99, [%rd68];
	shl.b32 	%r97, %r125, 2;
	add.s32 	%r99, %r57, %r97;
	atom.shared.add.f32 	%f100, [%r99], %f95;
	shl.b32 	%r100, %r43, 2;
	add.s32 	%r101, %r99, %r100;
	mul.f32 	%f101, %f95, %f98;
	atom.shared.add.f32 	%f102, [%r101], %f101;
	fma.rn.f32 	%f103, %f95, %f99, 0f00000000;
	sub.f32 	%f104, %f103, %f23;
	mul.f32 	%f105, %f22, %f98;
	sub.f32 	%f106, %f104, %f105;
	add.s64 	%rd69, %rd12, %rd66;
	ld.global.f32 	%f107, [%rd69];
	fma.rn.f32 	%f108, %f3, %f106, %f107;
	st.global.f32 	[%rd69], %f108;
	add.s32 	%r125, %r125, %r1;
	setp.lt.s32 	%p24, %r125, %r43;
	@%p24 bra 	$L__BB1_18;

$L__BB1_19:
	add.s32 	%r122, %r122, %r28;
	setp.lt.s32 	%p25, %r122, %r22;
	@%p25 bra 	$L__BB1_8;

$L__BB1_20:
	mov.u32 	%r115, %tid.x;
	setp.ge.s32 	%p33, %r115, %r43;
	bar.sync 	0;
	cvt.s64.s32 	%rd16, %r43;
	@%p33 bra 	$L__BB1_23;

	mov.u32 	%r126, %tid.x;
	shl.b64 	%rd72, %rd16, 2;
	shl.b32 	%r105, %r43, 2;

$L__BB1_22:
	mul.wide.s32 	%rd70, %r126, 4;
	add.s64 	%rd71, %rd4, %rd70;
	shl.b32 	%r102, %r126, 2;
	add.s32 	%r104, %r57, %r102;
	ld.shared.f32 	%f109, [%r104];
	atom.global.add.f32 	%f110, [%rd71], %f109;
	add.s64 	%rd73, %rd71, %rd72;
	add.s32 	%r106, %r104, %r105;
	ld.shared.f32 	%f111, [%r106];
	atom.global.add.f32 	%f112, [%rd73], %f111;
	add.s32 	%r126, %r126, %r3;
	setp.lt.s32 	%p27, %r126, %r43;
	@%p27 bra 	$L__BB1_22;

$L__BB1_23:
	mov.u32 	%r110, %tid.x;
	bar.sync 	0;
	setp.ne.s32 	%p28, %r110, 0;
	@%p28 bra 	$L__BB1_25;

	shl.b32 	%r120, %r43, 3;
	add.s32 	%r119, %r57, %r120;
	shl.b32 	%r113, %r43, 1;
	mul.wide.s32 	%rd74, %r113, 4;
	add.s64 	%rd75, %rd4, %rd74;
	atom.global.add.u32 	%r107, [%rd75], 1;
	st.shared.u32 	[%r119], %r107;

$L__BB1_25:
	shl.b32 	%r118, %r43, 3;
	add.s32 	%r117, %r57, %r118;
	mov.u32 	%r116, %tid.x;
	setp.ge.s32 	%p34, %r116, %r43;
	mov.u32 	%r111, %nctaid.x;
	bar.sync 	0;
	add.s32 	%r108, %r111, -1;
	ld.shared.u32 	%r109, [%r117];
	setp.ne.s32 	%p30, %r109, %r108;
	or.pred  	%p31, %p30, %p34;
	@%p31 bra 	$L__BB1_28;

	ld.param.u64 	%rd83, [layernorm_backward_kernel_param_1];
	ld.param.u64 	%rd82, [layernorm_backward_kernel_param_2];
	mov.u32 	%r127, %tid.x;
	cvta.to.global.u64 	%rd17, %rd82;
	cvta.to.global.u64 	%rd18, %rd83;
	shl.b64 	%rd79, %rd16, 2;

$L__BB1_27:
	mul.wide.s32 	%rd76, %r127, 4;
	add.s64 	%rd77, %rd4, %rd76;
	ld.global.f32 	%f113, [%rd77];
	add.s64 	%rd78, %rd17, %rd76;
	st.global.f32 	[%rd78], %f113;
	add.s64 	%rd80, %rd77, %rd79;
	ld.global.f32 	%f114, [%rd80];
	add.s64 	%rd81, %rd18, %rd76;
	st.global.f32 	[%rd81], %f114;
	add.s32 	%r127, %r127, %r3;
	setp.lt.s32 	%p32, %r127, %r43;
	@%p32 bra 	$L__BB1_27;

$L__BB1_28:
	ret;

}
	// .globl	layernorm_forward_kernel5
.visible .entry layernorm_forward_kernel5(
	.param .u64 layernorm_forward_kernel5_param_0,
	.param .u64 layernorm_forward_kernel5_param_1,
	.param .u64 layernorm_forward_kernel5_param_2,
	.param .u64 layernorm_forward_kernel5_param_3,
	.param .u64 layernorm_forward_kernel5_param_4,
	.param .u64 layernorm_forward_kernel5_param_5,
	.param .u32 layernorm_forward_kernel5_param_6,
	.param .u32 layernorm_forward_kernel5_param_7
)
{
	.reg .pred 	%p<33>;
	.reg .f32 	%f<79>;
	.reg .b32 	%r<82>;
	.reg .b64 	%rd<27>;
	// demoted variable
	.shared .align 4 .b8 _ZZ25layernorm_forward_kernel5E10shared_sum[128];
	// demoted variable
	.shared .align 4 .b8 _ZZ25layernorm_forward_kernel5E11shared_sum2[128];

	ld.param.u64 	%rd5, [layernorm_forward_kernel5_param_0];
	ld.param.u64 	%rd6, [layernorm_forward_kernel5_param_1];
	ld.param.u64 	%rd7, [layernorm_forward_kernel5_param_2];
	ld.param.u64 	%rd8, [layernorm_forward_kernel5_param_3];
	ld.param.u64 	%rd9, [layernorm_forward_kernel5_param_4];
	ld.param.u64 	%rd10, [layernorm_forward_kernel5_param_5];
	ld.param.u32 	%r9, [layernorm_forward_kernel5_param_7];
	mov.u32 	%r1, %ntid.x;
	shr.u32 	%r2, %r1, 5;
	mov.u32 	%r81, %tid.x;
	and.b32  	%r4, %r81, 31;
	mov.u32 	%r10, %ctaid.x;
	mul.lo.s32 	%r11, %r10, %r9;
	cvt.s64.s32 	%rd1, %r11;
	setp.ge.s32 	%p1, %r81, %r9;
	mov.f32 	%f77, 0f00000000;
	mov.f32 	%f75, %f77;
	mov.f32 	%f76, %f77;
	@%p1 bra 	$L__BB2_3;

	cvta.to.global.u64 	%rd2, %rd8;
	mov.u32 	%r80, %r81;

$L__BB2_2:
	cvt.s64.s32 	%rd11, %r80;
	add.s64 	%rd12, %rd11, %rd1;
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.f32 	%f17, [%rd14];
	add.f32 	%f75, %f75, %f17;
	fma.rn.f32 	%f76, %f17, %f17, %f76;
	add.s32 	%r80, %r80, %r1;
	setp.lt.s32 	%p2, %r80, %r9;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	mov.b32 	%r12, %f75;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p3, %r12, %r14, %r13, %r15;
	mov.b32 	%f19, %r16;
	add.f32 	%f20, %f75, %f19;
	mov.b32 	%r17, %f20;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p4, %r17, %r18, %r13, %r15;
	mov.b32 	%f21, %r19;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r20, %f22;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p5, %r20, %r21, %r13, %r15;
	mov.b32 	%f23, %r22;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r23, %f24;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p6, %r23, %r24, %r13, %r15;
	mov.b32 	%f25, %r25;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r26, %f26;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p7, %r26, %r27, %r13, %r15;
	mov.b32 	%f27, %r28;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r29, %f76;
	shfl.sync.bfly.b32 	%r30|%p8, %r29, %r14, %r13, %r15;
	mov.b32 	%f29, %r30;
	add.f32 	%f30, %f76, %f29;
	mov.b32 	%r31, %f30;
	shfl.sync.bfly.b32 	%r32|%p9, %r31, %r18, %r13, %r15;
	mov.b32 	%f31, %r32;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r33, %f32;
	shfl.sync.bfly.b32 	%r34|%p10, %r33, %r21, %r13, %r15;
	mov.b32 	%f33, %r34;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r35, %f34;
	shfl.sync.bfly.b32 	%r36|%p11, %r35, %r24, %r13, %r15;
	mov.b32 	%f35, %r36;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r37, %f36;
	shfl.sync.bfly.b32 	%r38|%p12, %r37, %r27, %r13, %r15;
	mov.b32 	%f37, %r38;
	add.f32 	%f38, %f36, %f37;
	shr.u32 	%r39, %r81, 3;
	and.b32  	%r40, %r39, 536870908;
	mov.u32 	%r41, _ZZ25layernorm_forward_kernel5E10shared_sum;
	add.s32 	%r42, %r41, %r40;
	st.shared.f32 	[%r42], %f28;
	mov.u32 	%r43, _ZZ25layernorm_forward_kernel5E11shared_sum2;
	add.s32 	%r44, %r43, %r40;
	st.shared.f32 	[%r44], %f38;
	bar.sync 	0;
	setp.ge.u32 	%p13, %r4, %r2;
	@%p13 bra 	$L__BB2_5;

	shl.b32 	%r45, %r4, 2;
	add.s32 	%r47, %r41, %r45;
	ld.shared.f32 	%f77, [%r47];

$L__BB2_5:
	mov.f32 	%f78, 0f00000000;
	@%p13 bra 	$L__BB2_7;

	shl.b32 	%r48, %r4, 2;
	add.s32 	%r50, %r43, %r48;
	ld.shared.f32 	%f78, [%r50];

$L__BB2_7:
	mov.b32 	%r51, %f77;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p15, %r51, %r53, %r52, %r54;
	mov.b32 	%f40, %r55;
	add.f32 	%f41, %f77, %f40;
	mov.b32 	%r56, %f41;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p16, %r56, %r57, %r52, %r54;
	mov.b32 	%f42, %r58;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r59, %f43;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p17, %r59, %r60, %r52, %r54;
	mov.b32 	%f44, %r61;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r62, %f45;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p18, %r62, %r63, %r52, %r54;
	mov.b32 	%f46, %r64;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r65, %f47;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p19, %r65, %r66, %r52, %r54;
	mov.b32 	%f48, %r67;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r68, %f78;
	shfl.sync.bfly.b32 	%r69|%p20, %r68, %r53, %r52, %r54;
	mov.b32 	%f50, %r69;
	add.f32 	%f51, %f78, %f50;
	mov.b32 	%r70, %f51;
	shfl.sync.bfly.b32 	%r71|%p21, %r70, %r57, %r52, %r54;
	mov.b32 	%f52, %r71;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r72, %f53;
	shfl.sync.bfly.b32 	%r73|%p22, %r72, %r60, %r52, %r54;
	mov.b32 	%f54, %r73;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r74, %f55;
	shfl.sync.bfly.b32 	%r75|%p23, %r74, %r63, %r52, %r54;
	mov.b32 	%f56, %r75;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r76, %f57;
	shfl.sync.bfly.b32 	%r77|%p24, %r76, %r66, %r52, %r54;
	mov.b32 	%f58, %r77;
	add.f32 	%f59, %f57, %f58;
	cvt.rn.f32.s32 	%f60, %r9;
	div.rn.f32 	%f11, %f49, %f60;
	div.rn.f32 	%f61, %f59, %f60;
	mul.f32 	%f62, %f11, %f11;
	sub.f32 	%f63, %f61, %f62;
	add.f32 	%f64, %f63, 0f3727C5AC;
	rsqrt.approx.f32 	%f12, %f64;
	setp.eq.s64 	%p25, %rd6, 0;
	setp.ne.s32 	%p26, %r81, 0;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB2_9;

	mul.wide.s32 	%rd16, %r10, 4;
	add.s64 	%rd15, %rd6, %rd16;
	// begin inline asm
	st.global.cs.f32 [%rd15], %f11;
	// end inline asm

$L__BB2_9:
	setp.eq.s64 	%p29, %rd7, 0;
	or.pred  	%p30, %p26, %p29;
	@%p30 bra 	$L__BB2_11;

	mul.wide.s32 	%rd18, %r10, 4;
	add.s64 	%rd17, %rd7, %rd18;
	// begin inline asm
	st.global.cs.f32 [%rd17], %f12;
	// end inline asm

$L__BB2_11:
	@%p1 bra 	$L__BB2_14;

	cvta.to.global.u64 	%rd3, %rd10;
	cvta.to.global.u64 	%rd4, %rd9;

$L__BB2_13:
	cvt.s64.s32 	%rd21, %r81;
	add.s64 	%rd22, %rd21, %rd1;
	shl.b64 	%rd23, %rd22, 2;
	add.s64 	%rd19, %rd8, %rd23;
	// begin inline asm
	ld.global.cs.f32 %f67, [%rd19];
	// end inline asm
	sub.f32 	%f69, %f67, %f11;
	mul.f32 	%f70, %f12, %f69;
	add.s64 	%rd20, %rd5, %rd23;
	mul.wide.s32 	%rd24, %r81, 4;
	add.s64 	%rd25, %rd4, %rd24;
	ld.global.nc.f32 	%f71, [%rd25];
	add.s64 	%rd26, %rd3, %rd24;
	ld.global.nc.f32 	%f72, [%rd26];
	fma.rn.f32 	%f68, %f71, %f70, %f72;
	// begin inline asm
	st.global.cs.f32 [%rd20], %f68;
	// end inline asm
	add.s32 	%r81, %r81, %r1;
	setp.lt.s32 	%p32, %r81, %r9;
	@%p32 bra 	$L__BB2_13;

$L__BB2_14:
	ret;

}
	// .globl	layernorm_forward_np_kernel5
.visible .entry layernorm_forward_np_kernel5(
	.param .u64 layernorm_forward_np_kernel5_param_0,
	.param .u64 layernorm_forward_np_kernel5_param_1,
	.param .u64 layernorm_forward_np_kernel5_param_2,
	.param .u64 layernorm_forward_np_kernel5_param_3,
	.param .u32 layernorm_forward_np_kernel5_param_4,
	.param .u32 layernorm_forward_np_kernel5_param_5,
	.param .f32 layernorm_forward_np_kernel5_param_6
)
{
	.reg .pred 	%p<33>;
	.reg .f32 	%f<77>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<20>;
	// demoted variable
	.shared .align 4 .b8 _ZZ28layernorm_forward_np_kernel5E10shared_sum[128];
	// demoted variable
	.shared .align 4 .b8 _ZZ28layernorm_forward_np_kernel5E11shared_sum2[128];

	ld.param.u64 	%rd3, [layernorm_forward_np_kernel5_param_0];
	ld.param.u64 	%rd4, [layernorm_forward_np_kernel5_param_1];
	ld.param.u64 	%rd5, [layernorm_forward_np_kernel5_param_2];
	ld.param.u64 	%rd6, [layernorm_forward_np_kernel5_param_3];
	ld.param.u32 	%r10, [layernorm_forward_np_kernel5_param_5];
	ld.param.f32 	%f13, [layernorm_forward_np_kernel5_param_6];
	mov.u32 	%r1, %ntid.x;
	shr.u32 	%r2, %r1, 5;
	mov.u32 	%r79, %tid.x;
	and.b32  	%r4, %r79, 31;
	mov.u32 	%r5, %ctaid.x;
	mul.lo.s32 	%r11, %r5, %r10;
	cvt.s64.s32 	%rd1, %r11;
	setp.ge.s32 	%p1, %r79, %r10;
	mov.f32 	%f75, 0f00000000;
	mov.f32 	%f73, %f75;
	mov.f32 	%f74, %f75;
	@%p1 bra 	$L__BB3_3;

	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r78, %r79;

$L__BB3_2:
	cvt.s64.s32 	%rd7, %r78;
	add.s64 	%rd8, %rd7, %rd1;
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.f32 	%f18, [%rd10];
	add.f32 	%f73, %f73, %f18;
	fma.rn.f32 	%f74, %f18, %f18, %f74;
	add.s32 	%r78, %r78, %r1;
	setp.lt.s32 	%p2, %r78, %r10;
	@%p2 bra 	$L__BB3_2;

$L__BB3_3:
	mov.b32 	%r12, %f73;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p3, %r12, %r14, %r13, %r15;
	mov.b32 	%f20, %r16;
	add.f32 	%f21, %f73, %f20;
	mov.b32 	%r17, %f21;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p4, %r17, %r18, %r13, %r15;
	mov.b32 	%f22, %r19;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r20, %f23;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p5, %r20, %r21, %r13, %r15;
	mov.b32 	%f24, %r22;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r23, %f25;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p6, %r23, %r24, %r13, %r15;
	mov.b32 	%f26, %r25;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r26, %f27;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p7, %r26, %r27, %r13, %r15;
	mov.b32 	%f28, %r28;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r29, %f74;
	shfl.sync.bfly.b32 	%r30|%p8, %r29, %r14, %r13, %r15;
	mov.b32 	%f30, %r30;
	add.f32 	%f31, %f74, %f30;
	mov.b32 	%r31, %f31;
	shfl.sync.bfly.b32 	%r32|%p9, %r31, %r18, %r13, %r15;
	mov.b32 	%f32, %r32;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r33, %f33;
	shfl.sync.bfly.b32 	%r34|%p10, %r33, %r21, %r13, %r15;
	mov.b32 	%f34, %r34;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r35, %f35;
	shfl.sync.bfly.b32 	%r36|%p11, %r35, %r24, %r13, %r15;
	mov.b32 	%f36, %r36;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r37, %f37;
	shfl.sync.bfly.b32 	%r38|%p12, %r37, %r27, %r13, %r15;
	mov.b32 	%f38, %r38;
	add.f32 	%f39, %f37, %f38;
	shr.u32 	%r39, %r79, 3;
	and.b32  	%r40, %r39, 536870908;
	mov.u32 	%r41, _ZZ28layernorm_forward_np_kernel5E10shared_sum;
	add.s32 	%r42, %r41, %r40;
	st.shared.f32 	[%r42], %f29;
	mov.u32 	%r43, _ZZ28layernorm_forward_np_kernel5E11shared_sum2;
	add.s32 	%r44, %r43, %r40;
	st.shared.f32 	[%r44], %f39;
	bar.sync 	0;
	setp.ge.u32 	%p13, %r4, %r2;
	@%p13 bra 	$L__BB3_5;

	shl.b32 	%r45, %r4, 2;
	add.s32 	%r47, %r41, %r45;
	ld.shared.f32 	%f75, [%r47];

$L__BB3_5:
	mov.f32 	%f76, 0f00000000;
	@%p13 bra 	$L__BB3_7;

	shl.b32 	%r48, %r4, 2;
	add.s32 	%r50, %r43, %r48;
	ld.shared.f32 	%f76, [%r50];

$L__BB3_7:
	mov.b32 	%r51, %f75;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p15, %r51, %r53, %r52, %r54;
	mov.b32 	%f41, %r55;
	add.f32 	%f42, %f75, %f41;
	mov.b32 	%r56, %f42;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p16, %r56, %r57, %r52, %r54;
	mov.b32 	%f43, %r58;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r59, %f44;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p17, %r59, %r60, %r52, %r54;
	mov.b32 	%f45, %r61;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r62, %f46;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p18, %r62, %r63, %r52, %r54;
	mov.b32 	%f47, %r64;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r65, %f48;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p19, %r65, %r66, %r52, %r54;
	mov.b32 	%f49, %r67;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r68, %f76;
	shfl.sync.bfly.b32 	%r69|%p20, %r68, %r53, %r52, %r54;
	mov.b32 	%f51, %r69;
	add.f32 	%f52, %f76, %f51;
	mov.b32 	%r70, %f52;
	shfl.sync.bfly.b32 	%r71|%p21, %r70, %r57, %r52, %r54;
	mov.b32 	%f53, %r71;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r72, %f54;
	shfl.sync.bfly.b32 	%r73|%p22, %r72, %r60, %r52, %r54;
	mov.b32 	%f55, %r73;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r74, %f56;
	shfl.sync.bfly.b32 	%r75|%p23, %r74, %r63, %r52, %r54;
	mov.b32 	%f57, %r75;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r76, %f58;
	shfl.sync.bfly.b32 	%r77|%p24, %r76, %r66, %r52, %r54;
	mov.b32 	%f59, %r77;
	add.f32 	%f60, %f58, %f59;
	cvt.rn.f32.s32 	%f61, %r10;
	div.rn.f32 	%f11, %f50, %f61;
	div.rn.f32 	%f62, %f60, %f61;
	mul.f32 	%f63, %f11, %f11;
	sub.f32 	%f64, %f62, %f63;
	add.f32 	%f65, %f64, %f13;
	rsqrt.approx.f32 	%f12, %f65;
	setp.eq.s64 	%p25, %rd4, 0;
	setp.ne.s32 	%p26, %r79, 0;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB3_9;

	mul.wide.s32 	%rd12, %r5, 4;
	add.s64 	%rd11, %rd4, %rd12;
	// begin inline asm
	st.global.cs.f32 [%rd11], %f11;
	// end inline asm

$L__BB3_9:
	setp.eq.s64 	%p29, %rd5, 0;
	or.pred  	%p30, %p26, %p29;
	@%p30 bra 	$L__BB3_11;

	mul.wide.s32 	%rd14, %r5, 4;
	add.s64 	%rd13, %rd5, %rd14;
	// begin inline asm
	st.global.cs.f32 [%rd13], %f12;
	// end inline asm

$L__BB3_11:
	@%p1 bra 	$L__BB3_13;

$L__BB3_12:
	cvt.s64.s32 	%rd17, %r79;
	add.s64 	%rd18, %rd17, %rd1;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd15, %rd6, %rd19;
	// begin inline asm
	ld.global.cs.f32 %f68, [%rd15];
	// end inline asm
	sub.f32 	%f70, %f68, %f11;
	mul.f32 	%f69, %f12, %f70;
	add.s64 	%rd16, %rd3, %rd19;
	// begin inline asm
	st.global.cs.f32 [%rd16], %f69;
	// end inline asm
	add.s32 	%r79, %r79, %r1;
	setp.lt.s32 	%p32, %r79, %r10;
	@%p32 bra 	$L__BB3_12;

$L__BB3_13:
	ret;

}
	// .globl	layernorm_backward_kernel3
.visible .entry layernorm_backward_kernel3(
	.param .u64 layernorm_backward_kernel3_param_0,
	.param .u64 layernorm_backward_kernel3_param_1,
	.param .u64 layernorm_backward_kernel3_param_2,
	.param .u64 layernorm_backward_kernel3_param_3,
	.param .u64 layernorm_backward_kernel3_param_4,
	.param .u64 layernorm_backward_kernel3_param_5,
	.param .u64 layernorm_backward_kernel3_param_6,
	.param .u64 layernorm_backward_kernel3_param_7,
	.param .u32 layernorm_backward_kernel3_param_8,
	.param .u32 layernorm_backward_kernel3_param_9,
	.param .u32 layernorm_backward_kernel3_param_10
)
{
	.reg .pred 	%p<24>;
	.reg .f32 	%f<64>;
	.reg .b32 	%r<97>;
	.reg .b64 	%rd<41>;


	ld.param.u64 	%rd10, [layernorm_backward_kernel3_param_0];
	ld.param.u64 	%rd11, [layernorm_backward_kernel3_param_1];
	ld.param.u64 	%rd12, [layernorm_backward_kernel3_param_2];
	ld.param.u64 	%rd13, [layernorm_backward_kernel3_param_3];
	ld.param.u64 	%rd14, [layernorm_backward_kernel3_param_4];
	ld.param.u64 	%rd17, [layernorm_backward_kernel3_param_5];
	ld.param.u64 	%rd15, [layernorm_backward_kernel3_param_6];
	ld.param.u64 	%rd16, [layernorm_backward_kernel3_param_7];
	ld.param.u32 	%r29, [layernorm_backward_kernel3_param_8];
	ld.param.u32 	%r30, [layernorm_backward_kernel3_param_9];
	ld.param.u32 	%r31, [layernorm_backward_kernel3_param_10];
	cvta.to.global.u64 	%rd1, %rd17;
	mov.u32 	%r32, %ntid.y;
	mov.u32 	%r33, %tid.z;
	mov.u32 	%r34, %tid.y;
	mad.lo.s32 	%r35, %r32, %r33, %r34;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r96, %tid.x;
	mad.lo.s32 	%r3, %r35, %r1, %r96;
	mul.lo.s32 	%r36, %r1, %r32;
	mov.u32 	%r37, %ntid.z;
	mad.lo.s32 	%r38, %r36, %r37, 31;
	bfe.u32 	%r4, %r38, 5, 16;
	mov.u32 	%r39, %ctaid.x;
	bfe.u32 	%r40, %r3, 5, 16;
	mad.lo.s32 	%r93, %r4, %r39, %r40;
	setp.ge.s32 	%p1, %r96, %r31;
	@%p1 bra 	$L__BB4_6;

	shl.b32 	%r6, %r1, 2;
	mov.u32 	%r92, %r96;

$L__BB4_2:
	.pragma "nounroll";
	shl.b32 	%r41, %r92, 2;
	mov.u32 	%r42, shared;
	add.s32 	%r8, %r42, %r41;
	mov.u32 	%r43, 0;
	st.shared.u32 	[%r8], %r43;
	shl.b32 	%r44, %r31, 2;
	add.s32 	%r9, %r8, %r44;
	st.shared.u32 	[%r9], %r43;
	add.s32 	%r10, %r92, %r1;
	setp.ge.s32 	%p2, %r10, %r31;
	@%p2 bra 	$L__BB4_6;

	add.s32 	%r11, %r8, %r6;
	st.shared.u32 	[%r11], %r43;
	add.s32 	%r12, %r9, %r6;
	st.shared.u32 	[%r12], %r43;
	add.s32 	%r13, %r10, %r1;
	setp.ge.s32 	%p3, %r13, %r31;
	@%p3 bra 	$L__BB4_6;

	add.s32 	%r14, %r11, %r6;
	mov.u32 	%r46, 0;
	st.shared.u32 	[%r14], %r46;
	add.s32 	%r15, %r12, %r6;
	st.shared.u32 	[%r15], %r46;
	add.s32 	%r16, %r13, %r1;
	setp.ge.s32 	%p4, %r16, %r31;
	@%p4 bra 	$L__BB4_6;

	add.s32 	%r47, %r14, %r6;
	st.shared.u32 	[%r47], %r46;
	add.s32 	%r49, %r15, %r6;
	st.shared.u32 	[%r49], %r46;
	add.s32 	%r92, %r16, %r1;
	setp.lt.s32 	%p5, %r92, %r31;
	@%p5 bra 	$L__BB4_2;

$L__BB4_6:
	bar.sync 	0;
	mul.lo.s32 	%r18, %r30, %r29;
	setp.ge.s32 	%p6, %r93, %r18;
	@%p6 bra 	$L__BB4_15;

	and.b32  	%r19, %r3, 31;
	cvt.rn.f32.s32 	%f1, %r31;
	cvta.to.global.u64 	%rd2, %rd13;
	cvta.to.global.u64 	%rd3, %rd14;
	cvta.to.global.u64 	%rd4, %rd10;
	cvta.to.global.u64 	%rd5, %rd16;
	cvta.to.global.u64 	%rd6, %rd15;
	mov.u32 	%r50, %nctaid.x;
	mul.lo.s32 	%r20, %r4, %r50;

$L__BB4_8:
	rem.s32 	%r51, %r93, %r30;
	sub.s32 	%r52, %r93, %r51;
	mul.lo.s32 	%r53, %r52, %r31;
	cvt.s64.s32 	%rd18, %r53;
	mul.lo.s32 	%r54, %r51, %r31;
	cvt.s64.s32 	%rd19, %r54;
	add.s64 	%rd7, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r93, 4;
	add.s64 	%rd21, %rd6, %rd20;
	ld.global.f32 	%f2, [%rd21];
	add.s64 	%rd22, %rd5, %rd20;
	ld.global.f32 	%f3, [%rd22];
	setp.ge.s32 	%p7, %r19, %r31;
	mov.f32 	%f62, 0f00000000;
	mov.f32 	%f63, %f62;
	@%p7 bra 	$L__BB4_11;

	mov.u32 	%r94, %r19;

$L__BB4_10:
	cvt.s64.s32 	%rd23, %r94;
	add.s64 	%rd24, %rd7, %rd23;
	shl.b64 	%rd25, %rd24, 2;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.f32 	%f18, [%rd26];
	sub.f32 	%f19, %f18, %f2;
	mul.f32 	%f20, %f3, %f19;
	mul.wide.s32 	%rd27, %r94, 4;
	add.s64 	%rd28, %rd1, %rd27;
	add.s64 	%rd29, %rd2, %rd25;
	ld.global.f32 	%f21, [%rd29];
	ld.global.f32 	%f22, [%rd28];
	mul.f32 	%f23, %f22, %f21;
	add.f32 	%f62, %f62, %f23;
	fma.rn.f32 	%f63, %f20, %f23, %f63;
	add.s32 	%r94, %r94, 32;
	setp.lt.s32 	%p8, %r94, %r31;
	@%p8 bra 	$L__BB4_10;

$L__BB4_11:
	mov.b32 	%r55, %f62;
	mov.u32 	%r56, 31;
	mov.u32 	%r57, 16;
	mov.u32 	%r58, -1;
	shfl.sync.bfly.b32 	%r59|%p9, %r55, %r57, %r56, %r58;
	mov.b32 	%f24, %r59;
	add.f32 	%f25, %f62, %f24;
	mov.b32 	%r60, %f25;
	mov.u32 	%r61, 8;
	shfl.sync.bfly.b32 	%r62|%p10, %r60, %r61, %r56, %r58;
	mov.b32 	%f26, %r62;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r63, %f27;
	mov.u32 	%r64, 4;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r56, %r58;
	mov.b32 	%f28, %r65;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r66, %f29;
	mov.u32 	%r67, 2;
	shfl.sync.bfly.b32 	%r68|%p12, %r66, %r67, %r56, %r58;
	mov.b32 	%f30, %r68;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r69, %f31;
	mov.u32 	%r70, 1;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r70, %r56, %r58;
	mov.b32 	%f32, %r71;
	mov.b32 	%r72, %f63;
	shfl.sync.bfly.b32 	%r73|%p14, %r72, %r57, %r56, %r58;
	mov.b32 	%f33, %r73;
	add.f32 	%f34, %f63, %f33;
	mov.b32 	%r74, %f34;
	shfl.sync.bfly.b32 	%r75|%p15, %r74, %r61, %r56, %r58;
	mov.b32 	%f35, %r75;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r76, %f36;
	shfl.sync.bfly.b32 	%r77|%p16, %r76, %r64, %r56, %r58;
	mov.b32 	%f37, %r77;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r78, %f38;
	shfl.sync.bfly.b32 	%r79|%p17, %r78, %r67, %r56, %r58;
	mov.b32 	%f39, %r79;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r80, %f40;
	shfl.sync.bfly.b32 	%r81|%p18, %r80, %r70, %r56, %r58;
	mov.b32 	%f41, %r81;
	add.f32 	%f10, %f40, %f41;
	add.f32 	%f11, %f31, %f32;
	@%p7 bra 	$L__BB4_14;

	div.rn.f32 	%f12, %f10, %f1;
	div.rn.f32 	%f13, %f11, %f1;
	mov.u32 	%r95, %r19;

$L__BB4_13:
	cvt.s64.s32 	%rd32, %r95;
	add.s64 	%rd33, %rd7, %rd32;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd30, %rd13, %rd34;
	// begin inline asm
	ld.global.cs.f32 %f42, [%rd30];
	// end inline asm
	add.s64 	%rd31, %rd14, %rd34;
	// begin inline asm
	ld.global.cs.f32 %f43, [%rd31];
	// end inline asm
	sub.f32 	%f44, %f43, %f2;
	mul.f32 	%f45, %f3, %f44;
	mul.wide.s32 	%rd35, %r95, 4;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.f32 	%f46, [%rd36];
	shl.b32 	%r82, %r95, 2;
	mov.u32 	%r83, shared;
	add.s32 	%r84, %r83, %r82;
	atom.shared.add.f32 	%f47, [%r84], %f42;
	shl.b32 	%r85, %r31, 2;
	add.s32 	%r86, %r84, %r85;
	mul.f32 	%f48, %f42, %f45;
	atom.shared.add.f32 	%f49, [%r86], %f48;
	fma.rn.f32 	%f50, %f42, %f46, 0f00000000;
	sub.f32 	%f51, %f50, %f13;
	mul.f32 	%f52, %f12, %f45;
	sub.f32 	%f53, %f51, %f52;
	add.s64 	%rd37, %rd4, %rd34;
	ld.global.f32 	%f54, [%rd37];
	fma.rn.f32 	%f55, %f3, %f53, %f54;
	st.global.f32 	[%rd37], %f55;
	add.s32 	%r95, %r95, 32;
	setp.lt.s32 	%p20, %r95, %r31;
	@%p20 bra 	$L__BB4_13;

$L__BB4_14:
	add.s32 	%r93, %r93, %r20;
	setp.lt.s32 	%p21, %r93, %r18;
	@%p21 bra 	$L__BB4_8;

$L__BB4_15:
	bar.sync 	0;
	@%p1 bra 	$L__BB4_18;

	cvta.to.global.u64 	%rd8, %rd12;
	cvta.to.global.u64 	%rd9, %rd11;
	mov.u32 	%r88, shared;
	shl.b32 	%r90, %r31, 2;

$L__BB4_17:
	mul.wide.s32 	%rd38, %r96, 4;
	add.s64 	%rd39, %rd8, %rd38;
	shl.b32 	%r87, %r96, 2;
	add.s32 	%r89, %r88, %r87;
	ld.shared.f32 	%f56, [%r89];
	atom.global.add.f32 	%f57, [%rd39], %f56;
	add.s64 	%rd40, %rd9, %rd38;
	add.s32 	%r91, %r89, %r90;
	ld.shared.f32 	%f58, [%r91];
	atom.global.add.f32 	%f59, [%rd40], %f58;
	add.s32 	%r96, %r96, %r1;
	setp.lt.s32 	%p23, %r96, %r31;
	@%p23 bra 	$L__BB4_17;

$L__BB4_18:
	ret;

}
	// .globl	layernorm_backward_kernel1
.visible .entry layernorm_backward_kernel1(
	.param .u64 layernorm_backward_kernel1_param_0,
	.param .u64 layernorm_backward_kernel1_param_1,
	.param .u64 layernorm_backward_kernel1_param_2,
	.param .u64 layernorm_backward_kernel1_param_3,
	.param .u64 layernorm_backward_kernel1_param_4,
	.param .u64 layernorm_backward_kernel1_param_5,
	.param .u64 layernorm_backward_kernel1_param_6,
	.param .u64 layernorm_backward_kernel1_param_7,
	.param .u32 layernorm_backward_kernel1_param_8,
	.param .u32 layernorm_backward_kernel1_param_9,
	.param .u32 layernorm_backward_kernel1_param_10
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<98>;


	ld.param.u64 	%rd49, [layernorm_backward_kernel1_param_0];
	ld.param.u64 	%rd50, [layernorm_backward_kernel1_param_1];
	ld.param.u64 	%rd51, [layernorm_backward_kernel1_param_2];
	ld.param.u64 	%rd52, [layernorm_backward_kernel1_param_3];
	ld.param.u64 	%rd53, [layernorm_backward_kernel1_param_4];
	ld.param.u64 	%rd54, [layernorm_backward_kernel1_param_5];
	ld.param.u64 	%rd47, [layernorm_backward_kernel1_param_6];
	ld.param.u64 	%rd48, [layernorm_backward_kernel1_param_7];
	ld.param.u32 	%r22, [layernorm_backward_kernel1_param_8];
	ld.param.u32 	%r20, [layernorm_backward_kernel1_param_9];
	ld.param.u32 	%r21, [layernorm_backward_kernel1_param_10];
	cvta.to.global.u64 	%rd1, %rd49;
	cvta.to.global.u64 	%rd2, %rd50;
	cvta.to.global.u64 	%rd3, %rd51;
	cvta.to.global.u64 	%rd4, %rd52;
	cvta.to.global.u64 	%rd5, %rd54;
	cvta.to.global.u64 	%rd6, %rd53;
	mov.u32 	%r23, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r1, %r24, %r23, %r25;
	mul.lo.s32 	%r26, %r20, %r22;
	setp.ge.s32 	%p1, %r1, %r26;
	@%p1 bra 	$L__BB5_15;

	cvta.to.global.u64 	%rd55, %rd47;
	rem.s32 	%r27, %r1, %r20;
	sub.s32 	%r28, %r1, %r27;
	mul.lo.s32 	%r29, %r28, %r21;
	cvt.s64.s32 	%rd7, %r29;
	mul.lo.s32 	%r30, %r27, %r21;
	cvt.s64.s32 	%rd8, %r30;
	mul.wide.s32 	%rd56, %r1, 4;
	add.s64 	%rd57, %rd55, %rd56;
	ld.global.f32 	%f1, [%rd57];
	cvta.to.global.u64 	%rd58, %rd48;
	add.s64 	%rd59, %rd58, %rd56;
	ld.global.f32 	%f2, [%rd59];
	setp.lt.s32 	%p2, %r21, 1;
	mov.f32 	%f146, 0f00000000;
	mov.f32 	%f147, %f146;
	@%p2 bra 	$L__BB5_8;

	add.s32 	%r32, %r21, -1;
	and.b32  	%r40, %r21, 3;
	setp.lt.u32 	%p3, %r32, 3;
	mov.f32 	%f147, 0f00000000;
	mov.u32 	%r39, 0;
	mov.f32 	%f146, %f147;
	@%p3 bra 	$L__BB5_5;

	sub.s32 	%r38, %r21, %r40;
	add.s64 	%rd60, %rd8, %rd7;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd62, %rd61, 8;
	add.s64 	%rd86, %rd4, %rd62;
	add.s64 	%rd85, %rd6, %rd62;
	mov.u64 	%rd87, %rd5;

$L__BB5_4:
	ld.global.f32 	%f26, [%rd85+-8];
	sub.f32 	%f27, %f26, %f1;
	mul.f32 	%f28, %f2, %f27;
	ld.global.f32 	%f29, [%rd86+-8];
	ld.global.f32 	%f30, [%rd87];
	mul.f32 	%f31, %f30, %f29;
	add.f32 	%f32, %f146, %f31;
	fma.rn.f32 	%f33, %f28, %f31, %f147;
	ld.global.f32 	%f34, [%rd85+-4];
	sub.f32 	%f35, %f34, %f1;
	mul.f32 	%f36, %f2, %f35;
	ld.global.f32 	%f37, [%rd86+-4];
	ld.global.f32 	%f38, [%rd87+4];
	mul.f32 	%f39, %f38, %f37;
	add.f32 	%f40, %f32, %f39;
	fma.rn.f32 	%f41, %f36, %f39, %f33;
	ld.global.f32 	%f42, [%rd85];
	sub.f32 	%f43, %f42, %f1;
	mul.f32 	%f44, %f2, %f43;
	ld.global.f32 	%f45, [%rd86];
	ld.global.f32 	%f46, [%rd87+8];
	mul.f32 	%f47, %f46, %f45;
	add.f32 	%f48, %f40, %f47;
	fma.rn.f32 	%f49, %f44, %f47, %f41;
	ld.global.f32 	%f50, [%rd85+4];
	sub.f32 	%f51, %f50, %f1;
	mul.f32 	%f52, %f2, %f51;
	ld.global.f32 	%f53, [%rd86+4];
	ld.global.f32 	%f54, [%rd87+12];
	mul.f32 	%f55, %f54, %f53;
	add.f32 	%f146, %f48, %f55;
	fma.rn.f32 	%f147, %f52, %f55, %f49;
	add.s32 	%r39, %r39, 4;
	add.s64 	%rd87, %rd87, 16;
	add.s64 	%rd86, %rd86, 16;
	add.s64 	%rd85, %rd85, 16;
	add.s32 	%r38, %r38, -4;
	setp.ne.s32 	%p4, %r38, 0;
	@%p4 bra 	$L__BB5_4;

$L__BB5_5:
	setp.eq.s32 	%p5, %r40, 0;
	@%p5 bra 	$L__BB5_8;

	cvt.s64.s32 	%rd63, %r39;
	mul.wide.s32 	%rd64, %r39, 4;
	add.s64 	%rd90, %rd5, %rd64;
	add.s64 	%rd65, %rd63, %rd8;
	add.s64 	%rd66, %rd65, %rd7;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd89, %rd4, %rd67;
	add.s64 	%rd88, %rd6, %rd67;

$L__BB5_7:
	.pragma "nounroll";
	ld.global.f32 	%f56, [%rd88];
	sub.f32 	%f57, %f56, %f1;
	mul.f32 	%f58, %f2, %f57;
	ld.global.f32 	%f59, [%rd89];
	ld.global.f32 	%f60, [%rd90];
	mul.f32 	%f61, %f60, %f59;
	add.f32 	%f146, %f146, %f61;
	fma.rn.f32 	%f147, %f58, %f61, %f147;
	add.s64 	%rd90, %rd90, 4;
	add.s64 	%rd89, %rd89, 4;
	add.s64 	%rd88, %rd88, 4;
	add.s32 	%r40, %r40, -1;
	setp.ne.s32 	%p6, %r40, 0;
	@%p6 bra 	$L__BB5_7;

$L__BB5_8:
	cvt.rn.f32.s32 	%f62, %r21;
	div.rn.f32 	%f17, %f146, %f62;
	div.rn.f32 	%f18, %f147, %f62;
	@%p2 bra 	$L__BB5_15;

	add.s32 	%r35, %r21, -1;
	and.b32  	%r44, %r21, 3;
	setp.lt.u32 	%p8, %r35, 3;
	mov.u32 	%r43, 0;
	@%p8 bra 	$L__BB5_12;

	sub.s32 	%r42, %r21, %r44;
	add.s64 	%rd68, %rd8, %rd7;
	shl.b64 	%rd69, %rd68, 2;
	add.s64 	%rd70, %rd69, 8;
	add.s64 	%rd93, %rd1, %rd70;
	add.s64 	%rd92, %rd4, %rd70;
	add.s64 	%rd91, %rd6, %rd70;
	mov.u64 	%rd94, %rd2;
	mov.u64 	%rd95, %rd3;
	mov.u64 	%rd96, %rd5;

$L__BB5_11:
	ld.global.f32 	%f63, [%rd91+-8];
	sub.f32 	%f64, %f63, %f1;
	mul.f32 	%f65, %f2, %f64;
	ld.global.f32 	%f66, [%rd92+-8];
	ld.global.f32 	%f67, [%rd96];
	atom.global.add.f32 	%f68, [%rd95], %f66;
	ld.global.f32 	%f69, [%rd92+-8];
	mul.f32 	%f70, %f65, %f69;
	atom.global.add.f32 	%f71, [%rd94], %f70;
	fma.rn.f32 	%f72, %f67, %f66, 0f00000000;
	sub.f32 	%f73, %f72, %f17;
	mul.f32 	%f74, %f18, %f65;
	sub.f32 	%f75, %f73, %f74;
	ld.global.f32 	%f76, [%rd93+-8];
	fma.rn.f32 	%f77, %f2, %f75, %f76;
	st.global.f32 	[%rd93+-8], %f77;
	ld.global.f32 	%f78, [%rd91+-4];
	sub.f32 	%f79, %f78, %f1;
	mul.f32 	%f80, %f2, %f79;
	ld.global.f32 	%f81, [%rd92+-4];
	ld.global.f32 	%f82, [%rd96+4];
	add.s64 	%rd71, %rd95, 4;
	atom.global.add.f32 	%f83, [%rd71], %f81;
	ld.global.f32 	%f84, [%rd92+-4];
	mul.f32 	%f85, %f80, %f84;
	add.s64 	%rd72, %rd94, 4;
	atom.global.add.f32 	%f86, [%rd72], %f85;
	fma.rn.f32 	%f87, %f82, %f81, 0f00000000;
	sub.f32 	%f88, %f87, %f17;
	mul.f32 	%f89, %f18, %f80;
	sub.f32 	%f90, %f88, %f89;
	ld.global.f32 	%f91, [%rd93+-4];
	fma.rn.f32 	%f92, %f2, %f90, %f91;
	st.global.f32 	[%rd93+-4], %f92;
	ld.global.f32 	%f93, [%rd91];
	sub.f32 	%f94, %f93, %f1;
	mul.f32 	%f95, %f2, %f94;
	ld.global.f32 	%f96, [%rd92];
	ld.global.f32 	%f97, [%rd96+8];
	add.s64 	%rd73, %rd95, 8;
	atom.global.add.f32 	%f98, [%rd73], %f96;
	ld.global.f32 	%f99, [%rd92];
	mul.f32 	%f100, %f95, %f99;
	add.s64 	%rd74, %rd94, 8;
	atom.global.add.f32 	%f101, [%rd74], %f100;
	fma.rn.f32 	%f102, %f97, %f96, 0f00000000;
	sub.f32 	%f103, %f102, %f17;
	mul.f32 	%f104, %f18, %f95;
	sub.f32 	%f105, %f103, %f104;
	ld.global.f32 	%f106, [%rd93];
	fma.rn.f32 	%f107, %f2, %f105, %f106;
	st.global.f32 	[%rd93], %f107;
	ld.global.f32 	%f108, [%rd91+4];
	sub.f32 	%f109, %f108, %f1;
	mul.f32 	%f110, %f2, %f109;
	ld.global.f32 	%f111, [%rd92+4];
	ld.global.f32 	%f112, [%rd96+12];
	add.s64 	%rd75, %rd95, 12;
	atom.global.add.f32 	%f113, [%rd75], %f111;
	ld.global.f32 	%f114, [%rd92+4];
	mul.f32 	%f115, %f110, %f114;
	add.s64 	%rd76, %rd94, 12;
	atom.global.add.f32 	%f116, [%rd76], %f115;
	fma.rn.f32 	%f117, %f112, %f111, 0f00000000;
	sub.f32 	%f118, %f117, %f17;
	mul.f32 	%f119, %f18, %f110;
	sub.f32 	%f120, %f118, %f119;
	ld.global.f32 	%f121, [%rd93+4];
	fma.rn.f32 	%f122, %f2, %f120, %f121;
	st.global.f32 	[%rd93+4], %f122;
	add.s32 	%r43, %r43, 4;
	add.s64 	%rd96, %rd96, 16;
	add.s64 	%rd95, %rd95, 16;
	add.s64 	%rd94, %rd94, 16;
	add.s64 	%rd93, %rd93, 16;
	add.s64 	%rd92, %rd92, 16;
	add.s64 	%rd91, %rd91, 16;
	add.s32 	%r42, %r42, -4;
	setp.ne.s32 	%p9, %r42, 0;
	@%p9 bra 	$L__BB5_11;

$L__BB5_12:
	setp.eq.s32 	%p10, %r44, 0;
	@%p10 bra 	$L__BB5_15;

	mul.wide.s32 	%rd97, %r43, 4;
	add.s64 	%rd77, %rd8, %rd7;
	shl.b64 	%rd78, %rd77, 2;
	add.s64 	%rd42, %rd1, %rd78;
	add.s64 	%rd43, %rd4, %rd78;
	add.s64 	%rd44, %rd6, %rd78;

$L__BB5_14:
	.pragma "nounroll";
	add.s64 	%rd79, %rd44, %rd97;
	ld.global.f32 	%f123, [%rd79];
	sub.f32 	%f124, %f123, %f1;
	mul.f32 	%f125, %f2, %f124;
	add.s64 	%rd80, %rd5, %rd97;
	add.s64 	%rd81, %rd43, %rd97;
	ld.global.f32 	%f126, [%rd81];
	ld.global.f32 	%f127, [%rd80];
	add.s64 	%rd82, %rd3, %rd97;
	atom.global.add.f32 	%f128, [%rd82], %f126;
	add.s64 	%rd83, %rd2, %rd97;
	ld.global.f32 	%f129, [%rd81];
	mul.f32 	%f130, %f125, %f129;
	atom.global.add.f32 	%f131, [%rd83], %f130;
	fma.rn.f32 	%f132, %f127, %f126, 0f00000000;
	sub.f32 	%f133, %f132, %f17;
	mul.f32 	%f134, %f18, %f125;
	sub.f32 	%f135, %f133, %f134;
	add.s64 	%rd84, %rd42, %rd97;
	ld.global.f32 	%f136, [%rd84];
	fma.rn.f32 	%f137, %f2, %f135, %f136;
	st.global.f32 	[%rd84], %f137;
	add.s64 	%rd97, %rd97, 4;
	add.s32 	%r44, %r44, -1;
	setp.ne.s32 	%p11, %r44, 0;
	@%p11 bra 	$L__BB5_14;

$L__BB5_15:
	ret;

}
	// .globl	layernorm_backward_kernel7
.visible .entry layernorm_backward_kernel7(
	.param .u64 layernorm_backward_kernel7_param_0,
	.param .u64 layernorm_backward_kernel7_param_1,
	.param .u64 layernorm_backward_kernel7_param_2,
	.param .u64 layernorm_backward_kernel7_param_3,
	.param .u64 layernorm_backward_kernel7_param_4,
	.param .u64 layernorm_backward_kernel7_param_5,
	.param .u64 layernorm_backward_kernel7_param_6,
	.param .u64 layernorm_backward_kernel7_param_7,
	.param .u64 layernorm_backward_kernel7_param_8,
	.param .u32 layernorm_backward_kernel7_param_9,
	.param .u32 layernorm_backward_kernel7_param_10,
	.param .u32 layernorm_backward_kernel7_param_11
)
{
	.reg .pred 	%p<35>;
	.reg .f32 	%f<123>;
	.reg .b32 	%r<128>;
	.reg .b64 	%rd<84>;


	ld.param.u64 	%rd19, [layernorm_backward_kernel7_param_0];
	ld.param.u64 	%rd26, [layernorm_backward_kernel7_param_3];
	ld.param.u64 	%rd22, [layernorm_backward_kernel7_param_4];
	ld.param.u64 	%rd23, [layernorm_backward_kernel7_param_5];
	ld.param.u64 	%rd27, [layernorm_backward_kernel7_param_6];
	ld.param.u64 	%rd24, [layernorm_backward_kernel7_param_7];
	ld.param.u64 	%rd25, [layernorm_backward_kernel7_param_8];
	ld.param.u32 	%r41, [layernorm_backward_kernel7_param_9];
	ld.param.u32 	%r42, [layernorm_backward_kernel7_param_10];
	ld.param.u32 	%r43, [layernorm_backward_kernel7_param_11];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd23;
	cvta.to.global.u64 	%rd3, %rd27;
	cvta.to.global.u64 	%rd4, %rd26;
	mov.u32 	%r1, WARP_SZ;
	mov.u32 	%r2, %tid.x;
	div.u32 	%r44, %r2, %r1;
	mov.u32 	%r3, %ntid.x;
	div.u32 	%r4, %r3, %r1;
	mov.u32 	%r45, %ctaid.x;
	mad.lo.s32 	%r122, %r4, %r45, %r44;
	mul.lo.s32 	%r46, %r44, %r1;
	sub.s32 	%r6, %r2, %r46;
	mov.u32 	%r7, %nctaid.x;
	setp.ge.s32 	%p1, %r2, %r43;
	@%p1 bra 	$L__BB6_6;

	shl.b32 	%r8, %r3, 2;
	mov.u32 	%r121, %r2;

$L__BB6_2:
	.pragma "nounroll";
	shl.b32 	%r47, %r121, 2;
	mov.u32 	%r48, shared;
	add.s32 	%r10, %r48, %r47;
	mov.u32 	%r49, 0;
	st.shared.u32 	[%r10], %r49;
	shl.b32 	%r50, %r43, 2;
	add.s32 	%r11, %r10, %r50;
	st.shared.u32 	[%r11], %r49;
	add.s32 	%r12, %r121, %r3;
	setp.ge.s32 	%p2, %r12, %r43;
	@%p2 bra 	$L__BB6_6;

	add.s32 	%r13, %r10, %r8;
	st.shared.u32 	[%r13], %r49;
	add.s32 	%r14, %r11, %r8;
	st.shared.u32 	[%r14], %r49;
	add.s32 	%r15, %r12, %r3;
	setp.ge.s32 	%p3, %r15, %r43;
	@%p3 bra 	$L__BB6_6;

	add.s32 	%r16, %r13, %r8;
	mov.u32 	%r52, 0;
	st.shared.u32 	[%r16], %r52;
	add.s32 	%r17, %r14, %r8;
	st.shared.u32 	[%r17], %r52;
	add.s32 	%r18, %r15, %r3;
	setp.ge.s32 	%p4, %r18, %r43;
	@%p4 bra 	$L__BB6_6;

	add.s32 	%r53, %r16, %r8;
	st.shared.u32 	[%r53], %r52;
	add.s32 	%r55, %r17, %r8;
	st.shared.u32 	[%r55], %r52;
	add.s32 	%r121, %r18, %r3;
	setp.lt.s32 	%p5, %r121, %r43;
	@%p5 bra 	$L__BB6_2;

$L__BB6_6:
	mov.u32 	%r57, shared;
	bar.sync 	0;
	mul.lo.s32 	%r22, %r42, %r41;
	setp.ge.s32 	%p6, %r122, %r22;
	@%p6 bra 	$L__BB6_20;

	cvt.rn.f32.s32 	%f1, %r43;
	sub.s32 	%r58, %r2, %r6;
	add.s32 	%r59, %r58, %r43;
	not.b32 	%r60, %r2;
	add.s32 	%r61, %r59, %r60;
	div.u32 	%r23, %r61, %r1;
	add.s32 	%r62, %r23, 1;
	and.b32  	%r24, %r62, 3;
	cvt.s64.s32 	%rd5, %r6;
	mul.wide.s32 	%rd28, %r6, 4;
	add.s64 	%rd6, %rd3, %rd28;
	add.s32 	%r25, %r6, %r1;
	cvt.s64.s32 	%rd7, %r25;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd8, %rd6, %rd11;
	add.s32 	%r26, %r25, %r1;
	cvt.s64.s32 	%rd9, %r26;
	add.s64 	%rd10, %rd8, %rd11;
	add.s32 	%r27, %r26, %r1;
	cvta.to.global.u64 	%rd12, %rd19;
	cvta.to.global.u64 	%rd13, %rd25;
	cvta.to.global.u64 	%rd14, %rd24;
	mul.lo.s32 	%r28, %r4, %r7;

$L__BB6_8:
	rem.s32 	%r63, %r122, %r42;
	sub.s32 	%r64, %r122, %r63;
	mul.lo.s32 	%r65, %r64, %r43;
	cvt.s64.s32 	%rd29, %r65;
	mul.lo.s32 	%r66, %r63, %r43;
	cvt.s64.s32 	%rd30, %r66;
	add.s64 	%rd15, %rd29, %rd30;
	mul.wide.s32 	%rd31, %r122, 4;
	add.s64 	%rd32, %rd14, %rd31;
	ld.global.f32 	%f2, [%rd32];
	add.s64 	%rd33, %rd13, %rd31;
	ld.global.f32 	%f3, [%rd33];
	setp.ge.s32 	%p7, %r6, %r43;
	mov.f32 	%f121, 0f00000000;
	mov.f32 	%f122, %f121;
	@%p7 bra 	$L__BB6_16;

	setp.eq.s32 	%p8, %r24, 0;
	mov.f32 	%f122, 0f00000000;
	mov.u32 	%r123, %r6;
	mov.f32 	%f121, %f122;
	@%p8 bra 	$L__BB6_13;

	setp.eq.s32 	%p9, %r24, 1;
	add.s64 	%rd34, %rd15, %rd5;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.f32 	%f29, [%rd36];
	sub.f32 	%f30, %f29, %f2;
	mul.f32 	%f31, %f3, %f30;
	add.s64 	%rd37, %rd1, %rd35;
	ld.global.f32 	%f32, [%rd37];
	ld.global.f32 	%f33, [%rd6];
	mul.f32 	%f34, %f33, %f32;
	add.f32 	%f121, %f34, 0f00000000;
	fma.rn.f32 	%f122, %f31, %f34, 0f00000000;
	mov.u32 	%r123, %r25;
	@%p9 bra 	$L__BB6_13;

	setp.eq.s32 	%p10, %r24, 2;
	add.s64 	%rd38, %rd15, %rd7;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.f32 	%f35, [%rd40];
	sub.f32 	%f36, %f35, %f2;
	mul.f32 	%f37, %f3, %f36;
	add.s64 	%rd41, %rd1, %rd39;
	ld.global.f32 	%f38, [%rd41];
	ld.global.f32 	%f39, [%rd8];
	mul.f32 	%f40, %f39, %f38;
	add.f32 	%f121, %f121, %f40;
	fma.rn.f32 	%f122, %f37, %f40, %f122;
	mov.u32 	%r123, %r26;
	@%p10 bra 	$L__BB6_13;

	add.s64 	%rd42, %rd15, %rd9;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.f32 	%f41, [%rd44];
	sub.f32 	%f42, %f41, %f2;
	mul.f32 	%f43, %f3, %f42;
	add.s64 	%rd45, %rd1, %rd43;
	ld.global.f32 	%f44, [%rd45];
	ld.global.f32 	%f45, [%rd10];
	mul.f32 	%f46, %f45, %f44;
	add.f32 	%f121, %f121, %f46;
	fma.rn.f32 	%f122, %f43, %f46, %f122;
	mov.u32 	%r123, %r27;

$L__BB6_13:
	setp.lt.u32 	%p11, %r23, 3;
	@%p11 bra 	$L__BB6_16;

$L__BB6_15:
	cvt.s64.s32 	%rd46, %r123;
	add.s64 	%rd47, %rd15, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd2, %rd48;
	ld.global.f32 	%f47, [%rd49];
	sub.f32 	%f48, %f47, %f2;
	mul.f32 	%f49, %f3, %f48;
	mul.wide.s32 	%rd50, %r123, 4;
	add.s64 	%rd51, %rd3, %rd50;
	add.s64 	%rd52, %rd1, %rd48;
	ld.global.f32 	%f50, [%rd52];
	ld.global.f32 	%f51, [%rd51];
	mul.f32 	%f52, %f51, %f50;
	add.f32 	%f53, %f121, %f52;
	fma.rn.f32 	%f54, %f49, %f52, %f122;
	add.s64 	%rd53, %rd49, %rd11;
	ld.global.f32 	%f55, [%rd53];
	sub.f32 	%f56, %f55, %f2;
	mul.f32 	%f57, %f3, %f56;
	add.s64 	%rd54, %rd51, %rd11;
	add.s64 	%rd55, %rd52, %rd11;
	ld.global.f32 	%f58, [%rd55];
	ld.global.f32 	%f59, [%rd54];
	mul.f32 	%f60, %f59, %f58;
	add.f32 	%f61, %f53, %f60;
	fma.rn.f32 	%f62, %f57, %f60, %f54;
	add.s32 	%r67, %r123, %r1;
	add.s32 	%r68, %r67, %r1;
	add.s64 	%rd56, %rd53, %rd11;
	ld.global.f32 	%f63, [%rd56];
	sub.f32 	%f64, %f63, %f2;
	mul.f32 	%f65, %f3, %f64;
	add.s64 	%rd57, %rd54, %rd11;
	add.s64 	%rd58, %rd55, %rd11;
	ld.global.f32 	%f66, [%rd58];
	ld.global.f32 	%f67, [%rd57];
	mul.f32 	%f68, %f67, %f66;
	add.f32 	%f69, %f61, %f68;
	fma.rn.f32 	%f70, %f65, %f68, %f62;
	add.s32 	%r69, %r68, %r1;
	add.s64 	%rd59, %rd56, %rd11;
	ld.global.f32 	%f71, [%rd59];
	sub.f32 	%f72, %f71, %f2;
	mul.f32 	%f73, %f3, %f72;
	add.s64 	%rd60, %rd57, %rd11;
	add.s64 	%rd61, %rd58, %rd11;
	ld.global.f32 	%f74, [%rd61];
	ld.global.f32 	%f75, [%rd60];
	mul.f32 	%f76, %f75, %f74;
	add.f32 	%f121, %f69, %f76;
	fma.rn.f32 	%f122, %f73, %f76, %f70;
	add.s32 	%r123, %r69, %r1;
	setp.lt.s32 	%p12, %r123, %r43;
	@%p12 bra 	$L__BB6_15;

$L__BB6_16:
	mov.b32 	%r70, %f121;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p13, %r70, %r72, %r71, %r73;
	mov.b32 	%f77, %r74;
	add.f32 	%f78, %f121, %f77;
	mov.b32 	%r75, %f78;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p14, %r75, %r76, %r71, %r73;
	mov.b32 	%f79, %r77;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r78, %f80;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p15, %r78, %r79, %r71, %r73;
	mov.b32 	%f81, %r80;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r81, %f82;
	mov.u32 	%r82, 2;
	shfl.sync.bfly.b32 	%r83|%p16, %r81, %r82, %r71, %r73;
	mov.b32 	%f83, %r83;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r84, %f84;
	mov.u32 	%r85, 1;
	shfl.sync.bfly.b32 	%r86|%p17, %r84, %r85, %r71, %r73;
	mov.b32 	%f85, %r86;
	mov.b32 	%r87, %f122;
	shfl.sync.bfly.b32 	%r88|%p18, %r87, %r72, %r71, %r73;
	mov.b32 	%f86, %r88;
	add.f32 	%f87, %f122, %f86;
	mov.b32 	%r89, %f87;
	shfl.sync.bfly.b32 	%r90|%p19, %r89, %r76, %r71, %r73;
	mov.b32 	%f88, %r90;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r91, %f89;
	shfl.sync.bfly.b32 	%r92|%p20, %r91, %r79, %r71, %r73;
	mov.b32 	%f90, %r92;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r93, %f91;
	shfl.sync.bfly.b32 	%r94|%p21, %r93, %r82, %r71, %r73;
	mov.b32 	%f92, %r94;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r95, %f93;
	shfl.sync.bfly.b32 	%r96|%p22, %r95, %r85, %r71, %r73;
	mov.b32 	%f94, %r96;
	add.f32 	%f20, %f93, %f94;
	add.f32 	%f21, %f84, %f85;
	@%p7 bra 	$L__BB6_19;

	div.rn.f32 	%f22, %f20, %f1;
	div.rn.f32 	%f23, %f21, %f1;
	mov.u32 	%r125, %r6;

$L__BB6_18:
	cvt.s64.s32 	%rd64, %r125;
	add.s64 	%rd65, %rd15, %rd64;
	shl.b64 	%rd66, %rd65, 2;
	add.s64 	%rd62, %rd22, %rd66;
	// begin inline asm
	ld.global.cs.f32 %f95, [%rd62];
	// end inline asm
	add.s64 	%rd63, %rd23, %rd66;
	// begin inline asm
	ld.global.cs.f32 %f96, [%rd63];
	// end inline asm
	sub.f32 	%f97, %f96, %f2;
	mul.f32 	%f98, %f3, %f97;
	mul.wide.s32 	%rd67, %r125, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.global.f32 	%f99, [%rd68];
	shl.b32 	%r97, %r125, 2;
	add.s32 	%r99, %r57, %r97;
	atom.shared.add.f32 	%f100, [%r99], %f95;
	shl.b32 	%r100, %r43, 2;
	add.s32 	%r101, %r99, %r100;
	mul.f32 	%f101, %f95, %f98;
	atom.shared.add.f32 	%f102, [%r101], %f101;
	fma.rn.f32 	%f103, %f95, %f99, 0f00000000;
	sub.f32 	%f104, %f103, %f23;
	mul.f32 	%f105, %f22, %f98;
	sub.f32 	%f106, %f104, %f105;
	add.s64 	%rd69, %rd12, %rd66;
	ld.global.f32 	%f107, [%rd69];
	fma.rn.f32 	%f108, %f3, %f106, %f107;
	st.global.f32 	[%rd69], %f108;
	add.s32 	%r125, %r125, %r1;
	setp.lt.s32 	%p24, %r125, %r43;
	@%p24 bra 	$L__BB6_18;

$L__BB6_19:
	add.s32 	%r122, %r122, %r28;
	setp.lt.s32 	%p25, %r122, %r22;
	@%p25 bra 	$L__BB6_8;

$L__BB6_20:
	mov.u32 	%r115, %tid.x;
	setp.ge.s32 	%p33, %r115, %r43;
	bar.sync 	0;
	cvt.s64.s32 	%rd16, %r43;
	@%p33 bra 	$L__BB6_23;

	mov.u32 	%r126, %tid.x;
	shl.b64 	%rd72, %rd16, 2;
	shl.b32 	%r105, %r43, 2;

$L__BB6_22:
	mul.wide.s32 	%rd70, %r126, 4;
	add.s64 	%rd71, %rd4, %rd70;
	shl.b32 	%r102, %r126, 2;
	add.s32 	%r104, %r57, %r102;
	ld.shared.f32 	%f109, [%r104];
	atom.global.add.f32 	%f110, [%rd71], %f109;
	add.s64 	%rd73, %rd71, %rd72;
	add.s32 	%r106, %r104, %r105;
	ld.shared.f32 	%f111, [%r106];
	atom.global.add.f32 	%f112, [%rd73], %f111;
	add.s32 	%r126, %r126, %r3;
	setp.lt.s32 	%p27, %r126, %r43;
	@%p27 bra 	$L__BB6_22;

$L__BB6_23:
	mov.u32 	%r110, %tid.x;
	bar.sync 	0;
	setp.ne.s32 	%p28, %r110, 0;
	@%p28 bra 	$L__BB6_25;

	shl.b32 	%r120, %r43, 3;
	add.s32 	%r119, %r57, %r120;
	shl.b32 	%r113, %r43, 1;
	mul.wide.s32 	%rd74, %r113, 4;
	add.s64 	%rd75, %rd4, %rd74;
	atom.global.add.u32 	%r107, [%rd75], 1;
	st.shared.u32 	[%r119], %r107;

$L__BB6_25:
	shl.b32 	%r118, %r43, 3;
	add.s32 	%r117, %r57, %r118;
	mov.u32 	%r116, %tid.x;
	setp.ge.s32 	%p34, %r116, %r43;
	mov.u32 	%r111, %nctaid.x;
	bar.sync 	0;
	add.s32 	%r108, %r111, -1;
	ld.shared.u32 	%r109, [%r117];
	setp.ne.s32 	%p30, %r109, %r108;
	or.pred  	%p31, %p30, %p34;
	@%p31 bra 	$L__BB6_28;

	ld.param.u64 	%rd83, [layernorm_backward_kernel7_param_1];
	ld.param.u64 	%rd82, [layernorm_backward_kernel7_param_2];
	mov.u32 	%r127, %tid.x;
	cvta.to.global.u64 	%rd17, %rd82;
	cvta.to.global.u64 	%rd18, %rd83;
	shl.b64 	%rd79, %rd16, 2;

$L__BB6_27:
	mul.wide.s32 	%rd76, %r127, 4;
	add.s64 	%rd77, %rd4, %rd76;
	ld.global.f32 	%f113, [%rd77];
	add.s64 	%rd78, %rd17, %rd76;
	st.global.f32 	[%rd78], %f113;
	add.s64 	%rd80, %rd77, %rd79;
	ld.global.f32 	%f114, [%rd80];
	add.s64 	%rd81, %rd18, %rd76;
	st.global.f32 	[%rd81], %f114;
	add.s32 	%r127, %r127, %r3;
	setp.lt.s32 	%p32, %r127, %r43;
	@%p32 bra 	$L__BB6_27;

$L__BB6_28:
	ret;

}
	// .globl	layernorm_backward_np_kernel7
.visible .entry layernorm_backward_np_kernel7(
	.param .u64 layernorm_backward_np_kernel7_param_0,
	.param .u64 layernorm_backward_np_kernel7_param_1,
	.param .u64 layernorm_backward_np_kernel7_param_2,
	.param .u64 layernorm_backward_np_kernel7_param_3,
	.param .u64 layernorm_backward_np_kernel7_param_4,
	.param .u64 layernorm_backward_np_kernel7_param_5,
	.param .u32 layernorm_backward_np_kernel7_param_6,
	.param .u32 layernorm_backward_np_kernel7_param_7,
	.param .u32 layernorm_backward_np_kernel7_param_8
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<99>;
	.reg .b32 	%r<72>;
	.reg .b64 	%rd<50>;


	ld.param.u64 	%rd11, [layernorm_backward_np_kernel7_param_0];
	ld.param.u64 	%rd12, [layernorm_backward_np_kernel7_param_2];
	ld.param.u64 	%rd13, [layernorm_backward_np_kernel7_param_3];
	ld.param.u64 	%rd14, [layernorm_backward_np_kernel7_param_4];
	ld.param.u64 	%rd15, [layernorm_backward_np_kernel7_param_5];
	ld.param.u32 	%r22, [layernorm_backward_np_kernel7_param_6];
	ld.param.u32 	%r20, [layernorm_backward_np_kernel7_param_7];
	ld.param.u32 	%r21, [layernorm_backward_np_kernel7_param_8];
	cvta.to.global.u64 	%rd1, %rd12;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r1, WARP_SZ;
	mov.u32 	%r2, %tid.x;
	div.u32 	%r23, %r2, %r1;
	mov.u32 	%r24, %ntid.x;
	div.u32 	%r3, %r24, %r1;
	mov.u32 	%r25, %ctaid.x;
	mad.lo.s32 	%r68, %r3, %r25, %r23;
	mul.lo.s32 	%r26, %r23, %r1;
	sub.s32 	%r5, %r2, %r26;
	mul.lo.s32 	%r6, %r20, %r22;
	setp.ge.s32 	%p1, %r68, %r6;
	@%p1 bra 	$L__BB7_14;

	cvt.rn.f32.s32 	%f1, %r21;
	sub.s32 	%r27, %r2, %r5;
	add.s32 	%r28, %r27, %r21;
	not.b32 	%r29, %r2;
	add.s32 	%r30, %r28, %r29;
	div.u32 	%r7, %r30, %r1;
	add.s32 	%r31, %r7, 1;
	and.b32  	%r8, %r31, 3;
	cvt.s64.s32 	%rd3, %r5;
	add.s32 	%r9, %r5, %r1;
	cvt.s64.s32 	%rd4, %r9;
	add.s32 	%r10, %r9, %r1;
	cvt.s64.s32 	%rd5, %r10;
	add.s32 	%r11, %r10, %r1;
	mul.wide.s32 	%rd6, %r1, 4;
	mov.u32 	%r32, %nctaid.x;
	mul.lo.s32 	%r12, %r3, %r32;
	cvta.to.global.u64 	%rd7, %rd14;
	cvta.to.global.u64 	%rd8, %rd15;
	cvta.to.global.u64 	%rd9, %rd11;

$L__BB7_2:
	ld.param.u32 	%r67, [layernorm_backward_np_kernel7_param_7];
	rem.s32 	%r33, %r68, %r67;
	sub.s32 	%r34, %r68, %r33;
	mul.lo.s32 	%r35, %r34, %r21;
	cvt.s64.s32 	%rd16, %r35;
	mul.lo.s32 	%r36, %r33, %r21;
	cvt.s64.s32 	%rd17, %r36;
	add.s64 	%rd10, %rd16, %rd17;
	mul.wide.s32 	%rd18, %r68, 4;
	add.s64 	%rd19, %rd7, %rd18;
	ld.global.f32 	%f2, [%rd19];
	add.s64 	%rd20, %rd8, %rd18;
	ld.global.f32 	%f3, [%rd20];
	setp.ge.s32 	%p2, %r5, %r21;
	mov.f32 	%f97, 0f00000000;
	mov.f32 	%f98, %f97;
	@%p2 bra 	$L__BB7_10;

	setp.eq.s32 	%p3, %r8, 0;
	mov.f32 	%f98, 0f00000000;
	mov.u32 	%r69, %r5;
	mov.f32 	%f97, %f98;
	@%p3 bra 	$L__BB7_7;

	setp.eq.s32 	%p4, %r8, 1;
	add.s64 	%rd21, %rd10, %rd3;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd2, %rd22;
	ld.global.f32 	%f29, [%rd23];
	sub.f32 	%f30, %f29, %f2;
	mul.f32 	%f31, %f3, %f30;
	add.s64 	%rd24, %rd1, %rd22;
	ld.global.f32 	%f32, [%rd24];
	add.f32 	%f97, %f32, 0f00000000;
	fma.rn.f32 	%f98, %f32, %f31, 0f00000000;
	mov.u32 	%r69, %r9;
	@%p4 bra 	$L__BB7_7;

	setp.eq.s32 	%p5, %r8, 2;
	add.s64 	%rd25, %rd10, %rd4;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd27, %rd2, %rd26;
	ld.global.f32 	%f33, [%rd27];
	sub.f32 	%f34, %f33, %f2;
	mul.f32 	%f35, %f3, %f34;
	add.s64 	%rd28, %rd1, %rd26;
	ld.global.f32 	%f36, [%rd28];
	add.f32 	%f97, %f97, %f36;
	fma.rn.f32 	%f98, %f36, %f35, %f98;
	mov.u32 	%r69, %r10;
	@%p5 bra 	$L__BB7_7;

	add.s64 	%rd29, %rd10, %rd5;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd2, %rd30;
	ld.global.f32 	%f37, [%rd31];
	sub.f32 	%f38, %f37, %f2;
	mul.f32 	%f39, %f3, %f38;
	add.s64 	%rd32, %rd1, %rd30;
	ld.global.f32 	%f40, [%rd32];
	add.f32 	%f97, %f97, %f40;
	fma.rn.f32 	%f98, %f40, %f39, %f98;
	mov.u32 	%r69, %r11;

$L__BB7_7:
	setp.lt.u32 	%p6, %r7, 3;
	@%p6 bra 	$L__BB7_10;

$L__BB7_9:
	cvt.s64.s32 	%rd33, %r69;
	add.s64 	%rd34, %rd10, %rd33;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.f32 	%f41, [%rd36];
	sub.f32 	%f42, %f41, %f2;
	mul.f32 	%f43, %f3, %f42;
	add.s64 	%rd37, %rd1, %rd35;
	ld.global.f32 	%f44, [%rd37];
	add.f32 	%f45, %f97, %f44;
	fma.rn.f32 	%f46, %f44, %f43, %f98;
	add.s64 	%rd38, %rd36, %rd6;
	ld.global.f32 	%f47, [%rd38];
	sub.f32 	%f48, %f47, %f2;
	mul.f32 	%f49, %f3, %f48;
	add.s64 	%rd39, %rd37, %rd6;
	ld.global.f32 	%f50, [%rd39];
	add.f32 	%f51, %f45, %f50;
	fma.rn.f32 	%f52, %f50, %f49, %f46;
	add.s32 	%r37, %r69, %r1;
	add.s32 	%r38, %r37, %r1;
	add.s64 	%rd40, %rd38, %rd6;
	ld.global.f32 	%f53, [%rd40];
	sub.f32 	%f54, %f53, %f2;
	mul.f32 	%f55, %f3, %f54;
	add.s64 	%rd41, %rd39, %rd6;
	ld.global.f32 	%f56, [%rd41];
	add.f32 	%f57, %f51, %f56;
	fma.rn.f32 	%f58, %f56, %f55, %f52;
	add.s32 	%r39, %r38, %r1;
	add.s64 	%rd42, %rd40, %rd6;
	ld.global.f32 	%f59, [%rd42];
	sub.f32 	%f60, %f59, %f2;
	mul.f32 	%f61, %f3, %f60;
	add.s64 	%rd43, %rd41, %rd6;
	ld.global.f32 	%f62, [%rd43];
	add.f32 	%f97, %f57, %f62;
	fma.rn.f32 	%f98, %f62, %f61, %f58;
	add.s32 	%r69, %r39, %r1;
	setp.lt.s32 	%p7, %r69, %r21;
	@%p7 bra 	$L__BB7_9;

$L__BB7_10:
	mov.b32 	%r40, %f97;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p8, %r40, %r42, %r41, %r43;
	mov.b32 	%f63, %r44;
	add.f32 	%f64, %f97, %f63;
	mov.b32 	%r45, %f64;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p9, %r45, %r46, %r41, %r43;
	mov.b32 	%f65, %r47;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r48, %f66;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p10, %r48, %r49, %r41, %r43;
	mov.b32 	%f67, %r50;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r51, %f68;
	mov.u32 	%r52, 2;
	shfl.sync.bfly.b32 	%r53|%p11, %r51, %r52, %r41, %r43;
	mov.b32 	%f69, %r53;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r54, %f70;
	mov.u32 	%r55, 1;
	shfl.sync.bfly.b32 	%r56|%p12, %r54, %r55, %r41, %r43;
	mov.b32 	%f71, %r56;
	mov.b32 	%r57, %f98;
	shfl.sync.bfly.b32 	%r58|%p13, %r57, %r42, %r41, %r43;
	mov.b32 	%f72, %r58;
	add.f32 	%f73, %f98, %f72;
	mov.b32 	%r59, %f73;
	shfl.sync.bfly.b32 	%r60|%p14, %r59, %r46, %r41, %r43;
	mov.b32 	%f74, %r60;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r61, %f75;
	shfl.sync.bfly.b32 	%r62|%p15, %r61, %r49, %r41, %r43;
	mov.b32 	%f76, %r62;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r63, %f77;
	shfl.sync.bfly.b32 	%r64|%p16, %r63, %r52, %r41, %r43;
	mov.b32 	%f78, %r64;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r65, %f79;
	shfl.sync.bfly.b32 	%r66|%p17, %r65, %r55, %r41, %r43;
	mov.b32 	%f80, %r66;
	add.f32 	%f20, %f79, %f80;
	add.f32 	%f21, %f70, %f71;
	@%p2 bra 	$L__BB7_13;

	div.rn.f32 	%f22, %f20, %f1;
	div.rn.f32 	%f23, %f21, %f1;
	mov.u32 	%r71, %r5;

$L__BB7_12:
	cvt.s64.s32 	%rd46, %r71;
	add.s64 	%rd47, %rd10, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd44, %rd12, %rd48;
	// begin inline asm
	ld.global.cs.f32 %f81, [%rd44];
	// end inline asm
	add.s64 	%rd45, %rd13, %rd48;
	// begin inline asm
	ld.global.cs.f32 %f82, [%rd45];
	// end inline asm
	sub.f32 	%f83, %f82, %f2;
	mul.f32 	%f84, %f3, %f83;
	add.f32 	%f85, %f81, 0f00000000;
	sub.f32 	%f86, %f85, %f23;
	mul.f32 	%f87, %f22, %f84;
	sub.f32 	%f88, %f86, %f87;
	add.s64 	%rd49, %rd9, %rd48;
	ld.global.f32 	%f89, [%rd49];
	fma.rn.f32 	%f90, %f3, %f88, %f89;
	st.global.f32 	[%rd49], %f90;
	add.s32 	%r71, %r71, %r1;
	setp.lt.s32 	%p19, %r71, %r21;
	@%p19 bra 	$L__BB7_12;

$L__BB7_13:
	add.s32 	%r68, %r68, %r12;
	setp.lt.s32 	%p20, %r68, %r6;
	@%p20 bra 	$L__BB7_2;

$L__BB7_14:
	ret;

}
	// .globl	layernorm_forward_kernel3_llm
.visible .entry layernorm_forward_kernel3_llm(
	.param .u64 layernorm_forward_kernel3_llm_param_0,
	.param .u64 layernorm_forward_kernel3_llm_param_1,
	.param .u64 layernorm_forward_kernel3_llm_param_2,
	.param .u64 layernorm_forward_kernel3_llm_param_3,
	.param .u64 layernorm_forward_kernel3_llm_param_4,
	.param .u64 layernorm_forward_kernel3_llm_param_5,
	.param .u32 layernorm_forward_kernel3_llm_param_6,
	.param .u32 layernorm_forward_kernel3_llm_param_7
)
{
	.reg .pred 	%p<33>;
	.reg .f32 	%f<111>;
	.reg .b32 	%r<108>;
	.reg .b64 	%rd<99>;


	ld.param.u64 	%rd37, [layernorm_forward_kernel3_llm_param_0];
	ld.param.u64 	%rd38, [layernorm_forward_kernel3_llm_param_1];
	ld.param.u64 	%rd39, [layernorm_forward_kernel3_llm_param_2];
	ld.param.u64 	%rd40, [layernorm_forward_kernel3_llm_param_3];
	ld.param.u64 	%rd41, [layernorm_forward_kernel3_llm_param_4];
	ld.param.u64 	%rd42, [layernorm_forward_kernel3_llm_param_5];
	ld.param.u32 	%r30, [layernorm_forward_kernel3_llm_param_6];
	ld.param.u32 	%r29, [layernorm_forward_kernel3_llm_param_7];
	cvta.to.global.u64 	%rd1, %rd40;
	mov.u32 	%r31, %tid.x;
	and.b32  	%r106, %r31, 31;
	shr.u32 	%r32, %r31, 5;
	mov.u32 	%r33, %ntid.x;
	shr.u32 	%r34, %r33, 5;
	mov.u32 	%r35, %ctaid.x;
	mad.lo.s32 	%r2, %r34, %r35, %r32;
	setp.ge.s32 	%p1, %r2, %r30;
	@%p1 bra 	$L__BB8_26;

	mul.lo.s32 	%r36, %r2, %r29;
	cvt.s64.s32 	%rd2, %r36;
	setp.ge.s32 	%p2, %r106, %r29;
	mov.f32 	%f105, 0f00000000;
	@%p2 bra 	$L__BB8_8;

	not.b32 	%r37, %r106;
	add.s32 	%r38, %r37, %r29;
	shr.u32 	%r39, %r38, 5;
	add.s32 	%r40, %r39, 1;
	and.b32  	%r97, %r40, 3;
	setp.eq.s32 	%p3, %r97, 0;
	mov.f32 	%f105, 0f00000000;
	mov.u32 	%r98, %r106;
	@%p3 bra 	$L__BB8_5;

	cvt.u64.u32 	%rd43, %r31;
	and.b64  	%rd44, %rd43, 31;
	add.s64 	%rd45, %rd2, %rd44;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd90, %rd1, %rd46;
	mov.u32 	%r98, %r106;

$L__BB8_4:
	.pragma "nounroll";
	ld.global.f32 	%f22, [%rd90];
	add.f32 	%f105, %f105, %f22;
	add.s32 	%r98, %r98, 32;
	add.s64 	%rd90, %rd90, 128;
	add.s32 	%r97, %r97, -1;
	setp.ne.s32 	%p4, %r97, 0;
	@%p4 bra 	$L__BB8_4;

$L__BB8_5:
	not.b32 	%r43, %r31;
	or.b32  	%r44, %r43, -32;
	add.s32 	%r45, %r44, %r29;
	setp.lt.u32 	%p5, %r45, 96;
	@%p5 bra 	$L__BB8_8;

	cvt.u64.u32 	%rd47, %r98;
	add.s64 	%rd48, %rd2, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd1, %rd49;
	add.s64 	%rd91, %rd50, 256;

$L__BB8_7:
	ld.global.f32 	%f23, [%rd91+-256];
	add.f32 	%f24, %f105, %f23;
	ld.global.f32 	%f25, [%rd91+-128];
	add.f32 	%f26, %f24, %f25;
	ld.global.f32 	%f27, [%rd91];
	add.f32 	%f28, %f26, %f27;
	ld.global.f32 	%f29, [%rd91+128];
	add.f32 	%f105, %f28, %f29;
	add.s64 	%rd91, %rd91, 512;
	add.s32 	%r98, %r98, 128;
	setp.lt.s32 	%p6, %r98, %r29;
	@%p6 bra 	$L__BB8_7;

$L__BB8_8:
	mov.b32 	%r46, %f105;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p7, %r46, %r48, %r47, %r49;
	mov.b32 	%f30, %r50;
	add.f32 	%f31, %f105, %f30;
	mov.b32 	%r51, %f31;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p8, %r51, %r52, %r47, %r49;
	mov.b32 	%f32, %r53;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r54, %f33;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p9, %r54, %r55, %r47, %r49;
	mov.b32 	%f34, %r56;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r57, %f35;
	mov.u32 	%r58, 2;
	shfl.sync.bfly.b32 	%r59|%p10, %r57, %r58, %r47, %r49;
	mov.b32 	%f36, %r59;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r60, %f37;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r47, %r49;
	mov.b32 	%f38, %r62;
	add.f32 	%f39, %f37, %f38;
	cvt.rn.f32.s32 	%f8, %r29;
	div.rn.f32 	%f9, %f39, %f8;
	setp.eq.s64 	%p12, %rd38, 0;
	setp.ne.s32 	%p13, %r106, 0;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB8_10;

	mul.wide.s32 	%rd52, %r2, 4;
	add.s64 	%rd51, %rd38, %rd52;
	// begin inline asm
	st.global.cs.f32 [%rd51], %f9;
	// end inline asm

$L__BB8_10:
	mov.f32 	%f110, 0f00000000;
	@%p2 bra 	$L__BB8_17;

	not.b32 	%r63, %r106;
	add.s32 	%r11, %r63, %r29;
	shr.u32 	%r64, %r11, 5;
	add.s32 	%r65, %r64, 1;
	and.b32  	%r101, %r65, 3;
	setp.eq.s32 	%p16, %r101, 0;
	mov.f32 	%f110, 0f00000000;
	mov.u32 	%r102, %r106;
	@%p16 bra 	$L__BB8_14;

	cvt.u64.u32 	%rd53, %r31;
	and.b64  	%rd54, %rd53, 31;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd92, %rd1, %rd56;
	mov.u32 	%r102, %r106;

$L__BB8_13:
	.pragma "nounroll";
	ld.global.f32 	%f45, [%rd92];
	sub.f32 	%f46, %f45, %f9;
	fma.rn.f32 	%f110, %f46, %f46, %f110;
	add.s32 	%r102, %r102, 32;
	add.s64 	%rd92, %rd92, 128;
	add.s32 	%r101, %r101, -1;
	setp.ne.s32 	%p17, %r101, 0;
	@%p17 bra 	$L__BB8_13;

$L__BB8_14:
	setp.lt.u32 	%p18, %r11, 96;
	@%p18 bra 	$L__BB8_17;

	cvt.u64.u32 	%rd57, %r102;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd60, %rd1, %rd59;
	add.s64 	%rd93, %rd60, 256;

$L__BB8_16:
	ld.global.f32 	%f47, [%rd93+-256];
	sub.f32 	%f48, %f47, %f9;
	fma.rn.f32 	%f49, %f48, %f48, %f110;
	ld.global.f32 	%f50, [%rd93+-128];
	sub.f32 	%f51, %f50, %f9;
	fma.rn.f32 	%f52, %f51, %f51, %f49;
	ld.global.f32 	%f53, [%rd93];
	sub.f32 	%f54, %f53, %f9;
	fma.rn.f32 	%f55, %f54, %f54, %f52;
	ld.global.f32 	%f56, [%rd93+128];
	sub.f32 	%f57, %f56, %f9;
	fma.rn.f32 	%f110, %f57, %f57, %f55;
	add.s64 	%rd93, %rd93, 512;
	add.s32 	%r102, %r102, 128;
	setp.lt.s32 	%p19, %r102, %r29;
	@%p19 bra 	$L__BB8_16;

$L__BB8_17:
	mov.b32 	%r67, %f110;
	mov.u32 	%r68, 31;
	mov.u32 	%r69, 16;
	mov.u32 	%r70, -1;
	shfl.sync.bfly.b32 	%r71|%p20, %r67, %r69, %r68, %r70;
	mov.b32 	%f58, %r71;
	add.f32 	%f59, %f110, %f58;
	mov.b32 	%r72, %f59;
	mov.u32 	%r73, 8;
	shfl.sync.bfly.b32 	%r74|%p21, %r72, %r73, %r68, %r70;
	mov.b32 	%f60, %r74;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r75, %f61;
	mov.u32 	%r76, 4;
	shfl.sync.bfly.b32 	%r77|%p22, %r75, %r76, %r68, %r70;
	mov.b32 	%f62, %r77;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r78, %f63;
	mov.u32 	%r79, 2;
	shfl.sync.bfly.b32 	%r80|%p23, %r78, %r79, %r68, %r70;
	mov.b32 	%f64, %r80;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r81, %f65;
	mov.u32 	%r82, 1;
	shfl.sync.bfly.b32 	%r83|%p24, %r81, %r82, %r68, %r70;
	mov.b32 	%f66, %r83;
	add.f32 	%f67, %f65, %f66;
	div.rn.f32 	%f68, %f67, %f8;
	add.f32 	%f69, %f68, 0f358637BD;
	rsqrt.approx.f32 	%f17, %f69;
	setp.eq.s64 	%p25, %rd39, 0;
	or.pred  	%p27, %p13, %p25;
	@%p27 bra 	$L__BB8_19;

	mul.wide.s32 	%rd62, %r2, 4;
	add.s64 	%rd61, %rd39, %rd62;
	// begin inline asm
	st.global.cs.f32 [%rd61], %f17;
	// end inline asm

$L__BB8_19:
	@%p2 bra 	$L__BB8_26;

	not.b32 	%r84, %r106;
	add.s32 	%r20, %r84, %r29;
	shr.u32 	%r85, %r20, 5;
	add.s32 	%r86, %r85, 1;
	and.b32  	%r105, %r86, 3;
	setp.eq.s32 	%p29, %r105, 0;
	@%p29 bra 	$L__BB8_23;

	cvt.u64.u32 	%rd63, %r31;
	and.b64  	%rd64, %rd63, 31;
	cvta.to.global.u64 	%rd65, %rd42;
	and.b32  	%r88, %r31, 31;
	mul.wide.u32 	%rd66, %r88, 4;
	add.s64 	%rd97, %rd65, %rd66;
	cvta.to.global.u64 	%rd67, %rd41;
	add.s64 	%rd96, %rd67, %rd66;
	add.s64 	%rd68, %rd2, %rd64;
	shl.b64 	%rd69, %rd68, 2;
	add.s64 	%rd95, %rd37, %rd69;
	add.s64 	%rd94, %rd40, %rd69;

$L__BB8_22:
	.pragma "nounroll";
	// begin inline asm
	ld.global.cs.f32 %f71, [%rd94];
	// end inline asm
	sub.f32 	%f73, %f71, %f9;
	mul.f32 	%f74, %f17, %f73;
	ld.global.nc.f32 	%f75, [%rd96];
	ld.global.nc.f32 	%f76, [%rd97];
	fma.rn.f32 	%f72, %f75, %f74, %f76;
	// begin inline asm
	st.global.cs.f32 [%rd95], %f72;
	// end inline asm
	add.s32 	%r106, %r106, 32;
	add.s64 	%rd97, %rd97, 128;
	add.s64 	%rd96, %rd96, 128;
	add.s64 	%rd95, %rd95, 128;
	add.s64 	%rd94, %rd94, 128;
	add.s32 	%r105, %r105, -1;
	setp.ne.s32 	%p30, %r105, 0;
	@%p30 bra 	$L__BB8_22;

$L__BB8_23:
	setp.lt.u32 	%p31, %r20, 96;
	@%p31 bra 	$L__BB8_26;

	shl.b64 	%rd72, %rd2, 2;
	add.s64 	%rd73, %rd72, 384;
	add.s64 	%rd27, %rd37, %rd73;
	mul.wide.u32 	%rd98, %r106, 4;
	add.s64 	%rd29, %rd40, %rd73;
	add.s64 	%rd74, %rd72, 256;
	add.s64 	%rd30, %rd37, %rd74;
	add.s64 	%rd31, %rd40, %rd74;
	add.s64 	%rd75, %rd72, 128;
	add.s64 	%rd32, %rd37, %rd75;
	add.s64 	%rd33, %rd40, %rd75;
	add.s64 	%rd34, %rd37, %rd72;

$L__BB8_25:
	mul.wide.s32 	%rd84, %r36, 4;
	add.s64 	%rd85, %rd40, %rd84;
	cvta.to.global.u64 	%rd86, %rd41;
	cvta.to.global.u64 	%rd87, %rd42;
	add.s64 	%rd76, %rd85, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f77, [%rd76];
	// end inline asm
	sub.f32 	%f85, %f77, %f9;
	mul.f32 	%f86, %f17, %f85;
	add.s64 	%rd77, %rd34, %rd98;
	add.s64 	%rd88, %rd86, %rd98;
	ld.global.nc.f32 	%f87, [%rd88];
	add.s64 	%rd89, %rd87, %rd98;
	ld.global.nc.f32 	%f88, [%rd89];
	fma.rn.f32 	%f78, %f87, %f86, %f88;
	// begin inline asm
	st.global.cs.f32 [%rd77], %f78;
	// end inline asm
	add.s64 	%rd78, %rd33, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f79, [%rd78];
	// end inline asm
	sub.f32 	%f89, %f79, %f9;
	mul.f32 	%f90, %f17, %f89;
	add.s64 	%rd79, %rd32, %rd98;
	ld.global.nc.f32 	%f91, [%rd88+128];
	ld.global.nc.f32 	%f92, [%rd89+128];
	fma.rn.f32 	%f80, %f91, %f90, %f92;
	// begin inline asm
	st.global.cs.f32 [%rd79], %f80;
	// end inline asm
	add.s64 	%rd80, %rd31, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f81, [%rd80];
	// end inline asm
	sub.f32 	%f93, %f81, %f9;
	mul.f32 	%f94, %f17, %f93;
	add.s64 	%rd81, %rd30, %rd98;
	ld.global.nc.f32 	%f95, [%rd88+256];
	ld.global.nc.f32 	%f96, [%rd89+256];
	fma.rn.f32 	%f82, %f95, %f94, %f96;
	// begin inline asm
	st.global.cs.f32 [%rd81], %f82;
	// end inline asm
	add.s64 	%rd82, %rd29, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f83, [%rd82];
	// end inline asm
	sub.f32 	%f97, %f83, %f9;
	mul.f32 	%f98, %f17, %f97;
	add.s64 	%rd83, %rd27, %rd98;
	ld.global.nc.f32 	%f99, [%rd88+384];
	ld.global.nc.f32 	%f100, [%rd89+384];
	fma.rn.f32 	%f84, %f99, %f98, %f100;
	// begin inline asm
	st.global.cs.f32 [%rd83], %f84;
	// end inline asm
	add.s64 	%rd98, %rd98, 512;
	add.s32 	%r106, %r106, 128;
	setp.lt.s32 	%p32, %r106, %r29;
	@%p32 bra 	$L__BB8_25;

$L__BB8_26:
	ret;

}
	// .globl	layernorm_forward_kernel4_llm
.visible .entry layernorm_forward_kernel4_llm(
	.param .u64 layernorm_forward_kernel4_llm_param_0,
	.param .u64 layernorm_forward_kernel4_llm_param_1,
	.param .u64 layernorm_forward_kernel4_llm_param_2,
	.param .u64 layernorm_forward_kernel4_llm_param_3,
	.param .u64 layernorm_forward_kernel4_llm_param_4,
	.param .u64 layernorm_forward_kernel4_llm_param_5,
	.param .f32 layernorm_forward_kernel4_llm_param_6,
	.param .u32 layernorm_forward_kernel4_llm_param_7,
	.param .u32 layernorm_forward_kernel4_llm_param_8
)
{
	.reg .pred 	%p<33>;
	.reg .f32 	%f<112>;
	.reg .b32 	%r<108>;
	.reg .b64 	%rd<99>;


	ld.param.u64 	%rd37, [layernorm_forward_kernel4_llm_param_0];
	ld.param.u64 	%rd38, [layernorm_forward_kernel4_llm_param_1];
	ld.param.u64 	%rd39, [layernorm_forward_kernel4_llm_param_2];
	ld.param.u64 	%rd40, [layernorm_forward_kernel4_llm_param_3];
	ld.param.u64 	%rd41, [layernorm_forward_kernel4_llm_param_4];
	ld.param.u64 	%rd42, [layernorm_forward_kernel4_llm_param_5];
	ld.param.f32 	%f18, [layernorm_forward_kernel4_llm_param_6];
	ld.param.u32 	%r30, [layernorm_forward_kernel4_llm_param_7];
	ld.param.u32 	%r29, [layernorm_forward_kernel4_llm_param_8];
	cvta.to.global.u64 	%rd1, %rd40;
	mov.u32 	%r31, %tid.x;
	and.b32  	%r106, %r31, 31;
	shr.u32 	%r32, %r31, 5;
	mov.u32 	%r33, %ntid.x;
	shr.u32 	%r34, %r33, 5;
	mov.u32 	%r35, %ctaid.x;
	mad.lo.s32 	%r2, %r34, %r35, %r32;
	setp.ge.s32 	%p1, %r2, %r30;
	@%p1 bra 	$L__BB9_26;

	mul.lo.s32 	%r36, %r2, %r29;
	cvt.s64.s32 	%rd2, %r36;
	setp.ge.s32 	%p2, %r106, %r29;
	mov.f32 	%f106, 0f00000000;
	@%p2 bra 	$L__BB9_8;

	not.b32 	%r37, %r106;
	add.s32 	%r38, %r37, %r29;
	shr.u32 	%r39, %r38, 5;
	add.s32 	%r40, %r39, 1;
	and.b32  	%r97, %r40, 3;
	setp.eq.s32 	%p3, %r97, 0;
	mov.f32 	%f106, 0f00000000;
	mov.u32 	%r98, %r106;
	@%p3 bra 	$L__BB9_5;

	cvt.u64.u32 	%rd43, %r31;
	and.b64  	%rd44, %rd43, 31;
	add.s64 	%rd45, %rd2, %rd44;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd90, %rd1, %rd46;
	mov.u32 	%r98, %r106;

$L__BB9_4:
	.pragma "nounroll";
	ld.global.f32 	%f23, [%rd90];
	add.f32 	%f106, %f106, %f23;
	add.s32 	%r98, %r98, 32;
	add.s64 	%rd90, %rd90, 128;
	add.s32 	%r97, %r97, -1;
	setp.ne.s32 	%p4, %r97, 0;
	@%p4 bra 	$L__BB9_4;

$L__BB9_5:
	not.b32 	%r43, %r31;
	or.b32  	%r44, %r43, -32;
	add.s32 	%r45, %r44, %r29;
	setp.lt.u32 	%p5, %r45, 96;
	@%p5 bra 	$L__BB9_8;

	cvt.u64.u32 	%rd47, %r98;
	add.s64 	%rd48, %rd2, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd1, %rd49;
	add.s64 	%rd91, %rd50, 256;

$L__BB9_7:
	ld.global.f32 	%f24, [%rd91+-256];
	add.f32 	%f25, %f106, %f24;
	ld.global.f32 	%f26, [%rd91+-128];
	add.f32 	%f27, %f25, %f26;
	ld.global.f32 	%f28, [%rd91];
	add.f32 	%f29, %f27, %f28;
	ld.global.f32 	%f30, [%rd91+128];
	add.f32 	%f106, %f29, %f30;
	add.s64 	%rd91, %rd91, 512;
	add.s32 	%r98, %r98, 128;
	setp.lt.s32 	%p6, %r98, %r29;
	@%p6 bra 	$L__BB9_7;

$L__BB9_8:
	mov.b32 	%r46, %f106;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p7, %r46, %r48, %r47, %r49;
	mov.b32 	%f31, %r50;
	add.f32 	%f32, %f106, %f31;
	mov.b32 	%r51, %f32;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p8, %r51, %r52, %r47, %r49;
	mov.b32 	%f33, %r53;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r54, %f34;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p9, %r54, %r55, %r47, %r49;
	mov.b32 	%f35, %r56;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r57, %f36;
	mov.u32 	%r58, 2;
	shfl.sync.bfly.b32 	%r59|%p10, %r57, %r58, %r47, %r49;
	mov.b32 	%f37, %r59;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r60, %f38;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r47, %r49;
	mov.b32 	%f39, %r62;
	add.f32 	%f40, %f38, %f39;
	cvt.rn.f32.s32 	%f8, %r29;
	div.rn.f32 	%f9, %f40, %f8;
	setp.eq.s64 	%p12, %rd38, 0;
	setp.ne.s32 	%p13, %r106, 0;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB9_10;

	mul.wide.s32 	%rd52, %r2, 4;
	add.s64 	%rd51, %rd38, %rd52;
	// begin inline asm
	st.global.cs.f32 [%rd51], %f9;
	// end inline asm

$L__BB9_10:
	mov.f32 	%f111, 0f00000000;
	@%p2 bra 	$L__BB9_17;

	not.b32 	%r63, %r106;
	add.s32 	%r11, %r63, %r29;
	shr.u32 	%r64, %r11, 5;
	add.s32 	%r65, %r64, 1;
	and.b32  	%r101, %r65, 3;
	setp.eq.s32 	%p16, %r101, 0;
	mov.f32 	%f111, 0f00000000;
	mov.u32 	%r102, %r106;
	@%p16 bra 	$L__BB9_14;

	cvt.u64.u32 	%rd53, %r31;
	and.b64  	%rd54, %rd53, 31;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd92, %rd1, %rd56;
	mov.u32 	%r102, %r106;

$L__BB9_13:
	.pragma "nounroll";
	ld.global.f32 	%f46, [%rd92];
	sub.f32 	%f47, %f46, %f9;
	fma.rn.f32 	%f111, %f47, %f47, %f111;
	add.s32 	%r102, %r102, 32;
	add.s64 	%rd92, %rd92, 128;
	add.s32 	%r101, %r101, -1;
	setp.ne.s32 	%p17, %r101, 0;
	@%p17 bra 	$L__BB9_13;

$L__BB9_14:
	setp.lt.u32 	%p18, %r11, 96;
	@%p18 bra 	$L__BB9_17;

	cvt.u64.u32 	%rd57, %r102;
	add.s64 	%rd58, %rd2, %rd57;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd60, %rd1, %rd59;
	add.s64 	%rd93, %rd60, 256;

$L__BB9_16:
	ld.global.f32 	%f48, [%rd93+-256];
	sub.f32 	%f49, %f48, %f9;
	fma.rn.f32 	%f50, %f49, %f49, %f111;
	ld.global.f32 	%f51, [%rd93+-128];
	sub.f32 	%f52, %f51, %f9;
	fma.rn.f32 	%f53, %f52, %f52, %f50;
	ld.global.f32 	%f54, [%rd93];
	sub.f32 	%f55, %f54, %f9;
	fma.rn.f32 	%f56, %f55, %f55, %f53;
	ld.global.f32 	%f57, [%rd93+128];
	sub.f32 	%f58, %f57, %f9;
	fma.rn.f32 	%f111, %f58, %f58, %f56;
	add.s64 	%rd93, %rd93, 512;
	add.s32 	%r102, %r102, 128;
	setp.lt.s32 	%p19, %r102, %r29;
	@%p19 bra 	$L__BB9_16;

$L__BB9_17:
	mov.b32 	%r67, %f111;
	mov.u32 	%r68, 31;
	mov.u32 	%r69, 16;
	mov.u32 	%r70, -1;
	shfl.sync.bfly.b32 	%r71|%p20, %r67, %r69, %r68, %r70;
	mov.b32 	%f59, %r71;
	add.f32 	%f60, %f111, %f59;
	mov.b32 	%r72, %f60;
	mov.u32 	%r73, 8;
	shfl.sync.bfly.b32 	%r74|%p21, %r72, %r73, %r68, %r70;
	mov.b32 	%f61, %r74;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r75, %f62;
	mov.u32 	%r76, 4;
	shfl.sync.bfly.b32 	%r77|%p22, %r75, %r76, %r68, %r70;
	mov.b32 	%f63, %r77;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r78, %f64;
	mov.u32 	%r79, 2;
	shfl.sync.bfly.b32 	%r80|%p23, %r78, %r79, %r68, %r70;
	mov.b32 	%f65, %r80;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r81, %f66;
	mov.u32 	%r82, 1;
	shfl.sync.bfly.b32 	%r83|%p24, %r81, %r82, %r68, %r70;
	mov.b32 	%f67, %r83;
	add.f32 	%f68, %f66, %f67;
	div.rn.f32 	%f69, %f68, %f8;
	add.f32 	%f70, %f69, %f18;
	rsqrt.approx.f32 	%f17, %f70;
	setp.eq.s64 	%p25, %rd39, 0;
	or.pred  	%p27, %p13, %p25;
	@%p27 bra 	$L__BB9_19;

	mul.wide.s32 	%rd62, %r2, 4;
	add.s64 	%rd61, %rd39, %rd62;
	// begin inline asm
	st.global.cs.f32 [%rd61], %f17;
	// end inline asm

$L__BB9_19:
	@%p2 bra 	$L__BB9_26;

	not.b32 	%r84, %r106;
	add.s32 	%r20, %r84, %r29;
	shr.u32 	%r85, %r20, 5;
	add.s32 	%r86, %r85, 1;
	and.b32  	%r105, %r86, 3;
	setp.eq.s32 	%p29, %r105, 0;
	@%p29 bra 	$L__BB9_23;

	cvt.u64.u32 	%rd63, %r31;
	and.b64  	%rd64, %rd63, 31;
	cvta.to.global.u64 	%rd65, %rd42;
	and.b32  	%r88, %r31, 31;
	mul.wide.u32 	%rd66, %r88, 4;
	add.s64 	%rd97, %rd65, %rd66;
	cvta.to.global.u64 	%rd67, %rd41;
	add.s64 	%rd96, %rd67, %rd66;
	add.s64 	%rd68, %rd2, %rd64;
	shl.b64 	%rd69, %rd68, 2;
	add.s64 	%rd95, %rd37, %rd69;
	add.s64 	%rd94, %rd40, %rd69;

$L__BB9_22:
	.pragma "nounroll";
	// begin inline asm
	ld.global.cs.f32 %f72, [%rd94];
	// end inline asm
	sub.f32 	%f74, %f72, %f9;
	mul.f32 	%f75, %f17, %f74;
	ld.global.nc.f32 	%f76, [%rd96];
	ld.global.nc.f32 	%f77, [%rd97];
	fma.rn.f32 	%f73, %f76, %f75, %f77;
	// begin inline asm
	st.global.cs.f32 [%rd95], %f73;
	// end inline asm
	add.s32 	%r106, %r106, 32;
	add.s64 	%rd97, %rd97, 128;
	add.s64 	%rd96, %rd96, 128;
	add.s64 	%rd95, %rd95, 128;
	add.s64 	%rd94, %rd94, 128;
	add.s32 	%r105, %r105, -1;
	setp.ne.s32 	%p30, %r105, 0;
	@%p30 bra 	$L__BB9_22;

$L__BB9_23:
	setp.lt.u32 	%p31, %r20, 96;
	@%p31 bra 	$L__BB9_26;

	shl.b64 	%rd72, %rd2, 2;
	add.s64 	%rd73, %rd72, 384;
	add.s64 	%rd27, %rd37, %rd73;
	mul.wide.u32 	%rd98, %r106, 4;
	add.s64 	%rd29, %rd40, %rd73;
	add.s64 	%rd74, %rd72, 256;
	add.s64 	%rd30, %rd37, %rd74;
	add.s64 	%rd31, %rd40, %rd74;
	add.s64 	%rd75, %rd72, 128;
	add.s64 	%rd32, %rd37, %rd75;
	add.s64 	%rd33, %rd40, %rd75;
	add.s64 	%rd34, %rd37, %rd72;

$L__BB9_25:
	mul.wide.s32 	%rd84, %r36, 4;
	add.s64 	%rd85, %rd40, %rd84;
	cvta.to.global.u64 	%rd86, %rd41;
	cvta.to.global.u64 	%rd87, %rd42;
	add.s64 	%rd76, %rd85, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f78, [%rd76];
	// end inline asm
	sub.f32 	%f86, %f78, %f9;
	mul.f32 	%f87, %f17, %f86;
	add.s64 	%rd77, %rd34, %rd98;
	add.s64 	%rd88, %rd86, %rd98;
	ld.global.nc.f32 	%f88, [%rd88];
	add.s64 	%rd89, %rd87, %rd98;
	ld.global.nc.f32 	%f89, [%rd89];
	fma.rn.f32 	%f79, %f88, %f87, %f89;
	// begin inline asm
	st.global.cs.f32 [%rd77], %f79;
	// end inline asm
	add.s64 	%rd78, %rd33, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f80, [%rd78];
	// end inline asm
	sub.f32 	%f90, %f80, %f9;
	mul.f32 	%f91, %f17, %f90;
	add.s64 	%rd79, %rd32, %rd98;
	ld.global.nc.f32 	%f92, [%rd88+128];
	ld.global.nc.f32 	%f93, [%rd89+128];
	fma.rn.f32 	%f81, %f92, %f91, %f93;
	// begin inline asm
	st.global.cs.f32 [%rd79], %f81;
	// end inline asm
	add.s64 	%rd80, %rd31, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f82, [%rd80];
	// end inline asm
	sub.f32 	%f94, %f82, %f9;
	mul.f32 	%f95, %f17, %f94;
	add.s64 	%rd81, %rd30, %rd98;
	ld.global.nc.f32 	%f96, [%rd88+256];
	ld.global.nc.f32 	%f97, [%rd89+256];
	fma.rn.f32 	%f83, %f96, %f95, %f97;
	// begin inline asm
	st.global.cs.f32 [%rd81], %f83;
	// end inline asm
	add.s64 	%rd82, %rd29, %rd98;
	// begin inline asm
	ld.global.cs.f32 %f84, [%rd82];
	// end inline asm
	sub.f32 	%f98, %f84, %f9;
	mul.f32 	%f99, %f17, %f98;
	add.s64 	%rd83, %rd27, %rd98;
	ld.global.nc.f32 	%f100, [%rd88+384];
	ld.global.nc.f32 	%f101, [%rd89+384];
	fma.rn.f32 	%f85, %f100, %f99, %f101;
	// begin inline asm
	st.global.cs.f32 [%rd83], %f85;
	// end inline asm
	add.s64 	%rd98, %rd98, 512;
	add.s32 	%r106, %r106, 128;
	setp.lt.s32 	%p32, %r106, %r29;
	@%p32 bra 	$L__BB9_25;

$L__BB9_26:
	ret;

}

